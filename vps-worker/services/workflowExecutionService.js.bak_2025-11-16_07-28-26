/**
 * REAL Workflow Execution Service
 * Executes workflows with actual AI calls and data flow between nodes
 * NO MOCKUPS - REAL FUNCTIONALITY ONLY
 */

const { getSupabase } = require('./supabase')
const aiService = require('./aiService')
const { narrativeStructureService } = require('./narrativeStructureService')
const { professionalBookFormatter } = require('./professionalBookFormatter')
const exportService = require('./exportService')
const { accentInstructionService } = require('./accentInstructionService')
const { sampleAnalysisService } = require('./sampleAnalysisService')
const { typographyService } = require('./typographyService')
const sessionManager = require('./sessionManager')
const bookCompilationService = require('./BookCompilationService.js')
const { NODE_ROLE_CONFIG } = require('../data/nodePalettes')
const aiResponseValidator = require('./aiResponseValidator.js')
const { getCelebrityStyle } = require('../config/celebrityStyles')

class WorkflowExecutionService {
  constructor() {
    this.executionState = new Map()
    this.checkpointStates = new Map() // Store checkpoint states for resume functionality
    this.currentSession = null // Current execution session
    // Use aiService for all providers since it fetches API keys from ai_providers table
    this.aiServices = {
      'openai': aiService,
      'anthropic': aiService,
      'google': aiService,
      'ai': aiService
    }
  }

  /**
   * Check for existing session and offer resume option
   * @param {string} flowId - Flow identifier
   * @param {string} userId - User identifier
   * @returns {Object} Session status and resume options
   */
  checkForExistingSession(flowId, userId) {
    return sessionManager.checkSessionStatus(flowId, userId)
  }

  /**
   * Start new execution session
   * @param {Object} params - Session parameters
   * @returns {Object} Session data
   */
  startNewSession({ flowId, userId, nodes, edges, initialInput }) {
    const sessionData = sessionManager.createSessionData({
      flowId,
      userId,
      nodes,
      edges,
      initialInput,
      currentPhase: 'initialization',
      completedNodes: [],
      currentNode: null,
      executionData: {},
      errors: [],
      warnings: []
    })

    this.currentSession = sessionData
    sessionManager.saveSession(flowId, userId, sessionData)
    
    console.log('üöÄ New execution session started:', flowId)
    return sessionData
  }

  /**
   * Resume existing session
   * @param {string} flowId - Flow identifier
   * @param {string} userId - User identifier
   * @returns {Object} Resumed session data
   */
  resumeSession(flowId, userId) {
    const session = sessionManager.loadSession(flowId, userId)
    if (!session) {
      throw new Error('No session found to resume')
    }

    this.currentSession = session
    console.log('üîÑ Resuming execution session:', flowId)
    return session
  }

  /**
   * Update session with current execution state
   * @param {Object} updates - Updates to apply
   */
  updateSession(updates) {
    if (!this.currentSession) {
      console.warn('‚ö†Ô∏è No active session to update')
      return
    }

    const updatedSession = {
      ...this.currentSession,
      ...updates,
      timestamp: Date.now()
    }

    this.currentSession = updatedSession
    sessionManager.saveSession(
      this.currentSession.flowId,
      this.currentSession.userId,
      updatedSession
    )
  }

  /**
   * Complete session and clear from storage
   */
  completeSession() {
    if (!this.currentSession) {
      console.warn('‚ö†Ô∏è No active session to complete')
      return
    }

    console.log('‚úÖ Execution session completed:', this.currentSession.flowId)
    sessionManager.clearSession(
      this.currentSession.flowId,
      this.currentSession.userId
    )
    this.currentSession = null
  }

  /**
   * Handle execution error and save session for recovery
   * @param {Error} error - Execution error
   * @param {Object} context - Error context
   */
  handleExecutionError(error, context = {}) {
    if (!this.currentSession) {
      console.error('‚ùå Execution error with no active session:', error)
      return
    }

    const errorData = {
      error: error.message,
      stack: error.stack,
      context,
      timestamp: new Date().toISOString()
    }

    this.updateSession({
      errors: [...(this.currentSession.errors || []), errorData],
      currentPhase: 'error_recovery'
    })

    console.error('‚ùå Execution error saved to session:', errorData)
  }

  /**
   * PRE-RUN TEST SYSTEM - Validate flow before execution
   * @param {Array} nodes - Workflow nodes
   * @param {Array} edges - Workflow edges
   * @param {Object} initialInput - User input
   * @param {Function} progressCallback - Progress update callback
   * @returns {Object} Test results and validation status
   */
  async preRunTest(nodes, edges, initialInput, progressCallback = null) {
    console.log('üß™ Starting Pre-Run Test for workflow validation...')
    console.log('üß™ Test Parameters:', { 
      nodesCount: nodes?.length || 0, 
      edgesCount: edges?.length || 0, 
      hasInitialInput: !!initialInput 
    })
    
    const testResults = {
      overallStatus: 'passing',
      nodeTests: {},
      connectivityTests: {},
      exportTests: {},
      warnings: [],
      errors: []
    }

    try {
      // 1. VALIDATE NODE CONFIGURATION
      console.log('üîç Testing node configurations...')
      for (const node of nodes) {
        const nodeTest = await this.testNodeConfiguration(node, initialInput)
        testResults.nodeTests[node.id] = nodeTest
        
        if (nodeTest.status === 'error') {
          testResults.overallStatus = 'failing'
          testResults.errors.push(`Node ${node.id}: ${nodeTest.error}`)
        } else if (nodeTest.status === 'warning') {
          testResults.warnings.push(`Node ${node.id}: ${nodeTest.warning}`)
        }
        
        if (progressCallback) {
          progressCallback({
            status: 'testing',
            message: `Testing node: ${node.data?.label || node.id}`,
            progress: (Object.keys(testResults.nodeTests).length / nodes.length) * 25
          })
        }
      }

      // 2. TEST AI CONNECTIVITY
      console.log('ü§ñ Testing AI connectivity...')
      const connectivityTest = await this.testAIConnectivity(nodes)
      testResults.connectivityTests = connectivityTest
      
      if (connectivityTest.status === 'error') {
        testResults.overallStatus = 'failing'
        testResults.errors.push(`AI Connectivity: ${connectivityTest.error}`)
      }

      if (progressCallback) {
        progressCallback({
          status: 'testing',
          message: 'Testing AI connectivity...',
          progress: 50
        })
      }

      // 3. TEST EXPORT SERVICES
      console.log('üìÑ Testing export services...')
      const exportTest = await this.testExportServices()
      testResults.exportTests = exportTest
      
      if (exportTest.status === 'error') {
        testResults.overallStatus = 'failing'
        testResults.errors.push(`Export Services: ${exportTest.error}`)
      }

      if (progressCallback) {
        progressCallback({
          status: 'testing',
          message: 'Testing export services...',
          progress: 75
        })
      }

      // 4. VALIDATE WORKFLOW STRUCTURE
      console.log('üîó Validating workflow structure...')
      const structureTest = this.validateWorkflowStructure(nodes, edges)
      if (!structureTest.valid) {
        testResults.overallStatus = 'failing'
        testResults.errors.push(`Workflow Structure: ${structureTest.error}`)
      }

      if (progressCallback) {
        progressCallback({
          status: testResults.overallStatus === 'passing' ? 'success' : 'error',
          message: testResults.overallStatus === 'passing' 
            ? '‚úÖ Pre-run test completed successfully!' 
            : '‚ùå Pre-run test failed - check errors',
          progress: 100,
          testResults
        })
      }

      console.log('üß™ Pre-Run Test Results:', testResults)
      return testResults

    } catch (error) {
      console.error('‚ùå Pre-Run Test Error:', error)
      console.error('‚ùå Error Stack:', error.stack)
      console.error('‚ùå Error Details:', {
        name: error.name,
        message: error.message,
        cause: error.cause,
        nodes: nodes?.length || 0,
        edges: edges?.length || 0,
        hasInput: !!initialInput
      })
      
      testResults.overallStatus = 'failing'
      testResults.errors.push(`CRITICAL SYSTEM ERROR: ${error.message}`)
      testResults.errors.push(`Error Type: ${error.name}`)
      testResults.errors.push(`Stack Trace: ${error.stack}`)
      
      // Add diagnostic information
      testResults.diagnostics = {
        nodesCount: nodes?.length || 0,
        edgesCount: edges?.length || 0,
        hasInitialInput: !!initialInput,
        inputType: typeof initialInput,
        errorTimestamp: new Date().toISOString(),
        userAgent: typeof navigator !== 'undefined' ? navigator.userAgent : 'Server-side',
        memoryUsage: typeof process !== 'undefined' ? process.memoryUsage() : 'N/A'
      }
      
      if (progressCallback) {
        progressCallback({
          status: 'error',
          message: `CRITICAL ERROR: ${error.message}`,
          progress: 100,
          testResults,
          errorDetails: {
            name: error.name,
            message: error.message,
            stack: error.stack,
            diagnostics: testResults.diagnostics
          }
        })
      }
      
      return testResults
    }
  }

  /**
   * Test individual node configuration
   */
  async testNodeConfiguration(node, initialInput) {
    try {
      // Check if node has required data
      if (!node.data) {
        return { status: 'error', error: 'Missing node data' }
      }

      // Test input nodes
      if (node.type === 'input') {
        if (!node.data.inputFields || !Array.isArray(node.data.inputFields)) {
          return { status: 'warning', warning: 'Input node missing inputFields' }
        }
        
        // Test if required fields are present
        const requiredFields = node.data.inputFields.filter(field => field.required)
        if (requiredFields.length === 0) {
          return { status: 'warning', warning: 'No required fields defined' }
        }
      }

      // Test process nodes
      if (node.type === 'process') {
        if (!node.data.role) {
          return { status: 'error', error: 'Process node missing role' }
        }

        if (!node.data.configuration) {
          return { status: 'warning', warning: 'Process node missing configuration' }
        }

        if (!node.data.configuration.systemPrompt || !node.data.configuration.userPrompt) {
          return { status: 'warning', warning: 'Process node missing prompts' }
        }
      }

      // Test output nodes
      if (node.type === 'output') {
        if (!node.data.role) {
          return { status: 'error', error: 'Output node missing role' }
        }
      }

      // Test preview nodes
      if (node.type === 'preview') {
        if (!node.data.approvalRequired) {
          return { status: 'warning', warning: 'Preview node should have approvalRequired' }
        }
      }

      return { status: 'passing', message: 'Node configuration valid' }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Test AI connectivity for all nodes
   */
  async testAIConnectivity(nodes) {
    try {
      const aiNodes = nodes.filter(node => 
        node.type === 'process' && 
        node.data?.aiEnabled && 
        node.data?.selectedModels?.length > 0
      )

      if (aiNodes.length === 0) {
        return { status: 'warning', warning: 'No AI-enabled nodes found' }
      }

      // Test each AI provider
      const providers = new Set()
      aiNodes.forEach(node => {
        node.data.selectedModels.forEach(model => {
          const provider = model.split('-')[0]?.toLowerCase()
          if (provider) providers.add(provider)
        })
      })

      const connectivityResults = {}
      for (const provider of providers) {
        try {
          // Test API key availability
          const testResult = await this.testAIProvider(provider)
          connectivityResults[provider] = testResult
        } catch (error) {
          connectivityResults[provider] = { status: 'error', error: error.message }
        }
      }

      const hasErrors = Object.values(connectivityResults).some(result => result.status === 'error')
      return {
        status: hasErrors ? 'error' : 'passing',
        providers: connectivityResults,
        error: hasErrors ? 'Some AI providers failed connectivity test' : null
      }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Test individual AI provider
   */
  async testAIProvider(provider) {
    try {
      // Test API key availability
      const { data: apiKeys, error } = await getSupabase()
        .from('ai_providers')
        .select('*')
        .eq('provider', provider)
        .eq('is_active', true)

      if (error) throw error

      if (!apiKeys || apiKeys.length === 0) {
        return { status: 'error', error: `No active API keys found for ${provider}` }
      }

      // Test basic connectivity (optional - can be expensive)
      // For now, just check API key availability
      return { 
        status: 'passing', 
        message: `${provider} API keys available`,
        keyCount: apiKeys.length
      }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Test export services
   */
  async testExportServices() {
    try {
      const exportTests = {}

      // Test PDF export capability
      try {
        // Simple test - check if exportService has PDF method
        if (typeof exportService.generatePDF === 'function') {
          exportTests.pdf = { status: 'passing', message: 'PDF export available' }
        } else {
          exportTests.pdf = { status: 'error', error: 'PDF export method not found' }
        }
      } catch (error) {
        exportTests.pdf = { status: 'error', error: error.message }
      }

      // Test DOCX export capability
      try {
        if (typeof exportService.generateDOCX === 'function') {
          exportTests.docx = { status: 'passing', message: 'DOCX export available' }
        } else {
          exportTests.docx = { status: 'error', error: 'DOCX export method not found' }
        }
      } catch (error) {
        exportTests.docx = { status: 'error', error: error.message }
      }

      // Test EPUB export capability
      try {
        if (typeof exportService.generateEPUB === 'function') {
          exportTests.epub = { status: 'passing', message: 'EPUB export available' }
        } else {
          exportTests.epub = { status: 'error', error: 'EPUB export method not found' }
        }
      } catch (error) {
        exportTests.epub = { status: 'error', error: error.message }
      }

      const hasErrors = Object.values(exportTests).some(test => test.status === 'error')
      return {
        status: hasErrors ? 'error' : 'passing',
        services: exportTests,
        error: hasErrors ? 'Some export services failed test' : null
      }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Validate workflow structure
   */
  validateWorkflowStructure(nodes, edges) {
    try {
      // Check if we have nodes
      if (!nodes || nodes.length === 0) {
        return { valid: false, error: 'No nodes found' }
      }

      // Check if we have edges
      if (!edges || edges.length === 0) {
        return { valid: false, error: 'No edges found' }
      }

      // Find input nodes
      const inputNodes = nodes.filter(node => node.type === 'input')
      if (inputNodes.length === 0) {
        return { valid: false, error: 'No input nodes found' }
      }

      // Find output nodes
      const outputNodes = nodes.filter(node => node.type === 'output')
      if (outputNodes.length === 0) {
        return { valid: false, error: 'No output nodes found' }
      }

      // Check if all edges reference valid nodes
      const nodeIds = new Set(nodes.map(node => node.id))
      for (const edge of edges) {
        if (!nodeIds.has(edge.source) || !nodeIds.has(edge.target)) {
          return { valid: false, error: `Edge references non-existent node: ${edge.source} -> ${edge.target}` }
        }
      }

      return { valid: true, message: 'Workflow structure is valid' }

    } catch (error) {
      return { valid: false, error: error.message }
    }
  }

  /**
   * Execute a complete workflow with real AI calls and data flow
   * @param {Array} nodes - Workflow nodes
   * @param {Array} edges - Workflow edges  
   * @param {Object} initialInput - User input from Lekhika root app
   * @param {string} workflowId - Unique workflow execution ID
   * @param {Function} progressCallback - Progress update callback
   * @returns {Object} Final workflow output
   */
  async executeWorkflow(nodes, edges, initialInput, workflowId, progressCallback = null, superAdminUser = null) {
    const startTime = Date.now() // DEFINE START TIME FOR EXECUTION TRACKING
    
    try {
      // Validate SuperAdmin authentication
      if (!superAdminUser || !superAdminUser.id) {
        throw new Error('SuperAdmin authentication required for workflow execution')
      }

      // Initialize execution state
      this.executionState.set(workflowId, {
        status: 'running',
        currentNode: null,
        results: {},
        errors: [],
        startTime: new Date(),
        progress: 0
      })

      // Build execution order based on edges
      const executionOrder = this.buildExecutionOrder(nodes, edges)
      
      // Build dependency maps for validation during execution
      const incomingEdges = new Map()
      const outgoingEdges = new Map()
      nodes.forEach(node => {
        incomingEdges.set(node.id, [])
        outgoingEdges.set(node.id, [])
      })
      edges.forEach(edge => {
        outgoingEdges.get(edge.source).push(edge.target)
        incomingEdges.get(edge.target).push(edge.source)
      })
      
      // Initialize data pipeline with user input
      let pipelineData = {
        userInput: initialInput,
        nodeOutputs: {},
        superAdminUser: superAdminUser,
        metadata: {
          workflowId,
          executionTime: new Date(),
          totalNodes: executionOrder.length
        }
      }

      console.log('üîç WORKFLOW EXECUTION DEBUG:')
      console.log('  - Initial input:', initialInput)
      console.log('  - Execution order:', executionOrder.map(n => `${n.id} (${n.type}) - ${n.data.label}`))
      console.log('  - Total nodes to execute:', executionOrder.length)
      console.log('  - Initial pipeline data:', pipelineData)

      // Initialize execution with starting progress
      this.updateExecutionState(workflowId, {
        currentNode: 'Initializing...',
        progress: 5,
        baseProgress: 5,
        nodeIndex: -1,
        totalNodes: executionOrder.length,
        status: 'running'
      })
      
      if (progressCallback) {
        progressCallback({
          nodeId: 'initializing',
          nodeName: 'Initializing...',
          progress: 5,
          status: 'running',
          providerName: null,
          baseProgress: 5,
          nodeIndex: -1,
          totalNodes: executionOrder.length
        })
      }

      // Execute nodes in sequence - CRITICAL: ENSURE PROPER ORDER
      console.log('üîç EXECUTION ORDER DEBUG:')
      console.log('  - Total nodes:', executionOrder.length)
      console.log('  - Execution sequence:', executionOrder.map((n, i) => `${i + 1}. ${n.data.label || n.id} (${n.data.type})`).join(', '))
      
      for (let i = 0; i < executionOrder.length; i++) {
        // Check if workflow was paused - wait until resumed
        if (this.isWorkflowPaused(workflowId)) {
          console.log(`‚è∏Ô∏è Workflow ${workflowId} is paused, waiting for resume...`)
          // Wait indefinitely until resumed - use Promise that resolves only on resume
          await this.waitForResume(workflowId)
        }

        // Check if workflow was resumed from a specific checkpoint
        const currentState = this.executionState.get(workflowId)
        if (currentState?.resumedFromNode) {
          console.log(`üîÑ Workflow resumed from node: ${currentState.resumedFromNode}`)
          // Skip to the node after the resumed checkpoint
          const resumedNodeIndex = executionOrder.findIndex(node => node.id === currentState.resumedFromNode)
          if (resumedNodeIndex !== -1 && resumedNodeIndex > i) {
            console.log(`‚è≠Ô∏è Skipping to node ${resumedNodeIndex + 1} (resumed from ${currentState.resumedFromNode})`)
            i = resumedNodeIndex // Skip to the resumed node
            // Clear the resumedFromNode flag
            this.updateExecutionState(workflowId, { resumedFromNode: null })
          }
        }

        // Check if workflow was stopped - ENHANCED CHECK
        if (this.isWorkflowStopped(workflowId) || await this.checkDatabaseStopSignal(workflowId)) {
          console.log(`üõë Workflow ${workflowId} stopped during execution`)
          
          // IMMEDIATELY clear all processing states
          this.updateExecutionState(workflowId, {
            status: 'stopped',
            currentNode: null,
            forceStopped: true
          })
          
          // Force all nodes to stopped state - NO LINGERING "Processing"
          if (progressCallback) {
            progressCallback({
              nodeId: null,
              nodeName: 'System',
              progress: 0,
              status: 'stopped',
              message: 'Workflow killed by user - all processing stopped',
              forceStopped: true
            })
          }
          
          // COLLECT ALL PARTIAL RESULTS - DON'T LOSE GENERATED CONTENT
          const currentState = this.executionState.get(workflowId)
          const partialResults = currentState?.results || {}
          const allNodeOutputs = pipelineData.nodeOutputs || {}
          
          console.log('üì¶ Collecting partial results:', {
            partialResults,
            allNodeOutputs,
            nodeCount: Object.keys(allNodeOutputs).length
          })
          
          return {
            success: false,
            error: 'Workflow stopped by user',
            results: partialResults,
            partialOutputs: allNodeOutputs,
            pipelineData: pipelineData,
            stopped: true,
            forceStopped: true,
            message: 'Workflow killed by user. All processing stopped immediately.'
          }
        }

        const node = executionOrder[i]
        
        // DYNAMIC PROGRESS CALCULATION - NO HARDCODING
        // Base progress on node position, but allow individual nodes to override
        // Calculate progress based on node position with granular updates
        const nodeStartProgress = (i / executionOrder.length) * 100
        const nodeEndProgress = ((i + 1) / executionOrder.length) * 100
        const progress = nodeStartProgress + 5 // Start with some progress for current node
        const baseProgress = progress
        const nodeProgress = progress
        
        this.updateExecutionState(workflowId, {
          currentNode: node.id,
          progress: nodeProgress,
          baseProgress: baseProgress,
          nodeIndex: i,
          totalNodes: executionOrder.length
        })
        
        if (progressCallback) {
          progressCallback({
            nodeId: node.id,
            nodeName: node.data.label,
            progress: nodeProgress,
            status: 'executing',
            providerName: null, // Will be updated when AI call starts
            baseProgress: baseProgress,
            nodeIndex: i,
            totalNodes: executionOrder.length,
            // CRITICAL: Send completed nodeOutputs so frontend can mark steps as done
            nodeResults: pipelineData.nodeOutputs || {}
          })
        }

        try {
          console.log(`üîç EXECUTING NODE ${i + 1}/${executionOrder.length}: ${node.id} (${node.data.label || node.data.type})`)
          console.log('  - Node type:', node.data.type)
          console.log('  - Node dependencies:', incomingEdges?.get(node.id) || 'none')
          console.log('  - Pipeline data before execution:', Object.keys(pipelineData.nodeOutputs || {}))
          
          // CRITICAL: Ensure this node should execute now
          const dependencies = incomingEdges?.get(node.id) || []
          const unmetDependencies = dependencies.filter(depId => !pipelineData.nodeOutputs[depId])
          if (unmetDependencies.length > 0) {
            console.error(`‚ùå EXECUTION ORDER ERROR: Node ${node.id} has unmet dependencies:`, unmetDependencies)
            throw new Error(`Execution order violation: Node ${node.id} cannot execute before dependencies: ${unmetDependencies.join(', ')}`)
          }
          
          // Execute individual node with pipeline data
          const nodeOutput = await this.executeNode(node, pipelineData, workflowId, progressCallback)
          
          console.log('  - Node output:', nodeOutput)
          
          // Add node output to pipeline with execution order tracking
          pipelineData.nodeOutputs[node.id] = {
            ...nodeOutput,
            sequenceNumber: i + 1, // Track execution order for proper display
            executionIndex: i,
            totalNodes: executionOrder.length,
            nodeName: node.data.label || node.data.type || node.id, // ENSURE NODE NAME IS PRESERVED
            nodeType: node.data.type,
            executedAt: new Date().toISOString()
          }
          
          console.log(`‚úÖ NODE COMPLETED: ${node.id} (sequence ${i + 1}/${executionOrder.length})`)
          console.log('  - Output type:', nodeOutput.type)
          console.log('  - Content length:', typeof nodeOutput.content === 'string' ? nodeOutput.content.length : 'N/A')
          console.log('  - Pipeline now has:', Object.keys(pipelineData.nodeOutputs).length, 'completed nodes')
          
          // SURGICAL FIX: Only update lastNodeOutput if node produced actual content
          // Skip gates/routing nodes that don't have content - they shouldn't overwrite Content Writer's output
          const hasActualContent = nodeOutput.content && 
                                   (typeof nodeOutput.content === 'string' || 
                                    nodeOutput.allChapters || 
                                    nodeOutput.chapters ||
                                    nodeOutput.type === 'ai_generation')
          const isGateOrRouting = nodeOutput.type === 'routing' || nodeOutput.type === 'condition_result'
          
          if (hasActualContent && !isGateOrRouting) {
            pipelineData.lastNodeOutput = nodeOutput
            console.log(`‚úÖ UPDATED lastNodeOutput to ${node.id} (has content)`)
          } else {
            console.log(`‚è≠Ô∏è SKIPPING lastNodeOutput update for ${node.id} (${nodeOutput.type}) - no content or is routing node`)
          }
          
          console.log('  - Pipeline data after execution:', pipelineData)
          
          // Update execution state
          const updatedState = {
            [`results.${node.id}`]: nodeOutput,
            nodeOutputs: pipelineData.nodeOutputs,
            currentNodeIndex: i,
            completedNodes: executionOrder.slice(0, i + 1).map(n => n.id)
          }
          this.updateExecutionState(workflowId, updatedState)

          // CREATE CHECKPOINT AFTER NODE COMPLETION
          this.createCheckpoint(workflowId, node.id, {
            ...this.executionState.get(workflowId),
            ...updatedState
          })

          // PROPER COMPLETION PROGRESS - DYNAMIC CALCULATION
          const completionProgress = ((i + 1) / executionOrder.length) * 100
          
          if (progressCallback) {
            progressCallback({
              nodeId: node.id,
              nodeName: node.data.label,
              progress: completionProgress, // DYNAMIC: Shows actual completion
              status: 'completed',
              output: nodeOutput,
              nodeIndex: i + 1,
              totalNodes: executionOrder.length,
              isNodeComplete: true,
              checkpointCreated: true, // Indicate checkpoint was created
              // CRITICAL: Send updated nodeOutputs after this node completes
              nodeResults: pipelineData.nodeOutputs || {}
            })
          }

        } catch (error) {
          const nodeError = {
            nodeId: node.id,
            nodeName: node.data.label,
            error: error.message,
            timestamp: new Date()
          }
          
          // CRITICAL: Save checkpoint data to DB for resume functionality
          const currentState = this.executionState.get(workflowId)
          const checkpointData = {
            nodeId: node.id,
            nodeName: node.data.label,
            nodeIndex: i,
            nodeOutputs: pipelineData.nodeOutputs || {},
            userInput: pipelineData.userInput,
            executionOrder: executionOrder.map(n => ({ id: n.id, type: n.data.type })),
            completedNodes: executionOrder.slice(0, i).map(n => n.id),
            failedAtNode: node.id,
            timestamp: new Date().toISOString()
          }
          
          // Save checkpoint to database
          try {
            const supabase = getSupabase()
            await supabase
              .from('engine_executions')
              .update({
                execution_data: {
                  ...(currentState?.executionData || {}),
                  resumable: true,
                  checkpointData: checkpointData,
                  failedNodeId: node.id,
                  failedNodeName: node.data.label,
                  error: error.message,
                  status: 'failed',
                  nodeResults: pipelineData.nodeOutputs || {}
                }
              })
              .eq('id', workflowId)
            console.log(`üíæ Checkpoint saved to DB for resume - failed at node: ${node.id}`)
          } catch (dbError) {
            console.error('‚ùå Failed to save checkpoint to DB:', dbError)
          }
          
          // PAUSE on failure instead of stopping - allow user to fix and resume
          this.updateExecutionState(workflowId, {
            status: 'paused',
            failedNodeId: node.id,
            failedNodeName: node.data.label,
            pauseReason: 'node_failure',
            resumable: true,
            checkpointData: checkpointData,
            [`errors`]: [...(this.executionState.get(workflowId)?.errors || []), nodeError]
          })

          if (progressCallback) {
            progressCallback({
              nodeId: node.id,
              nodeName: node.data.label,
              progress,
              status: 'failed',
              error: error.message,
              pauseReason: 'node_failure',
              resumable: true,
              message: `Node failed - workflow can be resumed from previous node.`,
              // Send nodeOutputs even on failure for partial results
              nodeResults: pipelineData.nodeOutputs || {}
            })
          }

          console.log(`‚è∏Ô∏è Workflow ${workflowId} paused due to node failure: ${node.data.label}`)
          
          // Wait for user to fix the issue and resume
          await this.waitForResume(workflowId)
          
          // After resume, retry the failed node or continue
          console.log(`‚ñ∂Ô∏è Workflow ${workflowId} resumed after node failure fix`)
          
          // Retry the current node after resume
          i-- // Decrement to retry the same node
          continue
        }
      }

      // Mark execution as completed
      this.updateExecutionState(workflowId, {
        status: 'completed',
        endTime: new Date(),
        progress: 100
      })

      // SURGICAL FIX: Build storyContext from compiled content for frontend display
      // This ensures chapters appear in AI Thinking Modal and downloads work
      let storyContext = null
      try {
        // Compile content to get sections (chapters)
        const compiledContent = this.compileWorkflowContent(pipelineData.nodeOutputs, pipelineData.userInput)
        
        // Build storyContext from compiled sections - DYNAMIC for all engines
        if (compiledContent && compiledContent.sections && compiledContent.sections.length > 0) {
          // Convert sections to chapters array
          const chapters = compiledContent.sections
            .filter(section => section && section.content && typeof section.content === 'string' && section.content.trim().length > 0)
            .map((section, idx) => {
              const chapterNumber = section.chapterNumber || section.metadata?.chapterNumber || idx + 1
              const title = section.title || section.metadata?.title || `Chapter ${chapterNumber}`
              const content = typeof section.content === 'string' ? section.content : String(section.content || '')
              
              return {
                id: section.nodeId || `chapter_${chapterNumber}`,
                number: chapterNumber,
                chapterNumber: chapterNumber,
                title: title,
                content: content,
                words: section.words || content.split(/\s+/).filter(Boolean).length,
                nodeId: section.nodeId,
                metadata: {
                  ...(section.metadata || {}),
                  contentType: section.contentType || 'chapter',
                  source: 'compiled_sections'
                }
              }
            })
          
          // Build structural content (foreword, intro, TOC)
          const structural = {
            foreword: compiledContent.structural?.foreword || null,
            introduction: compiledContent.structural?.introduction || null,
            tableOfContents: compiledContent.structural?.tableOfContents || null,
            tableOfContentsList: compiledContent.structural?.tableOfContentsList || []
          }
          
          // Build assets (images, cover)
          const assets = {
            images: compiledContent.assets?.images || [],
            cover: compiledContent.assets?.cover || null
          }
          
          storyContext = {
            chapters: chapters,
            structural: structural,
            assets: assets,
            metadata: {
              totalChapters: chapters.length,
              totalWords: compiledContent.totalWords || 0,
              totalCharacters: compiledContent.totalCharacters || 0,
              compiledAt: new Date().toISOString()
            }
          }
          
          console.log(`üìö Built storyContext: ${chapters.length} chapters, ${compiledContent.totalWords} words`)
        } else {
          console.warn('‚ö†Ô∏è No sections found in compiled content - storyContext will be null')
        }
      } catch (storyContextError) {
        console.error('‚ùå Error building storyContext:', storyContextError)
        // Don't fail execution - storyContext is optional for display
      }
      
      // Add storyContext to pipelineData for frontend
      if (storyContext) {
        pipelineData.storyContext = storyContext
      }

      // ENSURE FINAL 100% COMPLETION CALLBACK
      if (progressCallback) {
        progressCallback({
          status: 'completed',
          progress: 100, // GUARANTEED 100% completion
          message: 'Workflow execution completed successfully',
          nodeId: 'workflow-complete',
          nodeName: 'Workflow Complete',
          output: {
            success: true,
            results: pipelineData.nodeOutputs,
            lastNodeOutput: pipelineData.lastNodeOutput,
            nodeOutputs: pipelineData.nodeOutputs, // For deliverables access
            storyContext: storyContext, // Include storyContext in final callback
            metadata: {
              totalNodes: executionOrder.length,
              executionTime: Date.now() - startTime,
              workflowId,
              completedNodes: executionOrder.length,
              successRate: 100
            }
          }
        })
      }

      return pipelineData

    } catch (error) {
      // Save final error state with checkpoint data for resume
      const currentState = this.executionState.get(workflowId)
      if (currentState?.nodeOutputs) {
        try {
          const supabase = getSupabase()
          await supabase
            .from('engine_executions')
            .update({
              execution_data: {
                ...(currentState?.executionData || {}),
                resumable: true,
                checkpointData: {
                  nodeOutputs: currentState.nodeOutputs,
                  userInput: currentState.userInput,
                  completedNodes: currentState.completedNodes || [],
                  failedAtNode: currentState.failedNodeId || 'unknown',
                  timestamp: new Date().toISOString()
                },
                error: error.message,
                status: 'failed'
              }
            })
            .eq('id', workflowId)
          console.log(`üíæ Final checkpoint saved to DB for resume`)
        } catch (dbError) {
          console.error('‚ùå Failed to save final checkpoint:', dbError)
        }
      }
      
      this.updateExecutionState(workflowId, {
        status: 'error',
        endTime: new Date(),
        finalError: error.message
      })
      throw error
    }
  }

  /**
   * Resume execution from a failed checkpoint
   * @param {string} executionId - Execution ID to resume
   * @param {Array} nodes - Workflow nodes
   * @param {Array} edges - Workflow edges
   * @param {Function} progressCallback - Progress callback
   * @returns {Object} Execution result
   */
  async resumeExecution(executionId, nodes, edges, progressCallback = null) {
    try {
      console.log(`üîÑ Resuming execution: ${executionId}`)
      
      // Load checkpoint data from database
      const supabase = getSupabase()
      const { data: executionData, error } = await supabase
        .from('engine_executions')
        .select('execution_data, input_data')
        .eq('id', executionId)
        .single()
      
      if (error) {
        throw new Error(`Failed to load execution data: ${error.message}`)
      }
      
      const checkpointData = executionData.execution_data?.checkpointData
      if (!checkpointData || !checkpointData.nodeOutputs) {
        throw new Error('No checkpoint data found - cannot resume')
      }
      
      console.log(`üì¶ Loaded checkpoint:`, {
        completedNodes: checkpointData.completedNodes?.length || 0,
        failedAt: checkpointData.failedAtNode
      })
      
      // Reconstruct pipeline data from checkpoint
      const pipelineData = {
        userInput: checkpointData.userInput || executionData.input_data || {},
        nodeOutputs: checkpointData.nodeOutputs || {},
        lastNodeOutput: null
      }
      
      // Get the last completed node output
      const completedNodeIds = checkpointData.completedNodes || []
      if (completedNodeIds.length > 0) {
        const lastCompletedNodeId = completedNodeIds[completedNodeIds.length - 1]
        pipelineData.lastNodeOutput = checkpointData.nodeOutputs[lastCompletedNodeId] || null
      }
      
      // Calculate execution order
      const executionOrder = this.calculateExecutionOrder(nodes, edges)
      
      // Find the index to resume from (one step back from failed node)
      let resumeIndex = 0
      if (checkpointData.failedAtNode) {
        const failedNodeIndex = executionOrder.findIndex(n => n.id === checkpointData.failedAtNode)
        if (failedNodeIndex > 0) {
          resumeIndex = failedNodeIndex - 1 // Resume from previous node
          console.log(`‚èÆÔ∏è Resuming from node index ${resumeIndex} (one before failed node ${failedNodeIndex})`)
        } else if (failedNodeIndex === 0) {
          resumeIndex = 0 // Start from beginning if first node failed
        }
      } else {
        // No failed node specified - resume from last completed node
        if (completedNodeIds.length > 0) {
          const lastCompletedId = completedNodeIds[completedNodeIds.length - 1]
          resumeIndex = executionOrder.findIndex(n => n.id === lastCompletedId)
          if (resumeIndex === -1) resumeIndex = 0
        }
      }
      
      // Update execution state
      this.updateExecutionState(executionId, {
        status: 'running',
        nodeOutputs: pipelineData.nodeOutputs,
        resumed: true,
        resumedFromNode: executionOrder[resumeIndex]?.id || null
      })
      
      // Continue execution from resume point
      return await this.continueExecutionFromNode(
        executionId,
        nodes,
        edges,
        pipelineData.userInput,
        progressCallback,
        null, // superAdminUser
        resumeIndex,
        pipelineData.nodeOutputs
      )
      
    } catch (error) {
      console.error('‚ùå Resume failed:', error)
      throw error
    }
  }

  /**
   * Check if a node can be skipped based on content quality and workflow state
   */
  canSkipNode(node, pipelineData, workflowId = null) {
    // Skip Editor nodes if content is already high quality
    if (node.type === 'editor' && pipelineData.lastNodeOutput?.content) {
      const qualityCheck = this.assessContentQuality(pipelineData.lastNodeOutput.content)
      if (qualityCheck.isHighQuality) {
        console.log(`‚è≠Ô∏è Skipping ${node.type} node - content already high quality (${qualityCheck.score}/100)`)
        return {
          skip: true,
          reason: 'Content already meets quality standards',
          qualityScore: qualityCheck.score
        }
      }
    }

    // Skip duplicate content writer nodes if content already exists
    if (node.type === 'content_writer' && pipelineData.lastNodeOutput?.chapters?.length > 0) {
      const existingChapters = pipelineData.lastNodeOutput.chapters
      const targetChapters = pipelineData.chapter_count || pipelineData.totalChapters || 8
      
      if (existingChapters.length >= targetChapters) {
        console.log(`‚è≠Ô∏è Skipping ${node.type} node - sufficient chapters already generated (${existingChapters.length}/${targetChapters})`)
        return {
          skip: true,
          reason: 'Sufficient chapters already generated',
          existingChapters: existingChapters.length
        }
      }
    }

    // Skip research nodes if research data already exists
    if (node.type === 'researcher' && pipelineData.lastNodeOutput?.researchData) {
      console.log(`‚è≠Ô∏è Skipping ${node.type} node - research data already available`)
      return {
        skip: true,
        reason: 'Research data already available'
      }
    }

    return { skip: false }
  }

  /**
   * Execute a single node with real AI processing
   * @param {Object} node - Node to execute
   * @param {Object} pipelineData - Current pipeline data
   * @param {string} workflowId - Workflow execution ID
   * @returns {Object} Node output
   */
  async executeNode(node, pipelineData, workflowId, progressCallback = null) {
    console.log(`üîç EXECUTING NODE: ${node.id} (${node.type}) - ${node.data.label}`)
    console.log(`  - Pipeline data keys:`, Object.keys(pipelineData))
    console.log(`  - Last node output:`, pipelineData.lastNodeOutput ? 'EXISTS' : 'NONE')
    
    // Check if this node can be skipped for optimization
    const skipCheck = this.canSkipNode(node, pipelineData, workflowId)
    console.log(`  - Skip check result:`, skipCheck)
    
    if (skipCheck.skip) {
      console.log(`‚è≠Ô∏è SKIPPING NODE ${node.id} (${node.type}): ${skipCheck.reason}`)
      return {
        success: true,
        output: {
          content: pipelineData.lastNodeOutput?.content || '',
          skipped: true,
          skipReason: skipCheck.reason,
          ...skipCheck
        },
        metadata: {
          nodeType: node.type,
          processingTime: 0,
          skipped: true,
          skipReason: skipCheck.reason
        }
      }
    }
    const { type, data } = node
    
    switch (type) {
      case 'input':
        return await this.executeInputNode(data, pipelineData)
      
      case 'process':
        return await this.executeProcessNode(data, pipelineData, progressCallback, workflowId)
      
      case 'preview':
        return await this.executePreviewNode(data, pipelineData, progressCallback)
      
      case 'condition':
        return await this.executeConditionNode(data, pipelineData)
      
      case 'output':
        return await this.executeOutputNode(data, pipelineData)
      
      default:
        throw new Error(`Unknown node type: ${type}`)
    }
  }

  /**
   * Execute input node - validate and structure user input
   */
  async executeInputNode(nodeData, pipelineData) {
    const { userInput } = pipelineData
    const { testInputEnabled, testInputValues, processingInstructions } = nodeData

    // Use test input values if enabled, otherwise use regular userInput
    const inputToUse = testInputEnabled && testInputValues ? testInputValues : userInput
    
    console.log('üîç INPUT NODE JSON WRAPPER:')
    console.log('  - Using processingInstructions from nodePalettes.js')
    console.log('  - inputToUse:', inputToUse)

    // Create JSON wrapper as per nodePalettes.js instructions
    const jsonWrapper = {
      user_input: inputToUse,
      metadata: {
        node_id: nodeData.id || 'input-node',
        timestamp: new Date().toISOString(),
        status: 'processed',
        workflow_type: nodeData.role || 'universal'
      },
      next_node_data: {
        original_input: inputToUse,
        processing_instructions: 'All user data wrapped and ready for next node'
      }
    }

    console.log('üîç JSON WRAPPER CREATED:', jsonWrapper)

    return {
      type: 'input_json_wrapper',
      content: jsonWrapper,
      nodeName: nodeData.label || 'Input Processing',
      metadata: {
        nodeId: nodeData.id || 'input-node',
        nodeName: nodeData.label || 'Input Processing',
        timestamp: new Date(),
        wrapperCreated: true
      }
    }
  }

  /**
   * Execute process node - call AI with real API requests
   * DYNAMIC MULTI-CHAPTER SUPPORT: Reads userInput.chapterCount and follows node instructions
   */
  async executeProcessNode(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { 
      aiEnabled, 
      selectedModels, 
      systemPrompt, 
      userPrompt, 
      temperature, 
      maxTokens, 
      inputInstructions,
      processingInstructions
    } = nodeData
    
    // PERMISSION ENFORCEMENT: Get node role configuration with label-based fallback mapping
    let nodeRole = nodeData.role || nodeData.id
    
    // SURGICAL FIX: Map node labels to roles if role is missing
    if (!nodeData.role && nodeData.label) {
      const nodeLabel = (nodeData.label || '').toLowerCase()
      if (nodeLabel.includes('world-building') || nodeLabel.includes('world building') || nodeLabel.includes('character development')) {
        nodeRole = 'world_builder'
      } else if (nodeLabel.includes('plot architecture') || nodeLabel.includes('plot') || nodeLabel.includes('story structure')) {
        nodeRole = 'plot_architect'
      } else if (nodeLabel.includes('literary writing') || nodeLabel.includes('narrative craft') || nodeLabel.includes('content writer')) {
        nodeRole = 'content_writer'
      } else if (nodeLabel.includes('story architect') || nodeLabel.includes('story outline')) {
        nodeRole = 'story_outliner'
      } else if (nodeLabel.includes('editor') || nodeLabel.includes('proofread')) {
        nodeRole = 'editor'
      }
      console.log(`üîç LABEL-BASED ROLE MAPPING: "${nodeData.label}" ‚Üí ${nodeRole}`)
    }
    
    const roleConfig = NODE_ROLE_CONFIG[nodeRole]
    
    if (roleConfig) {
      console.log(`üîê PERMISSION CHECK for node "${nodeData.label}" (${nodeRole}):`)
      console.log(`   - canWriteContent: ${roleConfig.canWriteContent}`)
      console.log(`   - canEditStructure: ${roleConfig.canEditStructure}`)
      console.log(`   - canProofRead: ${roleConfig.canProofRead}`)
      
      // Store permissions in nodeData for AI prompt enforcement
      nodeData.permissions = {
        canWriteContent: roleConfig.canWriteContent,
        canEditStructure: roleConfig.canEditStructure,
        canProofRead: roleConfig.canProofRead
      }
    } else {
      console.warn(`‚ö†Ô∏è No role configuration found for node: ${nodeRole}`)
      // SECURITY-FIRST: Default to NO permissions (deny everything) - must explicitly configure roles
      nodeData.permissions = {
        canWriteContent: false,
        canEditStructure: false,
        canProofRead: false
      }
      console.warn(`   ‚ö†Ô∏è Node "${nodeData.label}" (${nodeRole}) defaulting to NO PERMISSIONS for security`)
    }
    
    if (!aiEnabled) {
      // Non-AI processing node
      return this.executeNonAIProcessing(nodeData, pipelineData)
    }

    // STEP 1: RECEIVE - Get previous node output and store in previousNodePassover
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    const { userInput } = pipelineData
    
    // STEP 2: STORE - Store complete previous node data in previousNodePassover
    const previousNodePassover = {
      previousOutput: previousOutput,
      originalUserInput: userInput,
      timestamp: new Date().toISOString(),
      nodeContext: 'stored_for_passover'
    }
    
    // Add previousNodePassover to pipelineData for template processing
    pipelineData.previousNodePassover = previousNodePassover
    // Backward-compatible alias expected by prompts and templates
    pipelineData.previous_node_output = previousNodePassover
    
    console.log('üì¶ PREVIOUS NODE PASSOVER: Stored previous data for context preservation')
    console.log('   - Previous output keys:', Object.keys(previousOutput || {}))
    console.log('   - User input keys:', Object.keys(userInput || {}))
    
    // CRITICAL: Extract user_input from JSON wrapper if available, otherwise fall back to userInput
    let structuredData = userInput
    
    if (previousOutput?.content?.user_input) {
      // Input node returned JSON wrapper - extract the user_input
      structuredData = previousOutput.content.user_input
      console.log('üîç PROCESS NODE: Using user_input from JSON wrapper:', structuredData)
    } else if (previousOutput?.structuredData) {
      // Legacy fallback for old structuredData format
      structuredData = previousOutput.structuredData
      console.log('üîç PROCESS NODE: Using legacy structuredData:', structuredData)
    } else {
      console.log('üîç PROCESS NODE: Using direct userInput:', structuredData)
    }
    

    // SURGICAL FIX: FORCE USER INPUT CHAPTER COUNT - Check user input FIRST, ignore preset contamination
    let chapterCount = null
    
    // Priority 1: Direct user input (highest priority)
    chapterCount = userInput.chapterCount || userInput.chapter_count || userInput['Chapter Count'] || userInput['Number of Chapters']
    
    // Priority 2: Structured data (from input node processing)  
    if (!chapterCount) {
      chapterCount = structuredData.chapterCount || structuredData.chapter_count || structuredData['Chapter Count'] || structuredData['Number of Chapters']
    }
    
    console.log('üîç CHAPTER COUNT EXTRACTION PRIORITY:')
    console.log('  - userInput.chapter_count:', userInput.chapter_count)
    console.log('  - userInput.chapterCount:', userInput.chapterCount)
    console.log('  - structuredData.chapter_count:', structuredData.chapter_count)
    console.log('  - FINAL chapterCount:', chapterCount)
    
    console.log('üîç CHAPTER COUNT RAW EXTRACTION:', chapterCount, 'Type:', typeof chapterCount)
    
    // Parse chapter count if it's a string like "2-3" or "6-8" 
    if (chapterCount && typeof chapterCount === 'string') {
      if (chapterCount.includes('-')) {
        // Take the higher number from ranges like "2-3" -> 3, "6-8" -> 8
        const parts = chapterCount.split('-')
        chapterCount = parseInt(parts[1]) || parseInt(parts[0]) || 1
      } else {
        chapterCount = parseInt(chapterCount) || 1
      }
    }
    
    console.log('üîç CHAPTER COUNT AFTER PARSING:', chapterCount, 'Type:', typeof chapterCount)
    
    if (!chapterCount || chapterCount < 1) {
      // NO AI DETERMINATION - Use sensible defaults based on content type
      const wordCount = parseInt(structuredData.word_count || structuredData['Word Count'] || 2000)
      
      // Simple logical defaults without AI calls
      if (wordCount <= 2000) {
        chapterCount = 3
      } else if (wordCount <= 5000) {
        chapterCount = 4
      } else if (wordCount <= 10000) {
        chapterCount = 6
      } else {
        chapterCount = 8
      }
      
      console.log('üîç DEFAULT CHAPTER ASSIGNMENT:')
      console.log('  - Word count:', wordCount)
      console.log('  - Default chapters:', chapterCount)
    } else {
      console.log('üîç USER-SPECIFIED CHAPTERS (RESPECTED):', chapterCount)
    }
    
    console.log('üîç CHAPTER COUNT DEBUG:')
    console.log('  - structuredData:', structuredData)
    console.log('  - structuredData.chapterCount:', structuredData.chapterCount)
    console.log('  - structuredData.chapter_count:', structuredData.chapter_count)
    console.log('  - structuredData["Chapter Count"]:', structuredData['Chapter Count'])
    console.log('  - userInput.chapterCount:', userInput.chapterCount)
    console.log('  - userInput.chapter_count:', userInput.chapter_count)
    console.log('  - userInput["Chapter Count"]:', userInput['Chapter Count'])
    console.log('  - Final chapterCount:', chapterCount)
    console.log('  - Type:', typeof chapterCount)
    
    // CRITICAL: Distinguish between content generation and content refinement
    // Use nodeRole from permission check above, with label-based fallback
    const nodeLabel = (nodeData.label || '').toLowerCase()
    const isLabelBasedWriter = nodeLabel.includes('writing') || 
                               nodeLabel.includes('literary') || 
                               nodeLabel.includes('narrative') || 
                               nodeLabel.includes('content writer') ||
                               nodeLabel.includes('technical writer') ||
                               nodeLabel.includes('copywriter')
    
    const isContentWriter = nodeRole === 'content_writer' || 
                           nodeRole === 'technical_writer' || 
                           nodeRole === 'copywriter' ||
                           isLabelBasedWriter
    
    const isEditor = nodeRole === 'editor'

    console.log(`üîç NODE ROLE CHECK: ${nodeRole}, isContentWriter: ${isContentWriter}, isEditor: ${isEditor}`)
    console.log(`üîç NODE LABEL: ${nodeData.label}`)
    console.log(`üîç LABEL-BASED WRITER DETECTION: ${isLabelBasedWriter}`)

    // SURGICAL FIX: Detect image generation nodes and handle differently
    const isImageNode = nodeRole === 'image_generator' || nodeRole === 'ecover_generator'
    
    if (isImageNode) {
      // Image generation: Call specialized image generation handler
      console.log(`üé® STARTING IMAGE GENERATION (${nodeRole})`)
      return await this.executeImageGeneration(nodeData, pipelineData, progressCallback, workflowId)
    } else if (parseInt(chapterCount) > 1 && isContentWriter) {
      // Multi-chapter generation: ONLY for content writing nodes
      console.log(`üîç STARTING MULTI-CHAPTER GENERATION: ${chapterCount} chapters`)
      return await this.generateMultipleChapters(nodeData, pipelineData, parseInt(chapterCount), progressCallback, workflowId)
    } else if (isEditor) {
      // Content refinement: Editor processes existing content with checklist
      console.log(`üîç STARTING CONTENT REFINEMENT (Editor node)`)
      return await this.executeContentRefinement(nodeData, pipelineData, progressCallback, workflowId)
    } else {
      // Single generation: For research, analysis, and other non-writing nodes
      console.log(`üîç STARTING SINGLE GENERATION (${isContentWriter ? 'content writer' : 'research/analysis node'})`)
      return await this.executeSingleAIGeneration(nodeData, pipelineData, progressCallback, workflowId)
    }
  }

  /**
   * Generate multiple chapters based on user input and node instructions
   * This follows the node's own instructions dynamically
   */
  async generateMultipleChapters(nodeData, pipelineData, chapterCount, progressCallback = null, workflowId = null) {
    const results = []
    
    console.log(`üîç GENERATING ${chapterCount} CHAPTERS`)
    
    for (let i = 1; i <= chapterCount; i++) {
        // CRITICAL: Check if workflow was stopped before each chapter
        if (workflowId && (this.isWorkflowStopped(workflowId) || await this.checkDatabaseStopSignal(workflowId))) {
        console.log(`üõë WORKFLOW STOPPED DURING CHAPTER GENERATION - PRESERVING ${results.length} COMPLETED CHAPTERS`)
        
        // Compile partial results into a book
        if (results.length > 0) {
          const nodeOutputs = this.convertResultsToNodeOutputs(results)
          const partialBookResult = await bookCompilationService.compileBook(nodeOutputs, pipelineData.userInput)
          const partialBook = partialBookResult.content
          return {
            type: 'partial_book',
            content: partialBook,
            metadata: {
              nodeId: nodeData.id,
              timestamp: new Date(),
              chaptersCompleted: results.length,
              totalChapters: chapterCount,
              stopped: true,
              partialGeneration: true
            },
            partialResults: results,
            stopped: true,
            message: `Workflow stopped. ${results.length} of ${chapterCount} chapters completed.`
          }
        } else {
          return {
            type: 'stopped_no_content',
            content: 'Workflow stopped before any chapters were generated.',
            metadata: {
              nodeId: nodeData.id,
              timestamp: new Date(),
              chaptersCompleted: 0,
              totalChapters: chapterCount,
              stopped: true
            },
            stopped: true,
            message: 'Workflow stopped before any content was generated.'
          }
        }
      }
      
      console.log(`üîç GENERATING CHAPTER ${i} OF ${chapterCount}`)
      
      // Update pipeline data with current chapter context
      const chapterPipelineData = {
        ...pipelineData,
        currentChapter: i,
        totalChapters: chapterCount,
        previousChapters: results
      }
      
      // Progress will be updated by executeSingleAIGeneration with actual metrics
      // No need for zero-metrics callback here
      
      // RETRY LOGIC: Attempt chapter generation with retries
      const MAX_RETRIES = 3
      let chapterResult = null
      let lastChapterError = null
      
      for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
        try {
          console.log(`üîÑ Chapter ${i} Generation Attempt ${attempt}/${MAX_RETRIES}`)
          
          // Create chapter-aware progressCallback wrapper
          const chapterProgressCallback = progressCallback ? (update) => {
            progressCallback({
              ...update,
              chapterInfo: {
                current: i,
                total: chapterCount,
                chapterStatus: update.status || 'generating'
              }
            })
          } : null
          
          // Generate this chapter using the node's instructions
          chapterResult = await this.executeSingleAIGeneration(nodeData, chapterPipelineData, chapterProgressCallback)
          
          // VALIDATE CHAPTER CONTENT BEFORE ACCEPTING - CRITICAL: Pass permissions to enforce restrictions
          const chapterValidation = aiResponseValidator.validateResponse(
            { content: chapterResult.content },
            'chapter',
            pipelineData,
            nodeData.permissions
          )
          
          if (!chapterValidation.isValid) {
            const validationErrors = chapterValidation.errors
              .map(e => `${e.code}: ${e.message}`)
              .join('; ')
            
            throw new Error(`Chapter validation failed: ${validationErrors}`)
          }
          
          // Chapter is valid - break retry loop
          console.log(`‚úÖ Chapter ${i} validated successfully`)
          break
          
        } catch (error) {
          lastChapterError = error
          console.error(`‚ùå Chapter ${i} Attempt ${attempt} failed:`, error.message)
          
          if (attempt < MAX_RETRIES) {
            // Exponential backoff: 2s, 4s, 8s
            const backoffDelay = Math.pow(2, attempt) * 1000
            console.log(`‚è≥ Waiting ${backoffDelay}ms before retry...`)
            await new Promise(resolve => setTimeout(resolve, backoffDelay))
          }
        }
      }
      
      // CRITICAL: If all retries failed, stop the workflow
      if (!chapterResult) {
        const errorMessage = `Chapter ${i} generation failed after ${MAX_RETRIES} attempts. Last error: ${lastChapterError?.message}`
        console.error(`üö® ${errorMessage}`)
        
        if (progressCallback) {
          progressCallback({
            nodeId: nodeData.id,
            nodeName: `${nodeData.label} (Failed)`,
            status: 'failed',
            error: errorMessage,
            chapterInfo: {
              currentChapter: i,
              totalChapters: chapterCount,
              chapterStatus: 'failed',
              attemptsBeforeFailing: MAX_RETRIES
            }
          })
        }
        
        throw new Error(errorMessage)
      }
      
      // Push validated chapter to results
      results.push({
        chapter: i,
        content: chapterResult.content,
        metadata: chapterResult.metadata,
        aiMetadata: chapterResult.aiMetadata
      })
      
      console.log(`‚úÖ CHAPTER ${i} COMPLETED: ${chapterResult.content.length} characters`)
      
      // Update progress callback with chapter completion
      if (progressCallback) {
        const chapterCompleteProgress = (i / chapterCount) * 100
        progressCallback({
          nodeId: nodeData.id,
          nodeName: `${nodeData.label} (Chapter ${i}/${chapterCount} Complete)`,
          status: i === chapterCount ? 'completed' : 'executing',
          progress: Math.round(chapterCompleteProgress),
          providerName: chapterResult.aiMetadata?.provider || null,
          timestamp: new Date().toLocaleTimeString(),
          cost: chapterResult.aiMetadata?.cost || 0,
          tokens: chapterResult.aiMetadata?.tokens || 0,
          words: chapterResult.aiMetadata?.words || 0,
          chapterInfo: {
            currentChapter: i,
            totalChapters: chapterCount,
            chapterStatus: 'completed',
            chapterWordCount: chapterResult.aiMetadata?.words || 0
          }
        })
      }
      
        // CRITICAL: Check if workflow was stopped after each chapter
        if (workflowId && (this.isWorkflowStopped(workflowId) || await this.checkDatabaseStopSignal(workflowId))) {
        console.log(`üõë WORKFLOW STOPPED AFTER CHAPTER ${i} - PRESERVING ${results.length} COMPLETED CHAPTERS`)
        
        // Compile partial results into a book
        const nodeOutputs = this.convertResultsToNodeOutputs(results)
      const partialBookResult = await bookCompilationService.compileBook(nodeOutputs, pipelineData.userInput)
      const partialBook = partialBookResult.content
        return {
          type: 'partial_book',
          content: partialBook,
          metadata: {
            nodeId: nodeData.id,
            timestamp: new Date(),
            chaptersCompleted: results.length,
            totalChapters: chapterCount,
            stopped: true,
            partialGeneration: true
          },
          partialResults: results,
          stopped: true,
          message: `Workflow stopped. ${results.length} of ${chapterCount} chapters completed.`
        }
      }
      
      // Add small delay between chapters to prevent rate limiting
      if (i < chapterCount) {
        await new Promise(resolve => setTimeout(resolve, 1000))
      }
    }
    
    // Compile all chapters into complete book
    const nodeOutputs = this.convertResultsToNodeOutputs(results)
    const completeBookResult = await bookCompilationService.compileBook(nodeOutputs, pipelineData.userInput)
    const completeBook = completeBookResult.content
    
    // Calculate total metrics
    const totalWords = results.reduce((sum, result) => sum + (result.aiMetadata?.words || 0), 0)
    const totalTokens = results.reduce((sum, result) => sum + (result.aiMetadata?.tokens || 0), 0)
    const totalCost = results.reduce((sum, result) => sum + (result.aiMetadata?.cost || 0), 0)
    
    return {
      type: 'multi_chapter_generation',
      content: completeBook,
      chapters: results,
      aiMetadata: {
        model: nodeData.selectedModels[0],
        totalChapters: chapterCount,
        totalWords: totalWords,
        totalTokens: totalTokens,
        totalCost: totalCost
      },
      metadata: {
        nodeId: nodeData.id || 'process-node',
        timestamp: new Date(),
        chapterCount: chapterCount,
        totalCharacters: completeBook.length
      },
      structuredData: {
        ...pipelineData.userInput,
        // Don't pass completeBook to subsequent nodes - only pass current chapter data
        currentChapter: chapterCount, // Mark this as the final chapter
        totalChapters: chapterCount,
        totalWords: totalWords,
        // Store chapters separately for final compilation only
        _chapters: results
      }
    }
  }

  /**
   * Execute single AI generation - used for both single chapters and individual chapters in multi-chapter mode
   */
  async executeSingleAIGeneration(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { selectedModels, systemPrompt, userPrompt, temperature, maxTokens, inputInstructions } = nodeData
    
    // Get previous node output
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    // Ensure previousNodePassover is available for template processing
    if (!pipelineData.previousNodePassover) {
      pipelineData.previousNodePassover = previousOutput || pipelineData.userInput
    }
    
    // Build dynamic prompt with real data substitution
    const processedPrompts = this.processPromptVariables({
      systemPrompt,
      userPrompt
    }, pipelineData, nodeData.permissions)

    // Get allData for word count enforcement
    const { userInput, nodeOutputs, lastNodeOutput } = pipelineData
    
    // Extract user_input from JSON wrapper if available
    let structuredData = userInput
    if (lastNodeOutput?.content?.user_input) {
      structuredData = lastNodeOutput.content.user_input
    } else if (lastNodeOutput?.structuredData) {
      structuredData = lastNodeOutput.structuredData
    }
    
    // Filter out completeBook and other compilation data to prevent passing to subsequent nodes
    const filteredStructuredData = { ...structuredData }
    delete filteredStructuredData.completeBook
    delete filteredStructuredData._chapters
    
    const allData = { 
      ...userInput, 
      ...filteredStructuredData,
      currentChapter: pipelineData.currentChapter,
      totalChapters: pipelineData.totalChapters,
      previousChapters: pipelineData.previousChapters
    }

    // Debug selectedModels
    console.log('üîç Selected models for AI generation:', selectedModels)
    console.log('üîç First selected model:', selectedModels[0])
    
    if (!selectedModels || selectedModels.length === 0) {
      throw new Error('No AI models selected for this node. Please configure AI Integration in the node modal.')
    }
    
      // CRITICAL: Check if workflow was stopped before AI call
      if (workflowId && (this.isWorkflowStopped(workflowId) || await this.checkDatabaseStopSignal(workflowId))) {
      console.log(`üõë WORKFLOW STOPPED BEFORE AI CALL - RETURNING STOPPED RESULT`)
      return {
        type: 'stopped_before_ai',
        content: 'Workflow stopped before AI generation could complete.',
        metadata: {
          nodeId: nodeData.id,
          timestamp: new Date(),
          stopped: true
        },
        stopped: true,
        message: 'Workflow stopped before AI generation.'
      }
    }

    // Get AI service for the selected model
    console.log('üîç DEBUG selectedModels:', selectedModels)
    console.log('üîç DEBUG selectedModels[0]:', selectedModels[0])
    console.log('üîç DEBUG selectedModels[0] type:', typeof selectedModels[0])
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    console.log('üîç Parsed model config:', modelConfig)
    const aiServiceInstance = this.getAIService(modelConfig.provider)

    // Add timeout to prevent infinite loops - increased for book generation
    // Use dynamic timeout based on content type
    const isBookGeneration = nodeData.label?.toLowerCase().includes('writing') || 
                           nodeData.label?.toLowerCase().includes('narrative') ||
                           nodeData.label?.toLowerCase().includes('literary')
    
    const timeoutDuration = isBookGeneration ? 1800000 : 600000 // 30 minutes for book generation, 10 minutes for others
    const timeoutPromise = new Promise((_, reject) => {
      setTimeout(() => reject(new Error(`AI generation timeout after ${timeoutDuration / 60000} minutes`)), timeoutDuration)
    })

    // Progress will be updated after AI generation with actual metrics
    // Removed zero-metrics callback to prevent overwriting real data

    if (!aiServiceInstance) {
      throw new Error(`AI service not available for provider: ${modelConfig.provider}`)
    }

    try {
      // Build final prompt combining system and user prompts
      let finalPrompt = processedPrompts.systemPrompt 
        ? `${processedPrompts.systemPrompt}\n\n${processedPrompts.userPrompt}`
        : processedPrompts.userPrompt

      // CELEBRITY STYLE INJECTION: Inject celebrity writing style characteristics if specified
      const celebrityStyleValue = allData.celebrity_style || userInput.celebrity_style
      if (celebrityStyleValue) {
        const celebrityData = getCelebrityStyle(celebrityStyleValue)
        if (celebrityData && celebrityData.styleGuide) {
          console.log(`üé≠ CELEBRITY STYLE ACTIVATED: ${celebrityData.label}`)
          finalPrompt = `${finalPrompt}\n\nüé≠ WRITING STYLE DIRECTIVE:\n${celebrityData.styleGuide}`
        }
      }

      // PERMISSION ENFORCEMENT: Prevent non-writer nodes from generating narrative chapters
      if (!nodeData.permissions?.canWriteContent) {
        finalPrompt += `\n\n‚ö†Ô∏è CRITICAL RESTRICTION - YOU ARE A PLANNING/RESEARCH NODE:
- DO NOT write narrative chapters, story prose, or complete book content
- DO NOT generate "Chapter 1", "Chapter 2", etc.
- ONLY provide: structural plans, outlines, architecture, research data, analysis
- Keep output concise and structured - this is planning/reference material
- Your output will be used by content writer nodes to create the actual narrative`
        console.log(`üîí PERMISSION RESTRICTION: Node "${nodeData.label}" (canWriteContent: false) - blocked from narrative generation`)
      }

      // SURGICAL FIX: Robust word count extraction with range handling
      let rawWordCount = allData.word_count || allData['Word Count'] || '2000'
      let rawChapterCount = allData.chapter_count || allData['Chapter Count'] || '1'
      
      // Handle word count ranges like "12000-20000" - take the higher number
      let wordCount = 2000 // Default fallback
      if (rawWordCount && typeof rawWordCount === 'string') {
        if (rawWordCount.includes('-')) {
          const parts = rawWordCount.split('-')
          wordCount = parseInt(parts[1]) || parseInt(parts[0]) || 2000
        } else {
          wordCount = parseInt(rawWordCount) || 2000
        }
      } else if (rawWordCount && typeof rawWordCount === 'number') {
        wordCount = rawWordCount
      }
      
      // Handle chapter count
      let chapterCount = parseInt(rawChapterCount) || 1
      
      // Calculate words per chapter with safety check
      const wordsPerChapter = Math.max(Math.floor(wordCount / chapterCount), 500) // Minimum 500 words per chapter
      
      console.log('üîç WORD COUNT CALCULATION DEBUG:')
      console.log('  - rawWordCount:', rawWordCount)
      console.log('  - rawChapterCount:', rawChapterCount)
      console.log('  - parsed wordCount:', wordCount)
      console.log('  - parsed chapterCount:', chapterCount)
      console.log('  - calculated wordsPerChapter:', wordsPerChapter)
      
      // DYNAMIC MAX TOKENS CALCULATION - Based on TOTAL output, not per-chapter
      // AI decides individual chapter lengths naturally, we just give it budget for the full output
      const totalWordsTarget = parseInt(wordCount)
      const dynamicMaxTokens = Math.ceil(totalWordsTarget * 1.3 * 1.5) // 50% buffer for full book
      
      // MODEL LIMITS - Cap at model's maximum to prevent API errors
      const MODEL_MAX_TOKENS = {
        'claude-3-5-haiku-20241022': 8192,
        'claude-3-5-sonnet-20241022': 8192,
        'claude-3-opus-20240229': 4096,
        'gpt-4': 8192,
        'gpt-4-turbo': 4096,
        'gpt-4o': 16384,
        'gpt-3.5-turbo': 4096,
        'gemini-pro': 8192,
        'gemini-1.5-pro': 8192,
        'gemini-1.5-flash': 8192,
        'gemini-2.0-flash': 8192,
        'mistral-medium': 8192,
        'mistral-large': 8192
      }
      
      const modelLimit = MODEL_MAX_TOKENS[modelConfig.model] || 4096 // Default 4096 if unknown
      const finalMaxTokens = modelLimit // NO RESTRICTIONS - Use full model capacity
      
      console.log('üîç DYNAMIC MAX TOKENS CALCULATION:')
      console.log('  - wordCount:', wordCount)
      console.log('  - chapterCount:', chapterCount)
      console.log('  - wordsPerChapter:', wordsPerChapter)
      console.log('  - dynamicMaxTokens:', dynamicMaxTokens)
      console.log('  - model:', modelConfig.model)
      console.log('  - modelLimit:', modelLimit)
      console.log('  - finalMaxTokens (capped):', finalMaxTokens)
      
      // DYNAMIC INPUT PROCESSING - Include ALL variables from ANY flow
      const dynamicInputs = []
      const excludedKeys = ['currentChapter', 'totalChapters', 'previousChapters', 'word_count', 'chapter_count', 'Word Count', 'Chapter Count']
      
      Object.entries(allData).forEach(([key, value]) => {
        if (value && !excludedKeys.includes(key) && typeof value === 'string' && value.trim() !== '') {
          const formattedKey = key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase())
          dynamicInputs.push(`- ${formattedKey}: ${value}`)
        }
      })

      const wordCountEnforcement = `

CRITICAL BOOK STRUCTURE REQUIREMENTS:
- TOTAL WORD COUNT: ${wordCount} words (NON-NEGOTIABLE)
- CHAPTER COUNT: ${chapterCount} chapters
- WORDS PER CHAPTER: ${wordsPerChapter} words (¬±10%)
- CURRENT CHAPTER: ${allData.currentChapter || 1} of ${chapterCount}

DYNAMIC USER INPUTS (ALL PROVIDED INFORMATION):
${dynamicInputs.length > 0 ? dynamicInputs.join('\n') : '- No specific inputs provided'}

CRITICAL ORCHESTRATION REQUIREMENTS:
- YOU MUST GENERATE A COMPLETE, PROFESSIONAL BOOK CHAPTER
- INCLUDE PROPER CHAPTER TITLE AND STRUCTURE
- USE ALL PROVIDED INPUTS ABOVE TO CREATE RELEVANT CONTENT
- INCORPORATE ALL USER-SPECIFIED DETAILS INTO THE NARRATIVE

CHAPTER STRUCTURE REQUIREMENTS:
- START WITH CHAPTER TITLE: "Chapter ${allData.currentChapter || 1}: [Descriptive Title]"
- INCLUDE PROPER INTRODUCTION TO THE CHAPTER
- DEVELOP MAIN CONTENT WITH CLEAR SECTIONS
- INCLUDE PRACTICAL EXAMPLES AND APPLICATIONS
- END WITH CHAPTER SUMMARY AND TRANSITION TO NEXT CHAPTER

WORD COUNT ENFORCEMENT:
- YOU MUST GENERATE EXACTLY ${wordsPerChapter} WORDS FOR THIS CHAPTER
- COUNT YOUR WORDS AND ENSURE YOU MEET THE REQUIREMENT
- NO LESS, NO MORE - EXACTLY ${wordsPerChapter} WORDS

CHAPTER CONTEXT:
- YOU ARE WRITING CHAPTER ${allData.currentChapter || 1} OF ${chapterCount} TOTAL CHAPTERS
- ${allData.currentChapter > 1 ? 'BUILD UPON PREVIOUS CHAPTERS WITHOUT REPEATING CONTENT' : 'ESTABLISH THE FOUNDATION FOR THE ENTIRE BOOK'}

${narrativeStructureService.buildChapterContext(
  allData.currentChapter || 1,
  allData.totalChapters || chapterCount,
  allData.previousChapters || [],
  allData
)}

CUSTOM INSTRUCTIONS: ${allData.custom_instructions || 'Generate comprehensive, professional content that provides real value to the target audience.'}`

      finalPrompt = finalPrompt + wordCountEnforcement
      
      // Debug final prompt
      console.log('üìù Final Prompt Debug:')
      console.log('  - System prompt:', processedPrompts.systemPrompt)
      console.log('  - User prompt:', processedPrompts.userPrompt)
      console.log('  - Dynamic inputs included:', dynamicInputs.length)
      console.log('  - Dynamic inputs:', dynamicInputs)
      console.log('  - Final prompt length:', finalPrompt.length)
      console.log('  - Final prompt preview:', finalPrompt.substring(0, 500))
      
      // Set SuperAdmin user in aiService to load API keys from database FIRST
      // The SuperAdmin user should be passed from the calling component
      if (pipelineData.superAdminUser) {
        await aiServiceInstance.setUser(pipelineData.superAdminUser)
      } else {
        throw new Error('SuperAdmin user not provided for AI service')
      }
      
      // Check if provider is available and has API key AFTER loading providers
      if (!aiServiceInstance.providers[modelConfig.providerName]) {
        throw new Error(`Provider ${modelConfig.providerName} not available. Please check API key configuration.`)
      }
      
      const providerConfig = aiServiceInstance.providers[modelConfig.providerName]
      if (!providerConfig.apiKey) {
        throw new Error(`No API key configured for ${modelConfig.providerName}. Please add API key in SuperAdmin settings.`)
      }
      
      // Make REAL AI API call using aiService with specific provider and maxTokens
      // Wrap with timeout to prevent infinite loops
      let aiResponse
      
      try {
        aiResponse = await Promise.race([
          aiServiceInstance.generateContent(finalPrompt, modelConfig.providerName, finalMaxTokens, modelConfig.model),
          timeoutPromise
        ])
        console.log('‚úÖ AI generation succeeded:', modelConfig.providerName, modelConfig.model)
      } catch (error) {
        console.error(`‚ùå AI generation failed with ${modelConfig.providerName} (${modelConfig.model}):`, error.message)
        
        // NO FALLBACKS - Report the actual error
        const errorDetails = {
          provider: modelConfig.providerName,
          model: modelConfig.model,
          error: error.message,
          errorType: error.name || 'AIProviderError',
          timestamp: new Date().toISOString()
        }
        
        console.error('‚ùå AI generation error details:', errorDetails)
        
        // Build actionable error message
        let errorMessage = `AI generation failed with ${modelConfig.providerName} (${modelConfig.model}): `
        if (error.message?.includes('rate limit')) {
          errorMessage += `Rate limit exceeded. Please try again later or check your API key limits.`
        } else if (error.message?.includes('timeout')) {
          errorMessage += `Request timeout. The content may be too complex or the API is slow.`
        } else if (error.message?.includes('invalid')) {
          errorMessage += `Invalid request. Please check your API configuration and content format.`
        } else if (error.message?.includes('authentication')) {
          errorMessage += `Authentication failed. Please check your API keys.`
        } else {
          errorMessage += error.message || 'Unknown error'
        }
        
        throw new Error(errorMessage)
      }

      // VALIDATE AI RESPONSE - NO GARBAGE ALLOWED
      console.log('üîç Validating AI response quality...')
      console.log('üîç AI Response BEFORE validation:', JSON.stringify(aiResponse).substring(0, 500))
      console.log('üîç AI Response type:', typeof aiResponse)
      console.log('üîç AI Response keys:', Object.keys(aiResponse || {}))

      // Determine expected content type based on node hints and role
      const nodeLabelLower = String(nodeData.label || '').toLowerCase()
      const nodeTypeLower = String(nodeData.type || nodeData.nodeType || nodeData.role || '').toLowerCase()
      const outputFormat = String(nodeData.outputFormat || '').toLowerCase()
      const contentType = String(nodeData.contentType || '').toLowerCase()
      
      // SURGICAL FIX: Determine content type based on node role and permissions
      // Non-writing nodes should NOT be validated as 'chapter' content
      const wantsOutline =
        nodeLabelLower.includes('outline') ||
        nodeTypeLower.includes('outline') ||
        outputFormat === 'outline' ||
        contentType === 'outline'
      
      // Check if this is a non-content-writing node (should not generate chapters)
      const isNonWritingNode = nodeData.permissions && nodeData.permissions.canWriteContent === false
      const isWorldBuilder = nodeTypeLower.includes('world') || nodeLabelLower.includes('world-building') || nodeLabelLower.includes('world building')
      const isPlotArchitect = nodeTypeLower.includes('plot') || nodeLabelLower.includes('plot architecture') || nodeLabelLower.includes('plot')
      const isCharacterDev = nodeLabelLower.includes('character development') || nodeLabelLower.includes('character')
      const isStoryOutliner = nodeTypeLower.includes('outliner') || nodeLabelLower.includes('story architect') || nodeLabelLower.includes('story outline')
      
      // Determine final content type based on node function
      let finalContentType = 'chapter' // Default
      
      // CRITICAL: Image nodes should NOT be validated with text content rules
      const isImageNode = nodeLabelLower.includes('image') || nodeLabelLower.includes('generator') || nodeData.type === 'image'
      
      if (isImageNode) {
        finalContentType = 'image_metadata' // Special type for image nodes
      } else if (wantsOutline || isStoryOutliner) {
        finalContentType = 'outline'
      } else if (isWorldBuilder || isPlotArchitect || isCharacterDev) {
        // Non-writing nodes generate data/structures, not chapters
        finalContentType = 'world_data' // Special type for non-chapter content
      } else if (isNonWritingNode) {
        // Any other non-writing node - use generic data type
        finalContentType = 'data'
      }
      
      console.log('üîç Content type detection debug:')
      console.log('  - Node label:', nodeData.label)
      console.log('  - Node type:', nodeData.type)
      console.log('  - Output format:', nodeData.outputFormat)
      console.log('  - Content type (from nodeData):', nodeData.contentType)
      console.log('  - Wants outline:', wantsOutline)
      console.log('  - Final content type for validator:', finalContentType)

      const validation = aiResponseValidator.validateResponse(aiResponse, finalContentType, allData, nodeData.permissions)
      
      // Debug AI response
      console.log('ü§ñ AI Response Debug:')
      console.log('  - Raw response type:', typeof aiResponse)
      console.log('  - Validation result:', validation.isValid ? '‚úÖ VALID' : '‚ùå INVALID')
      console.log('  - Errors:', validation.errors)
      console.log('  - Warnings:', validation.warnings)
      console.log('  - Extracted content length:', validation.content?.length || 0)
      console.log('  - Content preview:', validation.content?.substring(0, 200))
      
      // CRITICAL: Reject invalid responses - NO BAND-AIDS
      if (!validation.isValid) {
        const errorReport = aiResponseValidator.createErrorReport(validation)
        console.error('‚ùå AI Response Validation Failed:', errorReport)
        
        const errorMessage = validation.errors
          .map(e => `${e.code}: ${e.message}`)
          .join('; ')
        
        throw new Error(`AI response validation failed: ${errorMessage}. Recommendation: ${errorReport.recommendation}`)
      }
      
      // Use validated content
      const content = validation.content
      
      // Calculate real metrics with actual AI response data
      const actualWordCount = typeof content === 'string' ? content.split(' ').length : 0
      
      // Get actual token count from AI response usage data
      let actualTokens = 0
      let actualCost = 0
      
      if (aiResponse && aiResponse.usage) {
        // Get actual token count from AI provider response
        actualTokens = aiResponse.usage.total_tokens || 
                      aiResponse.usage.completion_tokens || 
                      aiResponse.token_count ||
                      Math.ceil(actualWordCount * 1.3) // Fallback estimation
        
        // Calculate cost using actual model pricing
        const modelCostPer1k = modelConfig.costPer1k || 0.00003
        actualCost = (actualTokens / 1000) * modelCostPer1k
      } else {
        // Fallback to estimation if no usage data
        actualTokens = Math.ceil(actualWordCount * 1.3)
        actualCost = (actualTokens / 1000) * (modelConfig.costPer1k || 0.00003)
      }
      
      // Send progress update with AI response data
      if (progressCallback) {
        progressCallback({
          nodeId: nodeData.id,
          nodeName: nodeData.label || 'AI Generation',
          status: 'processing',
          progress: 75, // Processing AI response
          providerName: modelConfig.providerName,
          timestamp: new Date().toLocaleTimeString(),
          cost: actualCost,
          tokens: actualTokens,
          words: actualWordCount,
          aiResponse: aiResponse,
          processedContent: content,
          rawData: {
            model: selectedModels[0],
            provider: modelConfig.provider,
            providerName: modelConfig.providerName,
            temperature: temperature || 0.7,
            maxTokens: maxTokens || 8000,
            systemPrompt: processedPrompts.systemPrompt,
            userPrompt: processedPrompts.userPrompt,
            inputData: allData,
            dynamicInputs: Object.keys(allData),
            modelCostPer1k: modelConfig.costPer1k,
            actualTokens: actualTokens,
            actualCost: actualCost
          }
        })
      }
      
      
      // Update progress with completion and metrics
      if (progressCallback) {
        progressCallback({
          nodeId: nodeData.id,
          nodeName: nodeData.label || 'AI Generation',
          status: 'completed',
          progress: 100,
          providerName: modelConfig.providerName,
          timestamp: new Date().toLocaleTimeString(),
          cost: actualCost,
          tokens: actualTokens,
          words: actualWordCount,
          output: content.substring(0, 200) + (content.length > 200 ? '...' : ''),
          aiResponse: aiResponse,
          processedContent: content,
          rawData: {
            model: selectedModels[0],
            provider: modelConfig.provider,
            temperature,
            maxTokens,
            systemPrompt: processedPrompts.systemPrompt,
            userPrompt: processedPrompts.userPrompt,
            inputData: previousOutput,
            dynamicInputs: dynamicInputs,
            modelCostPer1k: modelConfig.costPer1k
          }
        })
      }

      // STEP 4: COMBINE - AI output + previousNodePassover data
      const combinedDataPackage = {
        type: 'ai_generation',
        content: content,
        nodeName: nodeData.label || nodeData.id, // CRITICAL: Include node label for frontend display
        modelName: selectedModels[0], // Direct model access for frontend display
        previousNodePassover: pipelineData.previousNodePassover, // CRITICAL: Include previous node data
        aiMetadata: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          providerName: modelConfig.providerName,
          tokens: actualTokens,
          cost: actualCost,
          words: actualWordCount,
          modelCostPer1k: modelConfig.costPer1k
        },
        processedContent: content, // For AIThinkingModal compatibility
        aiResponse: aiResponse, // Raw AI response
        tokens: actualTokens, // Direct token access
        words: actualWordCount, // Direct word access
        inputData: previousOutput,
        instructions: inputInstructions,
        metadata: {
          nodeId: nodeData.id || 'process-node',
          nodeName: nodeData.label || nodeData.id,
          timestamp: new Date(),
          processingTime: aiResponse.processingTime || 0,
          dataPreservation: 'complete_context_maintained',
          permissions: nodeData.permissions // SURGICAL FIX: Include permissions for compilation filtering
        }
      }
      
      console.log('üîó COMBINED DATA PACKAGE: AI output + previous node data ready for next node')
      console.log('   - AI content length:', content?.length || 0)
      console.log('   - Previous data preserved:', !!pipelineData.previousNodePassover)
      
      // STEP 5: PASSOVER - Return combined package for next workflow node
      return combinedDataPackage

    } catch (error) {
      throw new Error(`AI generation failed: ${error.message}`)
    }
  }

  /**
   * Assess content quality to determine if refinement is needed
   */
  assessContentQuality(content) {
    if (!content || typeof content !== 'string') {
      return {
        isHighQuality: false,
        score: 0,
        wordCount: 0,
        assessment: 'Invalid or empty content'
      }
    }

    const wordCount = content.trim().split(/\s+/).filter(word => word.length > 0).length
    const sentences = content.split(/[.!?]+/).filter(s => s.trim().length > 0).length
    const paragraphs = content.split(/\n\s*\n/).filter(p => p.trim().length > 0).length

    let score = 0
    let assessment = []

    // Word count assessment (0-30 points)
    if (wordCount >= 1000) {
      score += 30
      assessment.push('Excellent word count')
    } else if (wordCount >= 500) {
      score += 20
      assessment.push('Good word count')
    } else if (wordCount >= 300) {
      score += 10
      assessment.push('Adequate word count')
    } else {
      assessment.push('Low word count')
    }

    // Structure assessment (0-25 points)
    if (sentences >= 20) {
      score += 25
      assessment.push('Well-structured with good sentence variety')
    } else if (sentences >= 10) {
      score += 15
      assessment.push('Decent sentence structure')
    } else if (sentences >= 5) {
      score += 10
      assessment.push('Basic sentence structure')
    } else {
      assessment.push('Poor sentence structure')
    }

    // Paragraph structure (0-20 points)
    if (paragraphs >= 5) {
      score += 20
      assessment.push('Good paragraph structure')
    } else if (paragraphs >= 3) {
      score += 15
      assessment.push('Adequate paragraph structure')
    } else if (paragraphs >= 1) {
      score += 10
      assessment.push('Basic paragraph structure')
    } else {
      assessment.push('Poor paragraph structure')
    }

    // Content richness (0-15 points)
    const hasDialogue = /"[^"]*"/.test(content) || /'[^']*'/.test(content)
    const hasDescriptions = content.includes('was') || content.includes('were') || content.includes('had')
    const hasAction = /(walked|ran|moved|went|came|looked|saw|heard|felt)/.test(content)

    if (hasDialogue && hasDescriptions && hasAction) {
      score += 15
      assessment.push('Rich content with dialogue, descriptions, and action')
    } else if (hasDescriptions && hasAction) {
      score += 10
      assessment.push('Good content with descriptions and action')
    } else if (hasDescriptions || hasAction) {
      score += 5
      assessment.push('Basic content elements present')
    } else {
      assessment.push('Limited content richness')
    }

    // Grammar and readability (0-10 points)
    const hasProperCapitalization = /[A-Z]/.test(content)
    const hasProperPunctuation = /[.!?]/.test(content)
    const averageWordLength = content.split(/\s+/).reduce((sum, word) => sum + word.length, 0) / wordCount

    if (hasProperCapitalization && hasProperPunctuation && averageWordLength > 4) {
      score += 10
      assessment.push('Good grammar and readability')
    } else if (hasProperCapitalization && hasProperPunctuation) {
      score += 7
      assessment.push('Decent grammar')
    } else if (hasProperCapitalization || hasProperPunctuation) {
      score += 4
      assessment.push('Basic grammar')
    } else {
      assessment.push('Poor grammar')
    }

    const isHighQuality = score >= 75 // Threshold for high quality content

    return {
      isHighQuality,
      score,
      wordCount,
      sentences,
      paragraphs,
      assessment: assessment.join(', ')
    }
  }

  /**
   * Execute content refinement - Editor processes existing content with checklist
   */
  async executeContentRefinement(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { selectedModels, systemPrompt, userPrompt, temperature, maxTokens, inputInstructions } = nodeData
    
    // Get previous node output (the content to refine)
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    console.log('üîç EDITOR NODE - Previous output check:', {
      hasPreviousOutput: !!previousOutput,
      previousOutputType: typeof previousOutput,
      hasContent: !!previousOutput?.content,
      hasChapters: !!previousOutput?.chapters,
      hasAllChapters: !!previousOutput?.allChapters,
      contentLength: previousOutput?.content?.length,
      keys: previousOutput ? Object.keys(previousOutput) : []
    })
    
    // SURGICAL FIX: Extract content from multiple possible locations
    let contentToEdit = null
    if (previousOutput?.content) {
      contentToEdit = previousOutput.content
    } else if (previousOutput?.allChapters && previousOutput.allChapters.length > 0) {
      // Content Writer returns chapters array - combine them
      contentToEdit = previousOutput.allChapters.map((ch, i) => {
        const title = ch.title || `Chapter ${i + 1}`
        const content = ch.content || ch.text || ''
        return `# ${title}\n\n${content}`
      }).join('\n\n')
      console.log('‚úÖ EDITOR: Combined chapters into content for editing')
    } else if (previousOutput?.chapters && previousOutput.chapters.length > 0) {
      // Alternative chapters structure
      contentToEdit = previousOutput.chapters.map((ch, i) => {
        const title = ch.title || `Chapter ${i + 1}`
        const content = ch.content || ch.text || ''
        return `# ${title}\n\n${content}`
      }).join('\n\n')
      console.log('‚úÖ EDITOR: Combined chapters (alt structure) into content for editing')
    } else if (typeof previousOutput === 'string') {
      // Previous output is just a string
      contentToEdit = previousOutput
    }
    
    if (!contentToEdit || contentToEdit.trim().length === 0) {
      console.error('‚ùå EDITOR ERROR: No content found to edit', {
        previousOutput,
        pipelineData: {
          hasLastNodeOutput: !!pipelineData.lastNodeOutput,
          hasUserInput: !!pipelineData.userInput,
          nodeOutputsKeys: Object.keys(pipelineData.nodeOutputs || {})
        }
      })
      throw new Error('No content available for refinement. Editor node must come after a content generation node.')
    }
    
    console.log(`‚úÖ EDITOR: Found ${contentToEdit.length} characters to edit`)
    
    // Update previousOutput to have the content in the right place
    if (!previousOutput.content) {
      previousOutput.content = contentToEdit
    }

    // SMART EDITOR LOGIC: Check if content is already high quality
    const contentQualityCheck = this.assessContentQuality(previousOutput.content)
    
    if (contentQualityCheck.isHighQuality) {
      console.log('‚úÖ Content is already high quality, skipping refinement')
      console.log(`üìä Quality metrics: ${contentQualityCheck.score}/100, Word count: ${contentQualityCheck.wordCount}`)
      
      // Return the content as-is with quality assessment
      return {
        success: true,
        output: {
          content: previousOutput.content,
          refinement: {
            skipped: true,
            reason: 'Content already meets quality standards',
            qualityScore: contentQualityCheck.score,
            wordCount: contentQualityCheck.wordCount,
            assessment: contentQualityCheck.assessment
          },
          metadata: {
            nodeType: 'editor',
            processingTime: 0,
            refinementSkipped: true,
            qualityScore: contentQualityCheck.score
          }
        }
      }
    }
    
    // Build dynamic prompt with real data substitution
    const processedPrompts = this.processPromptVariables({
      systemPrompt,
      userPrompt
    }, pipelineData, nodeData.permissions)

    // Get allData for refinement context
    const { userInput, nodeOutputs, lastNodeOutput } = pipelineData
    
    // Extract user_input from JSON wrapper if available
    let structuredData = userInput
    if (lastNodeOutput?.content?.user_input) {
      structuredData = lastNodeOutput.content.user_input
    } else if (lastNodeOutput?.structuredData) {
      structuredData = lastNodeOutput.structuredData
    }
    
    const allData = { 
      ...userInput, 
      ...structuredData,
      existingContent: previousOutput.content,
      previousNodeOutput: previousOutput
    }

    console.log('üîç Starting content refinement for Editor node')
    console.log('üîç Previous content length:', previousOutput.content?.length || 0)
    
    if (!selectedModels || selectedModels.length === 0) {
      throw new Error('No AI models selected for Editor node. Please configure AI Integration in the node modal.')
    }
    
      // CRITICAL: Check if workflow was stopped before AI call
      if (workflowId && (this.isWorkflowStopped(workflowId) || await this.checkDatabaseStopSignal(workflowId))) {
      console.log(`üõë WORKFLOW STOPPED BEFORE EDITOR AI CALL`)
      return {
        type: 'stopped_before_ai',
        content: 'Workflow stopped before content refinement could complete.',
        metadata: {
          nodeId: nodeData.id,
          timestamp: new Date(),
          stopped: true
        },
        stopped: true,
        message: 'Workflow stopped before content refinement.'
      }
    }

    // Get AI service for the selected model
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    console.log('üîç Editor parsed model config:', modelConfig)
    const aiServiceInstance = this.getAIService(modelConfig.provider)

    // Update progress with provider info
    if (progressCallback) {
      progressCallback({
        nodeId: nodeData.id,
        nodeName: nodeData.label || 'Content Refinement',
        status: 'executing',
        progress: 50,
        providerName: modelConfig.providerName,
        timestamp: new Date().toLocaleTimeString(),
        cost: 0,
        tokens: 0,
        words: 0,
        aiResponse: null,
        processedContent: null,
        rawData: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          providerName: modelConfig.providerName,
          temperature: temperature || 0.3, // Lower temperature for refinement
          maxTokens: maxTokens || 8000,
          systemPrompt: processedPrompts.systemPrompt,
          userPrompt: processedPrompts.userPrompt,
          inputData: allData,
          refinementMode: true
        }
      })
    }

    if (!aiServiceInstance) {
      throw new Error(`AI service not available for provider: ${modelConfig.provider}`)
    }

    try {
      // Build refinement-specific prompt
      const refinementPrompt = `
${processedPrompts.systemPrompt || 'You are an expert content editor and proofreader. Your role is to refine and improve existing content while maintaining its original intent and structure.'}

CONTENT TO REFINE:
${previousOutput.content}

REFINEMENT CHECKLIST:
- Fix any typos, grammatical errors, or spelling mistakes
- Ensure consistent tone and voice throughout
- Improve clarity and readability where needed
- Check for factual accuracy and logical flow
- Remove any hallucinations or made-up information
- Ensure proper formatting and structure
- Maintain the original content's intent and message
- Apply user-specific requirements (tone, accent, style preferences)

USER REQUIREMENTS:
${Object.entries(allData).filter(([key, value]) => 
  ['tone', 'accent', 'style', 'custom_instructions', 'branding_style'].includes(key) && value
).map(([key, value]) => `- ${key.replace(/_/g, ' ').toUpperCase()}: ${value}`).join('\n')}

${processedPrompts.userPrompt}

INSTRUCTIONS:
1. Read the existing content carefully
2. Identify areas that need improvement based on the checklist
3. Make only necessary corrections and improvements
4. Preserve the original structure and intent
5. Output the refined content with your changes clearly integrated

REFINED CONTENT:`

      // CELEBRITY STYLE INJECTION: Apply celebrity style to refinement if specified
      let finalRefinementPrompt = refinementPrompt
      const celebrityStyleValue = allData.celebrity_style || userInput.celebrity_style
      if (celebrityStyleValue) {
        const celebrityData = getCelebrityStyle(celebrityStyleValue)
        if (celebrityData && celebrityData.styleGuide) {
          console.log(`üé≠ CELEBRITY STYLE ACTIVATED (Refinement): ${celebrityData.label}`)
          finalRefinementPrompt = `${refinementPrompt}\n\nüé≠ WRITING STYLE DIRECTIVE:\n${celebrityData.styleGuide}`
        }
      }

      // Set SuperAdmin user in aiService
      if (pipelineData.superAdminUser) {
        await aiServiceInstance.setUser(pipelineData.superAdminUser)
      } else {
        throw new Error('SuperAdmin user not provided for AI service')
      }
      
      // Check if provider is available and has API key
      if (!aiServiceInstance.providers[modelConfig.providerName]) {
        throw new Error(`Provider ${modelConfig.providerName} not available. Please check API key configuration.`)
      }

      // Execute AI refinement with timeout
      const timeoutDuration = 1800000 // 30 minutes for refinement
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error(`Content refinement timeout after ${timeoutDuration / 60000} minutes`)), timeoutDuration)
      })

      const aiPromise = aiServiceInstance.generateContent(
        finalRefinementPrompt,
        modelConfig.providerName,
        maxTokens || 8000,
        modelConfig.modelId
      )

      const aiResponse = await Promise.race([aiPromise, timeoutPromise])
      const refinedContent = aiResponse.content || aiResponse

      // Calculate metrics
      const actualTokens = aiResponse.tokens || 0
      const actualCost = aiResponse.cost || 0
      const actualWordCount = refinedContent.split(/\s+/).length

      console.log('‚úÖ Content refinement completed')
      console.log(`   - Refined content length: ${refinedContent.length} characters`)
      console.log(`   - Word count: ${actualWordCount} words`)
      console.log(`   - Tokens used: ${actualTokens}`)
      console.log(`   - Cost: $${actualCost}`)

      // Update progress with completion
      if (progressCallback) {
        progressCallback({
          nodeId: nodeData.id,
          nodeName: nodeData.label || 'Content Refinement',
          status: 'completed',
          progress: 100,
          providerName: modelConfig.providerName,
          timestamp: new Date().toLocaleTimeString(),
          cost: actualCost,
          tokens: actualTokens,
          words: actualWordCount,
          output: refinedContent.substring(0, 200) + (refinedContent.length > 200 ? '...' : ''),
          aiResponse: aiResponse,
          processedContent: refinedContent,
          rawData: {
            model: selectedModels[0],
            provider: modelConfig.provider,
            temperature: temperature || 0.3,
            maxTokens: maxTokens || 8000,
            systemPrompt: processedPrompts.systemPrompt,
            userPrompt: processedPrompts.userPrompt,
            inputData: allData,
            originalContentLength: previousOutput.content?.length || 0,
            refinementMode: true
          }
        })
      }

      // Return refined content package
      const refinedDataPackage = {
        type: 'content_refinement',
        content: refinedContent,
        previousNodePassover: pipelineData.previousNodePassover,
        originalContent: previousOutput.content,
        aiMetadata: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          tokens: actualTokens,
          cost: actualCost,
          words: actualWordCount,
          modelCostPer1k: modelConfig.costPer1k,
          refinementMode: true
        },
        inputData: previousOutput,
        instructions: inputInstructions,
        metadata: {
          nodeId: nodeData.id || 'editor-node',
          timestamp: new Date(),
          processingTime: aiResponse.processingTime || 0,
          refinementApplied: true,
          originalLength: previousOutput.content?.length || 0,
          refinedLength: refinedContent.length
        }
      }
      
      console.log('üîó REFINED DATA PACKAGE: Editor output ready for next node')
      console.log('   - Refined content length:', refinedContent?.length || 0)
      console.log('   - Original content preserved:', !!previousOutput.content)
      
      return refinedDataPackage

    } catch (error) {
      throw new Error(`Content refinement failed: ${error.message}`)
    }
  }

  /**
   * Execute image generation - generate images using Gemini image models
   */
  async executeImageGeneration(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { selectedModels } = nodeData
    const { userInput } = pipelineData
    
    console.log('üé® IMAGE GENERATION NODE DETECTED')
    console.log('üé® Node:', nodeData.label || nodeData.id)
    console.log('üé® Role:', nodeData.role)
    
    // Get previous node output (likely story content or architecture)
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    // Build image generation prompt from previous content and user preferences
    const imagePrompt = this.buildImagePrompt(previousOutput, userInput, nodeData)
    
    console.log('üé® Image prompt:', imagePrompt.substring(0, 300) + '...')
    
    if (!selectedModels || selectedModels.length === 0) {
      console.warn('‚ö†Ô∏è No AI models selected for image node - skipping gracefully')
      return {
        success: true,
        output: {
          type: 'image_skipped',
          content: '',
          skipped: true,
          reason: 'No image model configured',
          metadata: {
            nodeType: 'image',
            skipped: true
          }
        }
      }
    }
    
    // Parse model config
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    const aiServiceInstance = this.getAIService(modelConfig.provider)
    
    // Set SuperAdmin user
    if (pipelineData.superAdminUser) {
      await aiServiceInstance.setUser(pipelineData.superAdminUser)
    } else {
      throw new Error('SuperAdmin user not provided for AI service')
    }
    
    try {
      // Call image generation
      const imageResult = await aiServiceInstance.generateImage(
        imagePrompt,
        modelConfig.providerName,
        modelConfig.modelId
      )
      
      console.log('‚úÖ Image generated successfully')
      console.log('üé® Image data type:', imageResult.type)
      
      // Return image data for storage/insertion
      return {
        success: true,
        output: {
          type: 'image_generation',
          imageData: imageResult.imageData,
          inlineData: imageResult.inlineData,
          metadata: {
            nodeType: 'image',
            nodeId: nodeData.id,
            nodeName: nodeData.label,
            timestamp: new Date(),
            model: modelConfig.modelId,
            provider: modelConfig.providerName,
            usage: imageResult.usage
          }
        }
      }
      
    } catch (error) {
      console.error('‚ùå Image generation failed:', error)
      
      // Skip gracefully if image generation fails
      console.log('‚ö†Ô∏è Skipping image generation gracefully due to error')
      return {
        success: true,
        output: {
          type: 'image_skipped',
          content: '',
          skipped: true,
          reason: error.message,
          metadata: {
            nodeType: 'image',
            skipped: true,
            error: error.message
          }
        }
      }
    }
  }

  /**
   * Build image generation prompt from story content and user preferences
   */
  buildImagePrompt(previousContent, userInput, nodeData) {
    // Extract relevant content for image generation
    let contentSummary = ''
    if (typeof previousContent?.content === 'string') {
      // Take first 500 chars of content as context
      contentSummary = previousContent.content.substring(0, 500)
    } else if (typeof previousContent === 'string') {
      contentSummary = previousContent.substring(0, 500)
    }
    
    // Extract image preferences from user input
    const imageStyle = userInput.image_style || 'photorealistic'
    const mood = userInput.mood || 'neutral'
    const composition = userInput.composition || 'centered'
    const aspectRatio = userInput.aspect_ratio || '16:9'
    
    // Build comprehensive image prompt
    const prompt = `Generate an image for this story:

STORY CONTEXT:
${contentSummary}

IMAGE SPECIFICATIONS:
- Style: ${imageStyle}
- Mood: ${mood}
- Composition: ${composition}
- Aspect Ratio: ${aspectRatio}

Create a visually striking image that captures the essence of the story.`
    
    return prompt
  }

  /**
   * Execute preview node - generate preview content for customer approval
   */
  async executePreviewNode(nodeData, pipelineData, progressCallback = null) {
    const { 
      aiEnabled, 
      selectedModels, 
      systemPrompt, 
      userPrompt, 
      maxAttempts = 3,
      previewLength = '1 chapter',
      currentAttempt = 0,
      customerFeedback = '',
      isApproved = false
    } = nodeData

    if (!aiEnabled || !selectedModels || selectedModels.length === 0) {
      throw new Error('Preview node requires AI configuration')
    }

    // Get previous output or user input
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    const { userInput, nodeOutputs } = pipelineData

    // Build dynamic prompts with customer feedback if available
    const processedPrompts = this.processPromptVariables(
      systemPrompt || 'Generate a preview of the content based on the user requirements.',
      userPrompt || 'Create a preview that showcases the writing style and content quality.',
      userInput,
      nodeOutputs,
      previousOutput,
      customerFeedback // Include customer feedback in prompt processing
    )

    // Generate preview content
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    const aiServiceInstance = this.getAIService(modelConfig.provider)
    
    // Set SuperAdmin user in aiService to load API keys from database
    if (pipelineData.superAdminUser) {
      await aiServiceInstance.setUser(pipelineData.superAdminUser)
    } else {
      throw new Error('SuperAdmin user not provided for AI service')
    }
    
    // Check if provider is available and has API key
    if (!aiServiceInstance.providers[modelConfig.providerName]) {
      throw new Error(`Provider ${modelConfig.providerName} not available. Please check API key configuration.`)
    }
    
    const providerConfig = aiServiceInstance.providers[modelConfig.providerName]
    if (!providerConfig.apiKey) {
      throw new Error(`No API key configured for ${modelConfig.providerName}. Please add API key in SuperAdmin settings.`)
    }
    
    // Build final prompt combining system and user prompts
    let finalPrompt = processedPrompts.systemPrompt 
      ? `${processedPrompts.systemPrompt}\n\n${processedPrompts.userPrompt}`
      : processedPrompts.userPrompt
    
    // DYNAMIC MAX TOKENS CALCULATION for preview node - Generous and intelligent
    const wordCount = userInput?.word_count || userInput?.['Word Count'] || '2000'
    const chapterCount = userInput?.chapter_count || userInput?.['Chapter Count'] || '1'
    const wordsPerChapter = Math.floor(parseInt(wordCount) / parseInt(chapterCount))
    const dynamicMaxTokens = Math.ceil(wordsPerChapter * 1.3 * 1.5) // 50% buffer
    const finalMaxTokens = 16384 // NO RESTRICTIONS - Maximum capacity
    
    console.log('üîç PREVIEW NODE DYNAMIC MAX TOKENS:')
    console.log('  - wordsPerChapter:', wordsPerChapter)
    console.log('  - finalMaxTokens:', finalMaxTokens)
    
    // Make REAL AI API call
    const aiResponse = await aiServiceInstance.generateContent(finalPrompt, modelConfig.providerName, finalMaxTokens, modelConfig.modelId)
    
    const content = aiResponse.content || aiResponse.text || JSON.stringify(aiResponse)
    
    // Calculate metrics
    const actualWordCount = typeof content === 'string' ? content.split(' ').length : 0
    let actualTokens = 0
    let actualCost = 0
    
    if (aiResponse && aiResponse.usage) {
      actualTokens = aiResponse.usage.total_tokens || 
                    aiResponse.usage.completion_tokens || 
                    aiResponse.token_count ||
                    Math.ceil(actualWordCount * 1.3)
      
      const modelCostPer1k = modelConfig.costPer1k || 0.00003
      actualCost = (actualTokens / 1000) * modelCostPer1k
    } else {
      actualTokens = Math.ceil(actualWordCount * 1.3)
      actualCost = (actualTokens / 1000) * (modelConfig.costPer1k || 0.00003)
    }

    // Send progress update
    if (progressCallback) {
      progressCallback({
        nodeId: nodeData.id,
        nodeName: nodeData.label || 'Preview Generation',
        status: 'completed',
        progress: 100,
        providerName: modelConfig.providerName,
        timestamp: new Date().toLocaleTimeString(),
        cost: actualCost,
        tokens: actualTokens,
        words: actualWordCount,
        output: content.substring(0, 200) + (content.length > 200 ? '...' : ''),
        aiResponse: aiResponse,
        processedContent: content,
        rawData: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          systemPrompt: processedPrompts.systemPrompt,
          userPrompt: processedPrompts.userPrompt,
          inputData: previousOutput,
          customerFeedback: customerFeedback,
          currentAttempt: currentAttempt + 1,
          maxAttempts: maxAttempts,
          previewLength: previewLength
        }
      })
    }

    return {
      type: 'preview_generation',
      content: content,
      aiMetadata: {
        model: selectedModels[0],
        provider: modelConfig.provider,
        tokens: actualTokens,
        cost: actualCost,
        words: actualWordCount,
        modelCostPer1k: modelConfig.costPer1k
      },
      inputData: previousOutput,
      instructions: processedPrompts.userPrompt,
      metadata: {
        nodeId: nodeData.id || 'preview-node',
        timestamp: new Date(),
        previewLength: previewLength,
        currentAttempt: currentAttempt + 1,
        maxAttempts: maxAttempts,
        customerFeedback: customerFeedback,
        isApproved: false, // Will be set to true when approved
        totalCharacters: content.length
      },
      structuredData: {
        ...previousOutput?.structuredData,
        previewContent: content,
        previewApproved: false,
        previewAttempt: currentAttempt + 1
      }
    }
  }

  /**
   * Execute condition node - evaluate conditions and route data
   */
  async executeConditionNode(nodeData, pipelineData) {
    const { conditions, aiEnabled } = nodeData
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput

    const evaluationResults = []

    for (const condition of conditions || []) {
      const result = this.evaluateCondition(condition, previousOutput, pipelineData)
      evaluationResults.push(result)

      // Execute action based on condition result
      if (result.passed && condition.trueAction) {
        await this.executeConditionAction(condition.trueAction, pipelineData)
      } else if (!result.passed && condition.falseAction) {
        await this.executeConditionAction(condition.falseAction, pipelineData)
      }
    }

    return {
      type: 'condition_evaluation',
      evaluations: evaluationResults,
      inputData: previousOutput,
      metadata: {
        nodeId: nodeData.id || 'condition-node',
        timestamp: new Date(),
        conditionCount: conditions?.length || 0
      }
    }
  }

  /**
   * Execute output node - format and deliver final results
   */
  async executeOutputNode(nodeData, pipelineData) {
    const { outputFormat, exportFormats, generateCover, includeImages, includeTOC } = nodeData
    const allNodeOutputs = pipelineData.nodeOutputs

    // Compile all content from previous nodes
    const compiledContent = this.compileWorkflowContent(allNodeOutputs, pipelineData.userInput)
    
    // VALIDATION: Ensure sections are properly structured before export
    console.log('üîç COMPILED CONTENT VALIDATION:')
    console.log('  - Total sections:', compiledContent.sections.length)
    console.log('  - Total words:', compiledContent.totalWords)
    console.log('  - Total characters:', compiledContent.totalCharacters)
    
    // Validate sections structure
    const invalidSections = compiledContent.sections.filter(section => {
      return !section.title || !section.content || typeof section.content !== 'string' || section.content.trim().length === 0
    })
    
    if (invalidSections.length > 0) {
      console.error('‚ùå INVALID SECTIONS DETECTED:', invalidSections)
      console.error('  - Sections missing title:', invalidSections.filter(s => !s.title).length)
      console.error('  - Sections missing content:', invalidSections.filter(s => !s.content).length)
      console.error('  - Sections with empty content:', invalidSections.filter(s => !s.content || !s.content.trim()).length)
    }
    
    if (compiledContent.sections.length === 0) {
      const errorMsg = `No valid sections found for export. Total nodes processed: ${compiledContent.metadata.nodeCount}, Total words: ${compiledContent.totalWords}`
      console.error('‚ùå', errorMsg)
      throw new Error(errorMsg)
    }

    // Get format options from user input (from input node) - ENHANCED FORMAT DETECTION
    let userInputFormats = pipelineData.userInput.output_formats || 
                          pipelineData.userInput.outputFormats || 
                          pipelineData.userInput.exportFormats || 
                          pipelineData.userInput['Output Formats'] || 
                          pipelineData.userInput['output formats'] || []
    const nodeFormats = exportFormats || []
    
    console.log('üìù RAW FORMAT INPUT DEBUG:')
    console.log('  - userInput keys:', Object.keys(pipelineData.userInput))
    console.log('  - output_formats:', pipelineData.userInput.output_formats)
    console.log('  - outputFormats:', pipelineData.userInput.outputFormats)
    console.log('  - exportFormats:', pipelineData.userInput.exportFormats)
    console.log('  - Output Formats:', pipelineData.userInput['Output Formats'])
    console.log('  - nodeFormats:', nodeFormats)
    
    // CRITICAL FIX: Handle multiple format input types
    if (typeof userInputFormats === 'string') {
      // If it's a comma-separated string like "pdf,docx,epub", split it
      userInputFormats = userInputFormats.split(',').map(f => f.trim().toLowerCase())
    } else if (Array.isArray(userInputFormats)) {
      // Normalize array formats to lowercase
      userInputFormats = userInputFormats.map(f => String(f).trim().toLowerCase())
    } else if (!Array.isArray(userInputFormats)) {
      // If it's not an array, make it one
      userInputFormats = []
    }
    
    // Remove empty or invalid formats
    userInputFormats = userInputFormats.filter(f => f && f.length > 1)
    
    console.log('üìù PROCESSED FORMATS:')
    console.log('  - userInputFormats (processed):', userInputFormats)
    console.log('  - nodeFormats:', nodeFormats)
    
    // Priority: user input formats > node formats > default format
    const finalExportFormats = userInputFormats.length > 0 ? userInputFormats : 
                                nodeFormats.length > 0 ? nodeFormats : 
                                [outputFormat || 'markdown']
    
    console.log('üéØ User input formats:', userInputFormats)
    console.log('üéØ Final export formats:', finalExportFormats)

    console.log('üéØ Output node format resolution:')
    console.log('  - User input formats:', userInputFormats)
    console.log('  - Node formats:', nodeFormats)
    console.log('  - Final formats:', finalExportFormats)

    // Format according to specified output format with comprehensive error handling
    let formattedOutput
    try {
      formattedOutput = await this.formatFinalOutput(compiledContent, {
        outputFormat,
        exportFormats: finalExportFormats,
        generateCover,
        includeImages,
        includeTOC
      })
    } catch (error) {
      // Detailed error reporting for debugging
      const errorDetails = {
        message: error.message,
        stack: error.stack,
        compiledContent: {
          sectionsCount: compiledContent.sections.length,
          sectionsStructure: compiledContent.sections.map(s => ({
            hasTitle: !!s.title,
            hasContent: !!s.content,
            contentLength: typeof s.content === 'string' ? s.content.length : 0,
            chapterNumber: s.chapterNumber,
            contentType: s.contentType
          })),
          totalWords: compiledContent.totalWords,
          totalCharacters: compiledContent.totalCharacters
        },
        requestedFormats: finalExportFormats,
        nodeOutputs: Object.keys(allNodeOutputs)
      }
      
      console.error('‚ùå EXPORT FAILURE - DETAILED DEBUG INFO:')
      console.error(JSON.stringify(errorDetails, null, 2))
      
      throw new Error(`Export failed: ${error.message}. Sections: ${compiledContent.sections.length}, Valid sections: ${compiledContent.sections.filter(s => s.title && s.content).length}`)
    }

    // Verification: ensure requested audio formats are present (log-only)
    try {
      const audioRequested = finalExportFormats.filter(f => ['mp3','wav','m4a','audiobook'].includes((f||'').toLowerCase()))
      if (audioRequested.length > 0) {
        const missing = audioRequested.filter(f => !(formattedOutput?.allFormats && formattedOutput.allFormats[f]))
        if (missing.length > 0) {
          console.warn('‚ö†Ô∏è Audio formats requested but not generated:', missing)
        } else {
          console.log('‚úÖ Audio formats generated:', audioRequested)
        }
      }
    } catch (e) {
      console.warn('Audio verification skipped:', e?.message)
    }

    // CRITICAL: Ensure all formats are properly stored and accessible
    const outputResult = {
      type: 'final_output',
      content: formattedOutput.content || formattedOutput,
      compiledData: compiledContent,
      deliverables: this.generateDeliverables(formattedOutput, nodeData),
      // CRITICAL: Store all formats at root level for easy access
      allFormats: formattedOutput.allFormats || {},
      requestedFormats: finalExportFormats,
      availableFormats: Object.keys(formattedOutput.allFormats || {}),
      metadata: {
        nodeId: nodeData.id || 'output-node',
        timestamp: new Date(),
        totalWords: compiledContent.totalWords || 0,
        totalCharacters: compiledContent.totalCharacters || 0,
        nodeCount: compiledContent.metadata?.nodeCount || 0,
        wordCountByNode: compiledContent.metadata?.wordCountByNode || {},
        characterCountByNode: compiledContent.metadata?.characterCountByNode || {},
        formats: finalExportFormats,
        allFormats: formattedOutput.allFormats || {}, // DUPLICATE for compatibility
        primaryFormat: formattedOutput.primaryFormat || finalExportFormats[0],
        requestedFormats: formattedOutput.requestedFormats || finalExportFormats,
        userRequestedFormats: userInputFormats, // TRACK ORIGINAL USER REQUEST
        nodeRequestedFormats: nodeFormats, // TRACK NODE CONFIGURATION
        generationStats: {
          totalSections: compiledContent.sections.length,
          averageWordsPerSection: compiledContent.sections.length > 0 ? Math.round(compiledContent.totalWords / compiledContent.sections.length) : 0,
          averageCharactersPerSection: compiledContent.sections.length > 0 ? Math.round(compiledContent.totalCharacters / compiledContent.sections.length) : 0,
          formatsGenerated: Object.keys(formattedOutput.allFormats || {}).length,
          formatDetails: Object.keys(formattedOutput.allFormats || {}).map(format => ({
            format,
            size: typeof formattedOutput.allFormats[format] === 'string' ? 
                  formattedOutput.allFormats[format].length : 
                  JSON.stringify(formattedOutput.allFormats[format]).length
          }))
        }
      }
    }
    
    console.log('‚úÖ OUTPUT NODE RESULT:')
    console.log('  - Available formats:', outputResult.availableFormats)
    console.log('  - Requested formats:', outputResult.requestedFormats)
    console.log('  - User requested:', userInputFormats)
    console.log('  - Node requested:', nodeFormats)
    console.log('  - Format details:', outputResult.metadata.generationStats.formatDetails)
    
    return outputResult
  }

  /**
   * Helper methods for execution
   */
  
  buildExecutionOrder(nodes, edges) {
    // Build dependency graph and return topologically sorted execution order
    console.log('üîç BUILDING EXECUTION ORDER:')
    console.log('  - Nodes:', nodes.map(n => `${n.id} (${n.data.label || n.data.type})`).join(', '))
    console.log('  - Edges:', edges.map(e => `${e.source} ‚Üí ${e.target}`).join(', '))
    
    const nodeMap = new Map(nodes.map(node => [node.id, node]))
    const incomingEdges = new Map()
    const outgoingEdges = new Map()

    // Initialize maps
    nodes.forEach(node => {
      incomingEdges.set(node.id, [])
      outgoingEdges.set(node.id, [])
    })

    // Build edge maps
    edges.forEach(edge => {
      if (!outgoingEdges.has(edge.source) || !incomingEdges.has(edge.target)) {
        console.error(`‚ùå INVALID EDGE: ${edge.source} ‚Üí ${edge.target} (missing nodes)`)
        return
      }
      outgoingEdges.get(edge.source).push(edge.target)
      incomingEdges.get(edge.target).push(edge.source)
    })

    // Debug dependency structure
    console.log('üîó DEPENDENCY STRUCTURE:')
    nodes.forEach(node => {
      const deps = incomingEdges.get(node.id)
      console.log(`  - ${node.id} (${node.data.label || node.data.type}) depends on: [${deps.join(', ') || 'none'}]`)
    })

    // Topological sort
    const visited = new Set()
    const result = []

    const visit = (nodeId) => {
      if (visited.has(nodeId)) return
      visited.add(nodeId)
      
      // Visit dependencies first
      incomingEdges.get(nodeId).forEach(depId => visit(depId))
      
      result.push(nodeMap.get(nodeId))
    }

    // SURGICAL FIX: PRESERVE SUPERADMIN VISUAL SEQUENCE
    // Sort nodes by Y position first (SuperAdmin visual order), then apply dependency constraints
    const visualOrder = [...nodes].sort((a, b) => {
      // Primary sort: Y position (top to bottom as designed in SuperAdmin)
      const yDiff = (a.position?.y || 0) - (b.position?.y || 0)
      if (Math.abs(yDiff) > 50) return yDiff // Different rows
      
      // Secondary sort: X position (left to right within same row)
      return (a.position?.x || 0) - (b.position?.x || 0)
    })

    console.log('üîç SUPERADMIN VISUAL ORDER (Y-position based):')
    visualOrder.forEach((node, i) => {
      console.log(`  ${i + 1}. ${node.id} (${node.data.label || node.data.type}) - Y:${node.position?.y || 0}`)
    })

    // Process nodes in visual order while respecting dependencies
    visualOrder.forEach(node => visit(node.id))

    // SURGICAL FIX: Force output nodes to run last, regardless of dependency graph
    const outputNodes = result.filter(node => node.data.type === 'output')
    const nonOutputNodes = result.filter(node => node.data.type !== 'output')
    const surgicalResult = [...nonOutputNodes, ...outputNodes]
    
    console.log('üîß SURGICAL EXECUTION ORDER APPLIED:')
    console.log('  - Non-output nodes:', nonOutputNodes.map(n => `${n.id} (${n.data.type})`).join(', '))
    console.log('  - Output nodes (moved to end):', outputNodes.map(n => `${n.id} (${n.data.type})`).join(', '))
    console.log('‚úÖ FINAL EXECUTION ORDER:', surgicalResult.map((n, i) => `${i + 1}. ${n.id} (${n.data.label || n.data.type})`).join(', '))
    
    // Validate the order (using surgicalResult instead of result)
    for (let i = 0; i < surgicalResult.length; i++) {
      const node = surgicalResult[i]
      const dependencies = incomingEdges.get(node.id)
      const unmetDeps = dependencies.filter(depId => {
        const depIndex = surgicalResult.findIndex(n => n.id === depId)
        return depIndex === -1 || depIndex > i
      })
      if (unmetDeps.length > 0) {
        console.error(`‚ùå EXECUTION ORDER VIOLATION: ${node.id} at position ${i + 1} has unmet dependencies:`, unmetDeps)
      }
    }

    return surgicalResult
  }

  validateInputFields(userInput, inputFields) {
    const errors = []
    
    console.log('üîç Input validation debug:')
    console.log('  - User input keys:', Object.keys(userInput))
    console.log('  - User input values:', userInput)
    console.log('  - Input fields:', inputFields?.map(f => ({ name: f.name, variable: f.variable, required: f.required })))
    
    inputFields?.forEach(field => {
      // Check both field.name and field.variable for compatibility with different flow types
      const fieldValue = userInput[field.name] || userInput[field.variable]
      console.log(`  - Checking field "${field.name}" (variable: "${field.variable}"):`, fieldValue, '(required:', field.required, ')')
      
      if (field.required && (!fieldValue || fieldValue === '')) {
        errors.push(`${field.name} is required`)
      }
    })

    console.log('  - Validation errors:', errors)
    return {
      isValid: errors.length === 0,
      errors
    }
  }

  async structureInputData(userInput, inputFields) {
    const structured = {}
    
    console.log('üîç STRUCTURE INPUT DATA DEBUG:')
    console.log('  - userInput:', userInput)
    console.log('  - inputFields:', inputFields)
    
    for (const field of inputFields || []) {
      console.log(`  - Processing field: ${field.name} (variable: ${field.variable || field.name})`)
      console.log(`    - userInput[${field.name}]:`, userInput[field.name])
      
      if (userInput[field.name] !== undefined) {
        // Handle file uploads specially
        if (field.type === 'file' && userInput[field.name]) {
          try {
            const uploadedUrl = await this.uploadFileToSupabase(userInput[field.name], field.name)
            structured[field.variable || field.name] = uploadedUrl
            console.log(`    - File uploaded: ${uploadedUrl}`)
          } catch (error) {
            console.error(`Failed to upload file for ${field.name}:`, error)
            structured[field.variable || field.name] = null
          }
        } else {
          structured[field.variable || field.name] = userInput[field.name]
          console.log(`    - Structured value: ${structured[field.variable || field.name]}`)
        }
      } else {
        console.log(`    - Field ${field.name} not found in userInput`)
      }
    }

    console.log('  - Final structured data:', structured)
    return structured
  }

  async uploadFileToSupabase(file, fieldName) {
    try {
      // Generate unique filename
      const timestamp = Date.now()
      const sanitizedName = fieldName.replace(/[^a-zA-Z0-9]/g, '_')
      const fileExtension = file.name.split('.').pop()
      const fileName = `${sanitizedName}_${timestamp}.${fileExtension}`
      
      // Upload to Supabase storage
      const { data, error } = await getSupabase().storage
        .from('workflow-assets')
        .upload(`covers/${fileName}`, file, {
          cacheControl: '3600',
          upsert: false
        })

      if (error) throw error

      // Get public URL
      const { data: urlData } = getSupabase().storage
        .from('workflow-assets')
        .getPublicUrl(`covers/${fileName}`)

      return urlData.publicUrl
    } catch (error) {
      console.error('File upload error:', error)
      throw new Error(`Failed to upload file: ${error.message}`)
    }
  }

  identifyMissingOptionals(userInput, inputFields) {
    const missing = []
    
    inputFields?.forEach(field => {
      if (!field.required && (!userInput[field.name] || userInput[field.name] === '')) {
        missing.push({
          field: field.name,
          variable: field.variable,
          type: field.type,
          placeholder: field.placeholder
        })
      }
    })

    return missing
  }

  createNextNodeInstructions(missingOptionals, baseInstructions) {
    if (missingOptionals.length === 0) {
      return baseInstructions
    }

    const missingFieldGuidance = missingOptionals.map(field => 
      `- ${field.field}: Not provided, use intelligent defaults or skip if not essential`
    ).join('\n')

    return `${baseInstructions}\n\nMISSING OPTIONAL FIELDS GUIDANCE:\n${missingFieldGuidance}\n\nAdjust processing accordingly to handle missing information gracefully.`
  }

  processPromptVariables(prompts, pipelineData, nodePermissions = null) {
    const { userInput, nodeOutputs, lastNodeOutput } = pipelineData
    
    // CRITICAL: Extract user_input from JSON wrapper if available, otherwise fall back to userInput
    let structuredData = userInput
    if (lastNodeOutput?.content?.user_input) {
      structuredData = lastNodeOutput.content.user_input
    } else if (lastNodeOutput?.structuredData) {
      structuredData = lastNodeOutput.structuredData
    }
    
    // FIXED: NO DUPLICATE VARIABLES - Clean data structure
    const allData = { 
      // Primary data source (no duplicates)
      ...structuredData,
      // Chapter context (if available)
      ...(pipelineData.currentChapter && { currentChapter: pipelineData.currentChapter }),
      ...(pipelineData.totalChapters && { totalChapters: pipelineData.totalChapters }),
      ...(pipelineData.previousChapters && { previousChapters: pipelineData.previousChapters }),
      // Single source of truth for previous node data
      previous_node_output: lastNodeOutput || null,
      // Clean user input reference
      user_input_data: structuredData
    }
    
    // Remove any undefined values to prevent confusion
    Object.keys(allData).forEach(key => {
      if (allData[key] === undefined) {
        delete allData[key]
      }
    })
    
    console.log(`üîç Processed variables for node:`, {
      variablesCount: Object.keys(allData).length,
      hasPreviousOutput: !!allData.previous_node_output,
      hasUserInput: !!allData.user_input_data,
      chapterContext: !!allData.currentChapter
    })
    
    // SURGICAL FIX: NO PERMISSION BLOCK INJECTION - Permissions enforced in validation only
    // This prevents instruction contamination in chapter content
    let enhancedSystemPrompt = prompts.systemPrompt || ''
    
    // Log permissions for debugging but DON'T inject into prompt
    if (nodePermissions) {
      console.log('üîê NODE PERMISSIONS (enforced in validation, NOT injected into prompt):')
      console.log('  - canWriteContent:', nodePermissions.canWriteContent)
      console.log('  - canEditStructure:', nodePermissions.canEditStructure) 
      console.log('  - canProofRead:', nodePermissions.canProofRead)
    }

    // DEBUG: Log ALL input variables
    console.log('üîç ALL INPUT VARIABLES DEBUG:')
    console.log('  - pipelineData keys:', Object.keys(pipelineData))
    console.log('  - userInput keys:', Object.keys(userInput))
    console.log('  - allData keys:', Object.keys(allData))
    console.log('  - Chapter Context:')
    console.log('    - currentChapter:', pipelineData.currentChapter)
    console.log('    - totalChapters:', pipelineData.totalChapters)
    console.log('    - previousChapters:', pipelineData.previousChapters)
    console.log('  - Word Count Variables:')
    console.log('    - word_count:', allData.word_count)
    console.log('    - chapter_count:', allData.chapter_count)
    console.log('    - Book Title:', allData.book_title)
    console.log('    - Author Name:', allData.author_name)
    console.log('  - All User Input Values:')
    Object.entries(userInput).forEach(([key, value]) => {
      console.log(`    - ${key}: ${value}`)
    })

    const processTemplate = (template) => {
      if (!template) return ''
      
      return template.replace(/\{([^}]+)\}/g, (match, variable) => {
        // Handle multiple possible variable name formats
        let value = allData[variable] || 
                   allData[variable.toLowerCase()] || 
                   allData[variable.replace(/\s+/g, '_').toLowerCase()] ||
                   allData[variable.replace(/\s+/g, '_')] ||
                   `[${variable} not provided]`
        
        // Special handling for chapter context variables
        if (variable === 'currentChapter' && allData.currentChapter) {
          value = allData.currentChapter
        } else if (variable === 'totalChapters' && allData.totalChapters) {
          value = allData.totalChapters
        } else if (variable === 'previousChapters' && allData.previousChapters) {
          // Format previous chapters context for AI
          const previousChapters = allData.previousChapters
          if (Array.isArray(previousChapters) && previousChapters.length > 0) {
            value = previousChapters.map((chapter, index) => {
              const chapterNum = chapter.chapter || (index + 1)
              const content = chapter.content || ''
              const summary = content.length > 200 ? content.substring(0, 200) + '...' : content
              return `Chapter ${chapterNum}: ${summary}`
            }).join('\n\n')
          } else {
            value = 'No previous chapters available'
          }
        } else if (variable === 'previousChapterSummary' && allData.previousChapters) {
          // Create a concise summary of previous chapters
          const previousChapters = allData.previousChapters
          if (Array.isArray(previousChapters) && previousChapters.length > 0) {
            const lastChapter = previousChapters[previousChapters.length - 1]
            const chapterNum = lastChapter.chapter || previousChapters.length
            const content = lastChapter.content || ''
            const summary = content.length > 150 ? content.substring(0, 150) + '...' : content
            value = `In Chapter ${chapterNum}, ${summary}`
          } else {
            value = 'This is the first chapter'
          }
        }
        
        // Special handling for common variable mappings
        if (value === `[${variable} not provided]`) {
          const mappings = {
            'Book Title': 'book_title',
            'Author Name': 'author_name', 
            'Author Bio': 'author_bio',
            'Word Count': 'word_count',
            'Chapter Count': 'chapter_count',
            'Tone': 'tone',
            'Accent': 'accent',
            'Target Audience': 'target_audience',
            'Industry Focus': 'industry_focus',
            'Custom Instructions': 'custom_instructions',
            'Include Case Studies': 'include_case_studies',
            'Include Templates': 'include_templates',
            'Include Worksheets': 'include_worksheets',
            'Research Level': 'research_level',
            'Practical Applications': 'practical_applications',
            'Content Depth': 'content_depth',
            'Publishing Format': 'publishing_format',
            'Business Model': 'business_model',
            'Typography Style': 'typography_style'
          }
          
          const mappedKey = mappings[variable]
          if (mappedKey && allData[mappedKey]) {
            value = allData[mappedKey]
          }
        }
        
        console.log(`üîç Variable ${variable}: ${value}`)
        return value
      })
    }

    // APPLY ACCENT-SPECIFIC INSTRUCTIONS TO ALL PROMPTS
    const accent = allData.accent || allData['Language Accent'] || 'neutral'
    const tone = allData.tone || allData.Tone || 'professional'
    
    let baseUserPrompt = processTemplate(prompts.userPrompt)
    
    // ENHANCE PROMPT WITH CHAPTER CONTEXT FOR MULTI-CHAPTER GENERATION
    if (allData.currentChapter && allData.totalChapters && allData.totalChapters > 1) {
      const chapterContext = this.generateChapterContext(allData)
      if (chapterContext) {
        baseUserPrompt = chapterContext + '\n\n' + baseUserPrompt
        console.log(`üìö CHAPTER CONTEXT ENHANCED PROMPT:`, chapterContext.substring(0, 200) + '...')
      }
    }
    
    // ENHANCE PROMPT WITH ACCENT INSTRUCTIONS
    if (accent && accent !== 'neutral') {
      baseUserPrompt = accentInstructionService.buildAccentSpecificPrompt(baseUserPrompt, accent, tone)
      console.log(`üéØ ACCENT ENHANCED PROMPT for ${accent}:`, baseUserPrompt.substring(0, 200) + '...')
    }

    // ENHANCE PROMPT WITH TYPOGRAPHY INSTRUCTIONS
    if (allData.typography_combo) {
      const typographyInstructions = typographyService.generateTypographyInstructions(allData.typography_combo)
      baseUserPrompt = baseUserPrompt + '\n\n' + typographyInstructions
      console.log(`üé® TYPOGRAPHY ENHANCED PROMPT for ${allData.typography_combo}:`, typographyInstructions.substring(0, 200) + '...')
    }

    const processedPrompts = {
      systemPrompt: processTemplate(enhancedSystemPrompt),
      userPrompt: baseUserPrompt
    }

    console.log('üîç PROCESSED SYSTEM PROMPT:', processedPrompts.systemPrompt.substring(0, 500) + '...')
    console.log('üîç PROCESSED USER PROMPT:', processedPrompts.userPrompt.substring(0, 500) + '...')

    return processedPrompts
  }

  /**
   * Generate chapter-specific context for multi-chapter generation
   */
  generateChapterContext(allData) {
    const currentChapter = allData.currentChapter
    const totalChapters = allData.totalChapters
    const previousChapters = allData.previousChapters
    
    if (!currentChapter || !totalChapters) return null
    
    // SURGICAL FIX: NO INSTRUCTION CONTAMINATION - Return minimal context only
    let context = ''
    
    // Add previous chapter context for continuity (without instruction garbage)
    if (previousChapters && Array.isArray(previousChapters) && previousChapters.length > 0) {
      context += `Previous chapters for continuity:\n`
      previousChapters.forEach((chapter, index) => {
        const chapterNum = chapter.chapter || (index + 1)
        const content = chapter.content || ''
        const summary = content.length > 100 ? content.substring(0, 100) + '...' : content
        context += `Chapter ${chapterNum}: ${summary}\n`
      })
      context += `\n`
    }
    
    return context
  }

  async parseModelConfig(modelString) {
    if (!modelString) {
      throw new Error('No model selected for AI generation')
    }
    
    // Handle both old format (OPENA-01-first) and new format (providerName:modelId)
    let providerName, modelId
    
    if (modelString.includes(':')) {
      // New format: providerName:modelId
      [providerName, modelId] = modelString.split(':')
      
      // Clean model ID - strip 'models/' prefix (like Vite proxy does)
      if (modelId && modelId.startsWith('models/')) {
        modelId = modelId.replace('models/', '')
        console.log('üîç Cleaned model ID:', modelId)
      }
      
      // Handle invalid model IDs by fetching from database
      if (modelId === 'default') {
        console.log('üîç Fetching default model for provider:', providerName)
        try {
          // Get the first available model for this provider from database
          const { data: defaultModel, error: modelError } = await getSupabase()
            .from('ai_model_metadata')
            .select('model_id')
            .eq('provider_name', providerName)
            .eq('status', 'active')
            .order('created_at')
            .limit(1)
            .single()
          
          if (!modelError && defaultModel?.model_id) {
            modelId = defaultModel.model_id
            console.log('üîç Fetched default model from database:', modelId)
          } else {
            throw new Error(`No active models found for provider ${providerName}`)
          }
        } catch (error) {
          console.error('üîç Failed to fetch default model from database:', error)
          throw new Error(`Invalid model ID 'default' for provider ${providerName}. Please select a valid model.`)
        }
      }
    } else {
      // Old format: OPENA-01-first - extract provider name from the string
      providerName = modelString
      modelId = 'default' // Use default model for old format
      
      console.log('üîç Using legacy model format:', modelString, '-> provider:', providerName, 'model:', modelId)
    }
    
    if (!providerName) {
      throw new Error(`Invalid model format: ${modelString}. Expected format: providerName:modelId or legacy format`)
    }
    
    // Query database to get actual provider type - NO HARDCODED MAPPING
    const { data: providerData, error } = await getSupabase()
      .from('ai_providers')
      .select('provider')
      .eq('name', providerName)
      .single()
    
    if (error || !providerData) {
      console.warn(`Provider ${providerName} not found in database, trying legacy mapping...`)
      
      // Legacy fallback - map old format to provider type
      let legacyProviderType = 'openai' // default
      if (providerName.startsWith('OPENA')) legacyProviderType = 'openai'
      else if (providerName.startsWith('MISTR')) legacyProviderType = 'mistral'
      else if (providerName.startsWith('GEMIN')) legacyProviderType = 'gemini'
      else if (providerName.startsWith('CLAUD')) legacyProviderType = 'claude'
      else if (providerName.startsWith('PERPL')) legacyProviderType = 'perplexity'
      else if (providerName.startsWith('GROK')) legacyProviderType = 'grok'
      else if (providerName.startsWith('COHER')) legacyProviderType = 'cohere'
      
      console.log(`üîç Using legacy provider mapping: ${providerName} -> ${legacyProviderType}`)
      
      // Create a mock provider data object for legacy format
      const mockProviderData = { provider: legacyProviderType }
      return { 
        provider: legacyProviderType, 
        model: modelId, 
        providerName,
        costPer1k: 0.00003 // Default cost
      }
    }
    
    // Query database to get model cost data
    const { data: modelData, error: modelError } = await getSupabase()
      .from('ai_model_metadata')
      .select('input_cost_per_million')
      .eq('key_name', providerName)
      .single()
    
    const costPer1k = modelData?.input_cost_per_million ? (modelData.input_cost_per_million / 1000) : 0.00003 // Convert from per million to per 1k tokens
    
    return { 
      provider: providerData.provider, 
      model: modelId, 
      providerName,
      costPer1k: costPer1k
    }
  }

  getAIService(provider) {
    return this.aiServices[provider] || this.aiServices['ai']
  }

  evaluateCondition(condition, data, pipelineData) {
    const { field, operator, value } = condition
    // Extract field value from JSON wrapper or structuredData
    let fieldValue = pipelineData.userInput[field]
    if (data?.content?.user_input?.[field]) {
      fieldValue = data.content.user_input[field]
    } else if (data?.structuredData?.[field]) {
      fieldValue = data.structuredData[field]
    }

    let passed = false

    switch (operator) {
      case 'equals':
        passed = fieldValue === value || fieldValue === (value === 'true')
        break
      case 'not_equals':
        passed = fieldValue !== value
        break
      case 'contains':
        passed = String(fieldValue).toLowerCase().includes(String(value).toLowerCase())
        break
      case 'greater_than':
        passed = Number(fieldValue) > Number(value)
        break
      case 'less_than':
        passed = Number(fieldValue) < Number(value)
        break
      default:
        passed = false
    }

    return {
      condition,
      fieldValue,
      passed,
      timestamp: new Date()
    }
  }

  async executeConditionAction(action, pipelineData) {
    switch (action.type) {
      case 'generate_content':
        // Execute AI generation for condition-based content
        if (action.prompt) {
          const processedPrompt = this.processPromptVariables({ userPrompt: action.prompt }, pipelineData)
          // Add to pipeline for next node
          pipelineData.conditionGeneration = processedPrompt.userPrompt
        }
        break
      case 'skip_to':
        // Mark node to skip to
        pipelineData.skipToNode = action.target
        break
      case 'continue':
        // Continue normal flow
        break
    }
  }

  executeNonAIProcessing(nodeData, pipelineData) {
    // Handle non-AI process nodes (data transformation, validation, etc.)
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    return {
      type: 'data_processing',
      processedData: previousOutput,
      instructions: nodeData.inputInstructions,
      metadata: {
        nodeId: nodeData.id || 'process-node',
        timestamp: new Date(),
        aiEnabled: false
      }
    }
  }

  compileWorkflowContent(nodeOutputs, userInput) {
    const content = {
      userInput,
      generatedContent: {},
      totalWords: 0,
      totalCharacters: 0,
      sections: [],
      structural: {
        foreword: null,
        introduction: null,
        tableOfContents: null
      },
      assets: {
        images: [], // { chapter, url, prompt, negative_prompt, aspect_ratio, sourceNode }
        cover: null // { url, prompt, layout, style }
      },
      metadata: {
        nodeCount: 0,
        wordCountByNode: {},
        characterCountByNode: {}
      }
    }

    Object.entries(nodeOutputs).forEach(([nodeId, output]) => {
      // EXTRACT STRUCTURAL CONTENT (Foreword, Introduction, TOC) from Structural Node
      const nodeLabel = output.metadata?.nodeName || output.metadata?.label || ''
      const isStructuralNode = /structural|structure|narrative.*architect/i.test(nodeLabel) || 
                               output.metadata?.nodeType === 'structural' ||
                               output.type === 'structural'
      
      if (isStructuralNode && output.content && typeof output.content === 'string') {
        // Extract foreword, introduction, and TOC from structural node output
        const structuralContent = output.content
        
        // Extract Foreword
        const forewordMatch = structuralContent.match(/(?:^|\n)#{1,3}\s*Foreword[\s\S]*?(?=\n#{1,3}\s*(?:Introduction|Table|Chapter|$))/i)
        if (forewordMatch) {
          const forewordText = forewordMatch[0]
            .replace(/^#{1,3}\s*Foreword\s*/i, '')
            .replace(/^#{1,3}\s*Foreword:\s*/i, '')
            .trim()
          if (forewordText.length > 50) {
            content.structural.foreword = this.__sanitizePermissions(forewordText)
            console.log(`üìñ Extracted Foreword from structural node ${nodeId}: ${forewordText.length} chars`)
          }
        }
        
        // Extract Introduction
        const introMatch = structuralContent.match(/(?:^|\n)#{1,3}\s*Introduction[\s\S]*?(?=\n#{1,3}\s*(?:Table|Chapter|$))/i)
        if (introMatch) {
          const introText = introMatch[0]
            .replace(/^#{1,3}\s*Introduction\s*/i, '')
            .replace(/^#{1,3}\s*Introduction:\s*/i, '')
            .trim()
          if (introText.length > 50) {
            content.structural.introduction = this.__sanitizePermissions(introText)
            console.log(`üìñ Extracted Introduction from structural node ${nodeId}: ${introText.length} chars`)
          }
        }
        
        // Extract Table of Contents
        const tocMatch = structuralContent.match(/(?:^|\n)#{1,3}\s*Table\s+of\s+Contents?[\s\S]*?(?=\n#{1,3}\s*(?:Foreword|Introduction|Chapter|$))/i)
        if (tocMatch) {
          const tocText = tocMatch[0]
            .replace(/^#{1,3}\s*Table\s+of\s+Contents?\s*/i, '')
            .replace(/^#{1,3}\s*TOC\s*/i, '')
            .trim()
          if (tocText.length > 20) {
            content.structural.tableOfContents = this.__sanitizePermissions(tocText)
            console.log(`üìñ Extracted TOC from structural node ${nodeId}: ${tocText.length} chars`)
          }
        }
      }
      
      // SURGICAL FIX: Handle image nodes - store as assets, not text content
      const isImageNode = output.type === 'image_generation' || output.type === 'image_skipped'
      
      if (isImageNode) {
        console.log(`üé® STORING image node ${nodeId} as asset (not text content)`)
        
        // Store image data in assets array for later insertion into book
        if (output.type === 'image_generation' && output.imageData) {
          const nodeLabel = output.metadata?.nodeName || nodeId
          const isEcover = nodeLabel.toLowerCase().includes('cover')
          
          if (isEcover) {
            // Store as cover image
            content.assets.cover = {
              imageData: output.imageData,
              inlineData: output.inlineData,
              sourceNode: nodeId,
              metadata: output.metadata
            }
            console.log(`üì∏ Stored e-cover image from ${nodeId}`)
          } else {
            // Store as regular image
            content.assets.images.push({
              imageData: output.imageData,
              inlineData: output.inlineData,
              sourceNode: nodeId,
              metadata: output.metadata
            })
            console.log(`üì∏ Stored image from ${nodeId} (total images: ${content.assets.images.length})`)
          }
        }
        
        return // Don't process as text content
      }
      
      // FIXED: Include all content generation types, not just 'ai_generation'
      // SURGICAL FIX: ONLY include nodes with explicit write permission
      const nodePermissions = output.metadata?.permissions || output.permissions
      const hasWritePermission = nodePermissions?.canWriteContent === true
      
      if ((output.type === 'ai_generation' || output.type === 'multi_chapter_generation' || output.type === 'process') && output.content) {
        
        // CRITICAL PERMISSION CHECK: ONLY include writer nodes in final book
        if (!hasWritePermission) {
          console.log(`üîí SKIPPING node ${nodeId} from book compilation (canWriteContent: ${nodePermissions?.canWriteContent}) - NOT a content writer`)
          return // Skip this node, don't include in final book
        }
        
        console.log(`‚úÖ INCLUDING node ${nodeId} in book compilation (canWriteContent: true) - content writer node`)
        
        // Handle multi-chapter generation (array content)
        if (output.type === 'multi_chapter_generation' && Array.isArray(output.content)) {
          console.log(`üìö Processing multi-chapter content from ${nodeId}: ${output.content.length} chapters`)
          
          // Process each chapter in the array and push as individual sections
          output.content.forEach((chapter, index) => {
            if (chapter && typeof chapter.content === 'string' && chapter.content.trim().length > 0) {
              const chNum = chapter.chapter || index + 1
              const wordCount = chapter.content.split(/\s+/).filter(word => word.length > 0).length
              const charCount = chapter.content.length

              content.totalWords += wordCount
              content.totalCharacters += charCount
              content.metadata.wordCountByNode[`${nodeId}_chapter_${chNum}`] = wordCount
              content.metadata.characterCountByNode[`${nodeId}_chapter_${chNum}`] = charCount
              content.metadata.nodeCount++

              content.sections.push({
                nodeId,
                title: chapter.title || `Chapter ${chNum}`,
                chapterNumber: chNum,
                content: chapter.content,
                metadata: { ...(output.metadata || {}), chapter: chNum },
                contentType: 'chapter'
              })

              console.log(`üìä Node ${nodeId} Chapter ${chNum} word count: ${wordCount} words, ${charCount} characters`)
            }
          })

          // Store the complete multi-chapter content for reference
          content.generatedContent[nodeId] = output.content
        }
        // Handle single content generation (string content)
        else if (typeof output.content === 'string') {
          // DYNAMIC: Check if content contains multiple chapters (separated by --- or multiple ## Chapter patterns)
          const multipleChapterPattern = /---[\s\S]*?##\s+Chapter\s+\d+/gi
          const hasMultipleChapters = multipleChapterPattern.test(output.content) || 
                                     (output.content.match(/##\s+Chapter\s+\d+/gi) || []).length > 1
          
          if (hasMultipleChapters) {
            // SPLIT: Content contains multiple chapters - split and extract each one
            console.log(`üìö Node ${nodeId}: Detected multiple chapters in single string, splitting...`)
            
            // Split by --- separator (most common) or by chapter headers if no separators
            let chapterParts = []
            if (output.content.includes('---')) {
              // Split by --- and filter empty parts + skip TOC/intro sections
              const allParts = output.content.split(/---+/).filter(part => part.trim().length > 0)
              
              // Filter out table of contents and intro sections (don't contain ## Chapter headers)
              chapterParts = allParts.filter(part => {
                const hasChapterHeader = /##\s+Chapter\s+\d+/gi.test(part)
                const isTOC = /table\s+of\s+contents|contents?|toc/gi.test(part) && !hasChapterHeader
                // Keep parts with chapter headers OR parts > 500 chars (likely content, not just TOC)
                return hasChapterHeader || (!isTOC && part.trim().length > 500)
              })
            } else {
              // No separators - split by chapter headers
              const chapterHeaderRegex = /(##\s+Chapter\s+\d+.*?(?=##\s+Chapter\s+\d+|$))/gsi
              chapterParts = [...output.content.matchAll(chapterHeaderRegex)].map(match => match[1])
            }
            
            console.log(`üìö Node ${nodeId}: Split into ${chapterParts.length} potential chapter parts`)
            
            // Extract each chapter part
            chapterParts.forEach((chapterPart, partIndex) => {
              const chapterStructure = this.__extractChapterStructure(chapterPart.trim())
              
              if (chapterStructure && chapterStructure.hasStructure) {
                // Valid chapter extracted
                const wordCount = chapterStructure.cleanContent.split(/\s+/).filter(word => word.length > 0).length
                const charCount = chapterStructure.cleanContent.length
                
                content.totalWords += wordCount
                content.totalCharacters += charCount
                content.metadata.wordCountByNode[`${nodeId}_chapter_${chapterStructure.chapterNumber}`] = wordCount
                content.metadata.characterCountByNode[`${nodeId}_chapter_${chapterStructure.chapterNumber}`] = charCount
                content.metadata.nodeCount++
                
                console.log(`üìä Node ${nodeId} Chapter ${chapterStructure.chapterNumber} (from split) word count: ${wordCount} words`)
                
                content.sections.push({
                  nodeId,
                  title: chapterStructure.title,
                  chapterNumber: chapterStructure.chapterNumber,
                  content: chapterStructure.cleanContent,
                  metadata: { 
                    ...(output.metadata || {}), 
                    chapter: chapterStructure.chapterNumber,
                    extractedTitle: chapterStructure.title,
                    splitFromMultiChapter: true
                  },
                  contentType: 'chapter'
                })
              } else if (chapterPart.trim().length > 100) {
                // Part is long enough to be content but no chapter structure - treat as content section
                const sanitized = this.__sanitizePermissions(chapterPart.trim())
                const wordCount = sanitized.split(/\s+/).filter(word => word.length > 0).length
                const charCount = sanitized.length
                
                content.totalWords += wordCount
                content.totalCharacters += charCount
                content.metadata.wordCountByNode[`${nodeId}_part_${partIndex + 1}`] = wordCount
                content.metadata.characterCountByNode[`${nodeId}_part_${partIndex + 1}`] = charCount
                content.metadata.nodeCount++
                
                content.sections.push({
                  nodeId,
                  title: output.metadata?.title || output.metadata?.nodeName || `Section ${content.sections.length + 1}`,
                  content: sanitized,
                  metadata: { 
                    ...(output.metadata || {}),
                    sectionIndex: partIndex + 1,
                    splitFromMultiChapter: true
                  },
                  contentType: 'single_content'
                })
              }
            })
            
            console.log(`‚úÖ Node ${nodeId}: Split ${chapterParts.length} parts into ${content.sections.length - (content.sections.length - chapterParts.length)} sections`)
          } else {
            // SINGLE: Content is single chapter or single content block
            const chapterStructure = this.__extractChapterStructure(output.content)
            
            if (chapterStructure && chapterStructure.hasStructure) {
              // Chapter structure detected - create standardized section
              const wordCount = chapterStructure.cleanContent.split(/\s+/).filter(word => word.length > 0).length
              const charCount = chapterStructure.cleanContent.length
              
              content.totalWords += wordCount
              content.totalCharacters += charCount
              content.metadata.wordCountByNode[`${nodeId}_chapter_${chapterStructure.chapterNumber}`] = wordCount
              content.metadata.characterCountByNode[`${nodeId}_chapter_${chapterStructure.chapterNumber}`] = charCount
              content.metadata.nodeCount++
              
              console.log(`üìä Node ${nodeId} Chapter ${chapterStructure.chapterNumber} word count: ${wordCount} words, ${charCount} characters`)
              
              content.sections.push({
                nodeId,
                title: chapterStructure.title,
                chapterNumber: chapterStructure.chapterNumber,
                content: chapterStructure.cleanContent,
                metadata: { 
                  ...(output.metadata || {}), 
                  chapter: chapterStructure.chapterNumber,
                  extractedTitle: chapterStructure.title
                },
                contentType: 'chapter'
              })
            } else {
              // No chapter structure - treat as single content but ensure title field exists
              const wordCount = output.content.split(/\s+/).filter(word => word.length > 0).length
              const charCount = output.content.length
              
              content.totalWords += wordCount
              content.totalCharacters += charCount
              content.metadata.wordCountByNode[nodeId] = wordCount
              content.metadata.characterCountByNode[nodeId] = charCount
              content.metadata.nodeCount++
              
              console.log(`üìä Node ${nodeId} word count: ${wordCount} words, ${charCount} characters`)
              
              // Extract title from metadata or use node-based fallback (DYNAMIC - no hardcoding)
              const sectionTitle = output.metadata?.title || 
                                  output.metadata?.nodeName || 
                                  `Section ${content.sections.length + 1}`
              
              // Store single content
              content.generatedContent[nodeId] = output.content
              content.sections.push({
                nodeId,
                title: sectionTitle,
                content: this.__sanitizePermissions(output.content),
                metadata: { 
                  ...(output.metadata || {}),
                  sectionIndex: content.sections.length + 1
                },
                contentType: 'single_content'
              })
            }
          }
        }
        // Handle other content types (objects, etc.)
        else {
          console.log(`üìä Node ${nodeId} content type: ${typeof output.content}, processing as-is`)
          
          content.generatedContent[nodeId] = output.content
          content.sections.push({
            nodeId,
            content: output.content,
            metadata: output.metadata,
            contentType: typeof output.content
          })
        }
      }
    })

    // Extract image assets from any image node outputs
    try {
      Object.entries(nodeOutputs).forEach(([nodeId, output]) => {
        const c = output?.content
        if (!c) return
        // Common shapes: { images: [...] } or array of images
        const images = Array.isArray(c?.images) ? c.images : (Array.isArray(c) && c.length && c[0]?.url ? c : [])
        if (images && images.length) {
          images.forEach((img) => {
            if (!img || !img.url) return
            content.assets.images.push({
              sourceNode: nodeId,
              chapter: output?.metadata?.chapter || img.chapter || null,
              url: img.url,
              prompt: img.prompt || '',
              negative_prompt: img.negative_prompt || '',
              aspect_ratio: img.aspect_ratio || ''
            })
          })
        }
        // Heuristic: if node appears to be an e‚Äëcover generator, treat first image as cover
        if (!content.assets.cover && (output?.metadata?.role === 'ecover_generator' || /cover|ecover/i.test(output?.metadata?.role || ''))) {
          const first = images && images[0]
          if (first && first.url) {
            content.assets.cover = {
              url: first.url,
              prompt: first.prompt || '',
              layout: first.layout || output?.metadata?.layout || '',
              style: first.style || output?.metadata?.style || ''
            }
          }
        }
      })
    } catch (err) {
      console.warn('‚ö†Ô∏è Image asset extraction warning:', err?.message)
    }

    console.log(`üìä Total compiled content: ${content.totalWords} words, ${content.totalCharacters} characters`)
    console.log(`üìä Total sections created: ${content.sections.length}`)
    
    // CRITICAL: Log section details for debugging
    if (content.sections.length === 0) {
      console.error('‚ùå CRITICAL: NO SECTIONS CREATED!')
      console.error('   - Node outputs:', Object.keys(nodeOutputs))
      console.error('   - Node output types:', Object.values(nodeOutputs).map(o => o?.type))
      console.error('   - Total words:', content.totalWords)
      console.error('   - Total characters:', content.totalCharacters)
      
      // EMERGENCY FALLBACK: Create at least one section from generatedContent if sections are empty
      const firstContent = Object.values(content.generatedContent)[0]
      if (firstContent && typeof firstContent === 'string' && firstContent.trim().length > 100) {
        console.warn('‚ö†Ô∏è EMERGENCY FALLBACK: Creating section from generatedContent')
        const wordCount = firstContent.split(/\s+/).filter(word => word.length > 0).length
        content.sections.push({
          nodeId: Object.keys(nodeOutputs)[0] || 'fallback',
          title: 'Content Section',
          content: this.__sanitizePermissions(firstContent),
          metadata: { emergencyFallback: true },
          contentType: 'single_content'
        })
        content.totalWords = wordCount
        content.totalCharacters = firstContent.length
        console.warn('‚ö†Ô∏è Emergency section created with', wordCount, 'words')
      }
    } else {
      console.log('‚úÖ Sections created successfully:')
      content.sections.forEach((section, idx) => {
        console.log(`   Section ${idx + 1}:`, {
          title: section.title,
          chapterNumber: section.chapterNumber,
          contentType: section.contentType,
          contentLength: typeof section.content === 'string' ? section.content.length : 'NOT STRING',
          hasContent: !!section.content && (typeof section.content === 'string' ? section.content.trim().length > 0 : false)
        })
      })
    }
    
    return content
  }

  // REMOVED: determineOptimalChapterCount() function - AI should not override user input

  // Helper: sanitize banners/quadrails/instruction echoes before export
  __sanitizePermissions(text) {
    if (typeof text !== 'string') return text
    let out = text
    
    // Remove permission blocks with ‚ïê‚ïê separators (including the "Only perform tasks" line)
    out = out.replace(/(^|\n)[‚ïê=]{6,}[\s\S]*?(Only perform tasks[\s\S]*?)?[‚ïê=]{6,}\s*/g, '\n')
    
    // Remove instruction text patterns
    out = out.replace(/Only perform tasks you are explicitly authorized for above\.?/gi, '')
    out = out.replace(/‚ö†Ô∏è\s*CRITICAL:\s*Violating these permissions.*?$/gmi, '')
    out = out.replace(/Only perform tasks.*?authorized.*?above\.?/gi, '')
    out = out.replace(/^\s*(?:‚úÖ|üö´)\s*(AUTHORIZED|FORBIDDEN):.*$/gmi, '')
    out = out.replace(/^\s*‚ö†Ô∏è\s*CRITICAL:.*$/gmi, '')
    out = out.replace(/^\s*NODE PERMISSIONS.*$/gmi, '')
    out = out.replace(/^\s*üîê\s*NODE PERMISSIONS.*$/gmi, '')
    out = out.replace(/^\s*STRICTLY ENFORCED.*$/gmi, '')
    out = out.replace(/^\s*(instructions|node instructions|permission instructions)\s*[:\-]*\s*/gmi, '')
    
    // Remove instruction-like phrases that might appear in content
    out = out.replace(/You are (only|explicitly) (authorized|allowed|permitted).*?above\.?/gi, '')
    out = out.replace(/Violating (these )?permissions.*?$/gi, '')
    out = out.replace(/This output must (only|exclusively).*?$/gi, '')
    
    // Remove markdown code blocks but keep content
    out = out.replace(/^```(?:\w+)?\s*([\s\S]*?)```/m, '$1')
    
    // Remove template variables
    out = out.replace(/\{[A-Za-z]\}/g, '')
    
    // Clean up excessive whitespace
    out = out.replace(/\n{3,}/g, '\n\n').trim()
    
    return out
  }

  /**
   * Extract chapter structure from markdown content dynamically
   * Detects various chapter header formats and extracts number + title
   * Handles multiple header formats in same content (e.g., ## Chapter 1 and **Chapter 1: Title**)
   * @param {string} content - Raw markdown content
   * @returns {Object|null} - {chapterNumber, title, cleanContent} or null if not a chapter
   */
  __extractChapterStructure(content) {
    if (!content || typeof content !== 'string') return null
    
    const sanitized = this.__sanitizePermissions(content)
    
    let chapterNumber = null
    let title = null
    let cleanContent = sanitized
    
    // Pattern 1: ## Chapter N: Title or ## Chapter N (markdown header)
    const headerPattern = /^##\s+Chapter\s+(\d+)(?:\s*:\s*(.+?))?(?:\s*$|\n)/mi
    const headerMatch = sanitized.match(headerPattern)
    if (headerMatch) {
      chapterNumber = parseInt(headerMatch[1], 10)
      const headerTitle = (headerMatch[2] || '').trim()
      if (headerTitle) title = headerTitle
      // Remove this header
      cleanContent = cleanContent.replace(headerPattern, '').trim()
    }
    
    // Pattern 2: **Chapter N: Title** or **Chapter N** (bold text - often has the actual title)
    const boldPattern = /^\*\*Chapter\s+(\d+)(?:\s*:\s*(.+?))\*\*(?:\s*$|\n)/mi
    const boldMatch = cleanContent.match(boldPattern)
    if (boldMatch) {
      const boldChapterNum = parseInt(boldMatch[1], 10)
      const boldTitle = (boldMatch[2] || '').trim()
      
      // If chapter number matches or wasn't set, use this one (it often has better title)
      if (!chapterNumber || boldChapterNum === chapterNumber) {
        chapterNumber = boldChapterNum
        if (boldTitle) title = boldTitle // Prefer title from bold format as it's often more complete
      }
      // Remove this bold header
      cleanContent = cleanContent.replace(boldPattern, '').trim()
    }
    
    // Pattern 3: # Chapter N: Title or # Chapter N (single hash header)
    if (!chapterNumber) {
      const singleHashPattern = /^#\s+Chapter\s+(\d+)(?:\s*:\s*(.+?))?(?:\s*$|\n)/mi
      const singleHashMatch = cleanContent.match(singleHashPattern)
      if (singleHashMatch) {
        chapterNumber = parseInt(singleHashMatch[1], 10)
        const singleHashTitle = (singleHashMatch[2] || '').trim()
        if (singleHashTitle) title = singleHashTitle
        cleanContent = cleanContent.replace(singleHashPattern, '').trim()
      }
    }
    
    // Pattern 4: Chapter N: Title (plain text at start)
    if (!chapterNumber) {
      const plainPattern = /^Chapter\s+(\d+)(?:\s*:\s*(.+?))(?:\s*$|\n)/mi
      const plainMatch = cleanContent.match(plainPattern)
      if (plainMatch) {
        chapterNumber = parseInt(plainMatch[1], 10)
        const plainTitle = (plainMatch[2] || '').trim()
        if (plainTitle) title = plainTitle
        cleanContent = cleanContent.replace(plainPattern, '').trim()
      }
    }
    
    // If we found a chapter number, return structured data
    if (chapterNumber) {
      // Remove any remaining duplicate headers for this chapter number
      cleanContent = cleanContent
        .replace(new RegExp(`^##?\\s+Chapter\\s+${chapterNumber}(?:\\s*:\\s*.+?)?(?=\\s|$)`, 'gmi'), '')
        .replace(new RegExp(`^\\*\\*Chapter\\s+${chapterNumber}(?:\\s*:\\s*.+?)\\*\\*(?=\\s|$)`, 'gmi'), '')
        .replace(new RegExp(`^#\\s+Chapter\\s+${chapterNumber}(?:\\s*:\\s*.+?)?(?=\\s|$)`, 'gmi'), '')
        // Remove word count markers like "(Word Count: 3495)" or "**Word Count: 3495**"
        .replace(/\s*\(Word\s+Count:\s*\d+\)\s*$/gmi, '')
        .replace(/\s*\*\*Word\s+Count:\s*\d+\*\*\s*$/gmi, '')
        .replace(/\s*Word\s+Count:\s*\d+\s*$/gmi, '')
        // Clean up excessive whitespace
        .replace(/\n{3,}/g, '\n\n')
        .trim()
      
      // Final safety check - ensure we have actual content
      if (cleanContent.length < 50) {
        // Content too short - might have removed too much, use original but without headers
        cleanContent = sanitized
          .replace(headerPattern, '')
          .replace(boldPattern, '')
          .trim()
      }
      
      return {
        chapterNumber,
        title: title || `Chapter ${chapterNumber}`, // Fallback to generic title if none found
        cleanContent: cleanContent,
        hasStructure: true
      }
    }
    
    // No chapter structure found - return null to indicate single content
    return null
  }

  async formatFinalOutput(compiledContent, formatOptions) {
    const { outputFormat = 'markdown', exportFormats = [] } = formatOptions
    
    // If exportFormats is provided (from input node), use those instead of single outputFormat
    let formatsToGenerate = exportFormats.length > 0 ? exportFormats : [outputFormat]
    
    // ALWAYS generate HTML for preview - add it if not already present
    if (!formatsToGenerate.includes('html') && !formatsToGenerate.includes('HTML')) {
      formatsToGenerate = ['html', ...formatsToGenerate]
      console.log('üìÑ Added HTML format for preview')
    }
    
    console.log('üéØ Formatting output for formats:', formatsToGenerate)
    console.log('üéØ Format types:', formatsToGenerate.map(f => typeof f))
    console.log('üìä Compiled content:', compiledContent)
    
    const formattedOutputs = {}
    
    // Generate content for each requested format using PROFESSIONAL FORMATTER
    for (const format of formatsToGenerate) {
      // CRITICAL FIX: Ensure format is a string, not a character
      const formatStr = typeof format === 'string' ? format : String(format)
      console.log(`üéØ Processing format: "${formatStr}" (type: ${typeof format})`)
      
      // Skip if format is empty or just a single character (likely from string splitting bug)
      if (!formatStr || formatStr.length < 2) {
        console.log(`‚ö†Ô∏è Skipping invalid format: "${formatStr}"`)
        continue
      }
      try {
        // Use PROFESSIONAL FORMATTER for all formats
        const userInput = compiledContent.userInput || {}
        
        // Extract the actual book content from the compiled workflow data
        // PRIORITY: Use sections (structured chapters) if available, otherwise fallback to generatedContent
        let bookContent = ''
        
        // CRITICAL FIX: Build bookContent from sections FIRST (this is where chapters are!)
        if (compiledContent.sections && Array.isArray(compiledContent.sections) && compiledContent.sections.length > 0) {
          console.log(`üìö Building bookContent from ${compiledContent.sections.length} sections`)
          // Build markdown from structured sections
          bookContent = compiledContent.sections
            .filter(section => section && section.content && typeof section.content === 'string' && section.content.trim().length > 0)
            .map((section, idx) => {
              const title = section.title || section.metadata?.title || `Chapter ${section.chapterNumber || section.metadata?.chapterNumber || (idx + 1)}`
              const body = this.__sanitizePermissions(section.content || '')
              return `## ${title}\n\n${body}`
            })
            .join('\n\n---\n\n')
          console.log(`üìö Built bookContent from sections: ${bookContent.length} characters`)
        } else if (compiledContent.generatedContent) {
          // Fallback: Get the main content from the last process node that generated book content
          console.log(`üìö Building bookContent from generatedContent (fallback)`)
          const contentNodes = Object.values(compiledContent.generatedContent)
          const lastContent = contentNodes[contentNodes.length - 1] || ''

          if (Array.isArray(lastContent)) {
            // Assemble markdown from chapters
            bookContent = lastContent
              .filter(ch => ch && typeof ch.content === 'string' && ch.content.trim().length > 0)
              .map((ch, idx) => {
                const title = ch.title || `Chapter ${ch.chapter || idx + 1}`
                const body = this.__sanitizePermissions(ch.content || '')
                return `## ${title}\n\n${body}`
              })
              .join('\n\n---\n\n')
          } else if (typeof lastContent === 'string') {
            // CRITICAL FIX: Clean multi-chapter text before formatting
            bookContent = this.__sanitizePermissions(lastContent)
              .replace(/<[^>]*>/g, '')
              .replace(/font-family[^;]*;?/g, '')
              .replace(/font-weight[^;]*;?/g, '')
              .replace(/font-size[^;]*;?/g, '')
              .replace(/margin[^;]*;?/g, '')
              .replace(/line-height[^;]*;?/g, '')
              .replace(/"[^"]*"/g, '')
              .replace(/\n{3,}/g, '\n\n')
              .trim()
          } else {
            bookContent = ''
          }
        } else {
          bookContent = this.__sanitizePermissions(compiledContent.content || compiledContent)
        }
        
        console.log(`üìö Generating ${formatStr} format`)
        console.log(`üìö Book content length:`, typeof bookContent === 'string' ? bookContent.length : 'Not a string')
        console.log(`üìö User input:`, Object.keys(userInput))
        
        // SURGICAL FIX: ALWAYS use professionalBookFormatter first for consistent beautiful formatting
        console.log(`üìö SURGICAL FIX: Using professionalBookFormatter for ALL formats: ${formatStr}`)
        
        // Extract user typography preferences with PROFESSIONAL DEFAULTS
        const typographyPrefs = {
          // Font families with professional hierarchy
          fontFamily: userInput.typography_combo || userInput.font_family || 'Georgia, serif',
          headingFont: userInput.heading_font || 'Impact, Arial Black, sans-serif',
          bodyFont: userInput.body_font || 'Tahoma, Arial, sans-serif',
          chapterFont: userInput.chapter_font || 'Georgia, Times New Roman, serif',
          
          // Font sizes with professional hierarchy  
          fontSize: userInput.font_size || '12pt',
          headingSize: userInput.heading_size || '36pt',
          chapterSize: userInput.chapter_size || '16pt',
          
          // Font weights with professional hierarchy
          fontWeight: userInput.font_weight || 'normal',
          headingWeight: userInput.heading_weight || 'bold', 
          chapterWeight: userInput.chapter_weight || '600',
          
          // Layout and style
          writingStyle: userInput.writing_style || 'descriptive',
          tone: userInput.tone || 'professional',
          format: formatStr,
          pageSize: userInput.book_size || 'A5',
          margins: userInput.margins || '1in',
          lineHeight: userInput.line_height || '1.8',
          textAlign: userInput.text_align || 'justify',
          colorScheme: userInput.color_scheme || 'classic'
        }
        
        // SURGICAL: Load template if user specified one
        let templateData = null
        if (userInput.book_template && userInput.book_template !== 'none') {
          console.log(`üìö Loading template: ${userInput.book_template}`)
          templateData = await professionalBookFormatter.loadTemplate(userInput.book_template)
          if (templateData) {
            // Override typographyPrefs with template settings
            Object.assign(typographyPrefs, {
              fontFamily: templateData.font_family || typographyPrefs.fontFamily,
              headingFont: templateData.heading_font || typographyPrefs.headingFont,
              bodyFont: templateData.body_font || typographyPrefs.bodyFont,
              chapterFont: templateData.chapter_font || typographyPrefs.chapterFont,
              fontSize: templateData.font_size || typographyPrefs.fontSize,
              headingSize: templateData.heading_size || typographyPrefs.headingSize,
              chapterSize: templateData.chapter_size || typographyPrefs.chapterSize,
              fontWeight: templateData.font_weight || typographyPrefs.fontWeight,
              headingWeight: templateData.heading_weight || typographyPrefs.headingWeight,
              chapterWeight: templateData.chapter_weight || typographyPrefs.chapterWeight,
              pageSize: templateData.page_size || typographyPrefs.pageSize,
              margins: templateData.margins || typographyPrefs.margins,
              lineHeight: templateData.line_height || typographyPrefs.lineHeight,
              textAlign: templateData.text_align || typographyPrefs.textAlign,
              colorScheme: templateData.color_scheme || typographyPrefs.colorScheme
            })
            console.log(`‚úÖ Template applied: ${userInput.book_template}`)
          }
        }
        
        // STEP 1: Generate beautiful formatted content using professionalBookFormatter
        const beautifulContent = await professionalBookFormatter.formatCompleteBook(
          this.__sanitizePermissions(bookContent), 
          userInput, 
          formatStr,
          { ...typographyPrefs, template: templateData }
        )
        
        // STEP 2: For binary formats, pass beautiful content to exportService for conversion
        if (['pdf', 'docx', 'epub'].includes(formatStr.toLowerCase())) {
          console.log(`üìö Converting beautiful ${formatStr} content to binary format`)
          
          // Prepare enhanced data with beautiful pre-formatted content
          const enhancedData = {
            userInput: userInput,
            sections: compiledContent.sections || [],
            totalWords: compiledContent.totalWords || 0,
            metadata: compiledContent.metadata || {},
            content: bookContent,
            beautifulContent: beautifulContent,  // SURGICAL: Pass beautiful formatted content
            typographyPrefs: typographyPrefs     // SURGICAL: Pass typography preferences
          }
          
          switch (formatStr.toLowerCase()) {
            case 'pdf':
              formattedOutputs[formatStr] = await exportService.generatePDF(enhancedData)
              break
            case 'docx':
              console.log('üîß SURGICAL: Converting beautiful content to DOCX')
              formattedOutputs[formatStr] = await exportService.generateDOCX(enhancedData)
              break
            case 'epub':
              formattedOutputs[formatStr] = await exportService.generateEPUB(enhancedData)
              break
          }
        } else {
          // STEP 4: For text formats, use beautiful content directly
          console.log(`üìö Using beautiful ${formatStr} content directly`)
          formattedOutputs[formatStr] = beautifulContent
        }
        
        const contentLength = typeof formattedOutputs[formatStr] === 'string' ? 
          formattedOutputs[formatStr].length : 
          JSON.stringify(formattedOutputs[formatStr]).length
        
        console.log(`‚úÖ Generated PROFESSIONAL ${formatStr} format (${contentLength} chars)`)
      } catch (error) {
        console.error(`‚ùå Error generating ${formatStr} format:`, error)
        // NO FALLBACK - FAIL FAST AND REPORT ERROR
        throw new Error(`Professional formatter failed for ${formatStr}: ${error.message}`)
      }
    }
    
    // ALWAYS use text/markdown for preview display, NEVER blob URLs
    const previewFormat = formattedOutputs['text'] || formattedOutputs['markdown'] || formattedOutputs['html'] || formattedOutputs['HTML']
    const primaryFormat = 'text'  // Always text for display
    
    console.log('üìÑ Preview format content type:', typeof previewFormat)
    console.log('üìÑ All formats generated:', Object.keys(formattedOutputs))
    
    return {
      content: previewFormat,  // Text/Markdown for display
      allFormats: formattedOutputs,
      requestedFormats: formatsToGenerate,
      primaryFormat: primaryFormat
    }
  }

  generateMarkdownOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const totalWords = compiledContent.totalWords || 0
    
    let markdown = `# ${title}\n\n`
    markdown += `**Author:** ${author}\n\n`
    markdown += `**Generated:** ${new Date().toLocaleDateString()}\n\n`
    markdown += `**Total Words:** ${totalWords.toLocaleString()}\n\n`
    markdown += `---\n\n`
    
    // Add table of contents if multiple sections
    if (compiledContent.sections.length > 1) {
      markdown += `## Table of Contents\n\n`
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        markdown += `${index + 1}. [${sectionTitle}](#${sectionTitle.toLowerCase().replace(/\s+/g, '-')})\n`
      })
      markdown += `\n---\n\n`
    }
    
    // Add content sections with clean formatting
    compiledContent.sections.forEach((section, index) => {
      const sectionTitle = section.title || `Chapter ${index + 1}`
      markdown += `## ${sectionTitle}\n\n`
      
      // Clean up the content - preserve paragraph structure and format properly
      let cleanContent = section.content
        .replace(/\n{4,}/g, '\n\n\n') // Replace excessive newlines with triple newlines (preserve paragraph breaks)
        .replace(/^\s+|\s+$/gm, '') // Trim whitespace from each line
        .replace(/[ \t]{2,}/g, ' ') // Replace multiple spaces/tabs with single space (preserve newlines)
        .replace(/([.!?])\s*([A-Z])/g, '$1\n\n$2') // Add paragraph breaks after sentences
        .replace(/([.!?])\s*\n\s*([A-Z])/g, '$1\n\n$2') // Ensure proper paragraph spacing
      
      markdown += `${cleanContent}\n\n`
      
      // Add page break between chapters (except for the last one)
      if (index < compiledContent.sections.length - 1) {
        markdown += `---\n\n`
      }
    })
    
    return markdown
  }

  generateHTMLOutput(compiledContent) {
    // Generate clean, professional HTML format
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const totalWords = compiledContent.totalWords || 0
    
    let html = `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>${title}</title>
    <style>
        body { 
            font-family: 'Georgia', 'Times New Roman', serif; 
            line-height: 1.6; 
            margin: 0; 
            padding: 40px; 
            max-width: 800px; 
            margin: 0 auto; 
            background: #fff;
            color: #333;
        }
        h1 { 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db; 
            padding-bottom: 15px; 
            text-align: center;
            margin-bottom: 30px;
        }
        h2 { 
            color: #34495e; 
            margin-top: 40px; 
            margin-bottom: 20px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        .metadata { 
            background: #f8f9fa; 
            padding: 20px; 
            border-radius: 8px; 
            margin-bottom: 30px; 
            border-left: 4px solid #3498db;
        }
        .section { 
            margin-bottom: 40px; 
            padding: 20px;
            background: #fafafa;
            border-radius: 5px;
        }
        .toc {
            background: #e8f4f8;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        .toc h2 {
            margin-top: 0;
            color: #2c3e50;
        }
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        .toc li {
            padding: 5px 0;
            border-bottom: 1px solid #ddd;
        }
        .toc li:last-child {
            border-bottom: none;
        }
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        hr {
            border: none;
            border-top: 2px solid #ecf0f1;
            margin: 30px 0;
        }
    </style>
</head>
<body>
    <h1>${title}</h1>
    <div class="metadata">
        <p><strong>Author:</strong> ${author}</p>
        <p><strong>Generated:</strong> ${new Date().toLocaleDateString()}</p>
        <p><strong>Total Words:</strong> ${totalWords.toLocaleString()}</p>
    </div>`
    
    // Add table of contents if multiple sections
    if (compiledContent.sections.length > 1) {
      html += `
    <div class="toc">
        <h2>Table of Contents</h2>
        <ul>`
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        html += `<li>${index + 1}. ${sectionTitle}</li>`
      })
      html += `
        </ul>
    </div>`
    }
    
    // Add content sections with clean formatting
    compiledContent.sections.forEach((section, index) => {
      const sectionTitle = section.title || `Chapter ${index + 1}`
      
      // Clean up the content
      let cleanContent = section.content
        .replace(/\n{3,}/g, '\n\n') // Replace multiple newlines with double newlines
        .replace(/^\s+|\s+$/gm, '') // Trim whitespace from each line
        .replace(/\s{2,}/g, ' ') // Replace multiple spaces with single space
        .replace(/\n/g, '</p><p>') // Convert newlines to paragraph breaks
      
      html += `
    <div class="section">
        <h2>${sectionTitle}</h2>
        <p>${cleanContent}</p>
    </div>`
    })
    
    html += `
</body>
</html>`
    
    return html
  }

  generatePlainTextOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const totalWords = compiledContent.totalWords || 0
    
    let textContent = `${title}\n\n`
    textContent += `Author: ${author}\n\n`
    textContent += `Generated: ${new Date().toLocaleDateString()}\n\n`
    textContent += `Total Words: ${totalWords.toLocaleString()}\n\n`
    textContent += `${'='.repeat(50)}\n\n`
    
    // Add table of contents if multiple sections
    if (compiledContent.sections.length > 1) {
      textContent += `Table of Contents\n\n`
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        textContent += `${index + 1}. ${sectionTitle}\n`
      })
      textContent += `\n${'='.repeat(50)}\n\n`
    }
    
    // Add content sections with clean formatting
    compiledContent.sections.forEach((section, index) => {
      const sectionTitle = section.title || `Chapter ${index + 1}`
      textContent += `${sectionTitle}\n\n`
      
      // Clean up the content - remove excessive whitespace and format properly
      let cleanContent = section.content
        .replace(/\n{3,}/g, '\n\n') // Replace multiple newlines with double newlines
        .replace(/^\s+|\s+$/gm, '') // Trim whitespace from each line
        .replace(/\s{2,}/g, ' ') // Replace multiple spaces with single space
      
      textContent += `${cleanContent}\n\n`
      
      // Add page break between chapters (except for the last one)
      if (index < compiledContent.sections.length - 1) {
        textContent += `${'='.repeat(50)}\n\n`
      }
    })
    
    return textContent
  }

  async generateFormatOutput(compiledContent, format) {
    // Simple, dynamic format generation - whatever the user selected
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const content = compiledContent.sections.map(section => section.content).join('\n\n')
    
    // Generate based on format - simple and dynamic
    switch (format.toLowerCase()) {
      case 'json':
        return JSON.stringify(compiledContent, null, 2)
      
      case 'markdown':
      case 'md':
        return this.generateMarkdownOutput(compiledContent)
      
      case 'html':
        return this.generateHTMLOutput(compiledContent)
      
      case 'pdf':
        return this.generatePDFOutput(compiledContent)
      
      case 'epub':
        return await this.generateEPUBOutput(compiledContent)
      
      case 'docx':
        return await this.generateDOCXOutput(compiledContent)
      
      case 'text':
      case 'txt':
      default:
        return content
    }
  }

  generatePDFOutput(compiledContent) {
    try {
      // Import jsPDF with compatibility for different versions
      const jsPDF = require('jspdf').jsPDF || require('jspdf')
      
      const title = compiledContent.userInput.book_title || 'Generated Content'
      const author = compiledContent.userInput.author_name || 'AI Generated'
      const totalWords = compiledContent.totalWords || 0
      
      // Create new PDF document
      const doc = new jsPDF()
      
      // Set font and add title
      doc.setFontSize(20)
      doc.setFont(undefined, 'bold')
      doc.text(title, 20, 30)
      
      // Add author and metadata
      doc.setFontSize(12)
      doc.setFont(undefined, 'normal')
      doc.text(`Author: ${author}`, 20, 45)
      doc.text(`Generated: ${new Date().toLocaleDateString()}`, 20, 55)
      doc.text(`Total Words: ${totalWords.toLocaleString()}`, 20, 65)
      
      let yPosition = 85
      const pageHeight = doc.internal.pageSize.height
      const margin = 20
      const lineHeight = 7
      
      // Add table of contents if multiple sections
      if (compiledContent.sections.length > 1) {
        doc.setFontSize(16)
        doc.setFont(undefined, 'bold')
        doc.text('Table of Contents', 20, yPosition)
        yPosition += 15
        
        doc.setFontSize(12)
        doc.setFont(undefined, 'normal')
        compiledContent.sections.forEach((section, index) => {
          const sectionTitle = section.title || `Chapter ${index + 1}`
          doc.text(`${index + 1}. ${sectionTitle}`, 25, yPosition)
          yPosition += lineHeight
          
          // Check if we need a new page
          if (yPosition > pageHeight - 30) {
            doc.addPage()
            yPosition = 30
          }
        })
        yPosition += 10
      }
      
      // Add content sections
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        
        // Add chapter title
        doc.setFontSize(16)
        doc.setFont(undefined, 'bold')
        doc.text(sectionTitle, 20, yPosition)
        yPosition += 15
        
        // Add chapter content
        doc.setFontSize(11)
        doc.setFont(undefined, 'normal')
        
        // Split content into lines that fit the page width
        const content = section.content.replace(/\n{3,}/g, '\n\n').replace(/^\s+|\s+$/gm, '').replace(/\s{2,}/g, ' ')
        const lines = doc.splitTextToSize(content, 170) // 170mm width
        
        lines.forEach(line => {
          // Check if we need a new page
          if (yPosition > pageHeight - 30) {
            doc.addPage()
            yPosition = 30
          }
          
          doc.text(line, 20, yPosition)
          yPosition += lineHeight
        })
        
        yPosition += 10
      })
      
      // Return PDF as base64 string for download
      return doc.output('datauristring')
      
    } catch (error) {
      console.error('Error generating PDF:', error)
      // NO FALLBACK - THROW ERROR INSTEAD
      throw new Error(`PDF generation failed: ${error.message}`)
    }
  }

  async generateEPUBOutput(compiledContent) {
    // TEMPORARILY DISABLED: epub-gen is a Node.js library and cannot run in browser
    throw new Error('EPUB generation is temporarily unavailable. The epub-gen library requires Node.js and cannot run in the browser. Please use PDF, DOCX, or Markdown formats instead.')
    
    /* ORIGINAL CODE COMMENTED OUT FOR FUTURE SERVER-SIDE IMPLEMENTATION
    try {
      // Import epub-gen dynamically to avoid SSR issues
      const epub = require('epub-gen')
      
      const title = compiledContent.userInput.book_title || 'Generated Content'
      const author = compiledContent.userInput.author_name || 'AI Generated'
      
      // Prepare EPUB content structure
      const epubContent = {
        title: title,
        author: author,
        language: 'en',
        content: compiledContent.sections.map((section, index) => ({
          title: section.title || `Chapter ${index + 1}`,
          data: `<h1>${section.title || `Chapter ${index + 1}`}</h1>\n\n${section.content.replace(/\n/g, '<br>')}`
        })),
        output: false, // Don't write to file, return buffer
        verbose: false
      }
      
      // Generate EPUB buffer
      const buffer = await epub(epubContent)
      // Return as base64 string for download
      return `data:application/epub+zip;base64,${buffer.toString('base64')}`
      
    } catch (error) {
      console.error('Error generating EPUB:', error)
      // NO FALLBACK - THROW ERROR INSTEAD
      throw new Error(`EPUB generation failed: ${error.message}`)
    }
    */
  }

  async generateDOCXOutput(compiledContent) {
    try {
      // Import docx dynamically to avoid SSR issues
      const { Document, Packer, Paragraph, TextRun, HeadingLevel } = require('docx')
      
      const title = compiledContent.userInput.book_title || 'Generated Content'
      const author = compiledContent.userInput.author_name || 'AI Generated'
      const totalWords = compiledContent.totalWords || 0
      
      // Create document sections
      const sections = []
      
      // Add title
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: title,
              bold: true,
              size: 32
            })
          ],
          heading: HeadingLevel.TITLE,
          spacing: { after: 400 }
        })
      )
      
      // Add metadata
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: `Author: ${author}`,
              size: 24
            })
          ],
          spacing: { after: 200 }
        })
      )
      
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: `Generated: ${new Date().toLocaleDateString()}`,
              size: 24
            })
          ],
          spacing: { after: 200 }
        })
      )
      
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: `Total Words: ${totalWords.toLocaleString()}`,
              size: 24
            })
          ],
          spacing: { after: 400 }
        })
      )
      
      // Add table of contents if multiple sections
      if (compiledContent.sections.length > 1) {
        sections.push(
          new Paragraph({
            children: [
              new TextRun({
                text: 'Table of Contents',
                bold: true,
                size: 28
              })
            ],
            heading: HeadingLevel.HEADING_1,
            spacing: { after: 300 }
          })
        )
        
        compiledContent.sections.forEach((section, index) => {
          const sectionTitle = section.title || `Chapter ${index + 1}`
          sections.push(
            new Paragraph({
              children: [
                new TextRun({
                  text: `${index + 1}. ${sectionTitle}`,
                  size: 24
                })
              ],
              spacing: { after: 150 }
            })
          )
        })
      }
      
      // Add content sections
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        
        // Add chapter title
        sections.push(
          new Paragraph({
            children: [
              new TextRun({
                text: sectionTitle,
                bold: true,
                size: 28
              })
            ],
            heading: HeadingLevel.HEADING_1,
            spacing: { before: 400, after: 300 }
          })
        )
        
        // Add chapter content
        const cleanContent = section.content
          .replace(/\n{3,}/g, '\n\n')
          .replace(/^\s+|\s+$/gm, '')
          .replace(/\s{2,}/g, ' ')
        
        // Split content into paragraphs
        const paragraphs = cleanContent.split('\n\n').filter(p => p.trim())
        paragraphs.forEach(paragraph => {
          sections.push(
            new Paragraph({
              children: [
                new TextRun({
                  text: paragraph.trim(),
                  size: 24
                })
              ],
              spacing: { after: 200 }
            })
          )
        })
      })
      
      // Create document
      const doc = new Document({
        sections: [{
          properties: {},
          children: sections
        }]
      })
      
      // Generate DOCX blob (browser-compatible)
      const blob = await Packer.toBlob(doc)
      // Convert blob to base64 for download
      const arrayBuffer = await blob.arrayBuffer()
      const base64 = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)))
      return `data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,${base64}`
      
    } catch (error) {
      console.error('Error generating DOCX:', error)
      // NO FALLBACK - THROW ERROR INSTEAD
      throw new Error(`DOCX generation failed: ${error.message}`)
    }
  }

  generateXMLOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    let xml = `<?xml version="1.0" encoding="UTF-8"?>
<document>
    <title>${title}</title>
    <metadata>
        <generated>${new Date().toISOString()}</generated>
        <totalWords>${compiledContent.totalWords || 0}</totalWords>
        <type>${compiledContent.userInput.book_type || 'Content'}</type>
    </metadata>
    <content>`
    
    compiledContent.sections.forEach((section, index) => {
      xml += `
        <section id="${index + 1}">
            <heading>Section ${index + 1}</heading>
            <text><![CDATA[${section.content}]]></text>
        </section>`
    })
    
    xml += `
    </content>
</document>`
    
    return xml
  }

  generateCSVOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    let csv = 'Section,Content,WordCount\n'
    
    compiledContent.sections.forEach((section, index) => {
      const content = section.content.replace(/"/g, '""').replace(/\n/g, ' ')
      const wordCount = section.content.split(' ').length
      csv += `"Section ${index + 1}","${content}",${wordCount}\n`
    })
    
    return csv
  }

  generateYAMLOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    
    return {
      title: title,
      metadata: {
        generated: new Date().toISOString(),
        totalWords: compiledContent.totalWords || 0,
        type: compiledContent.userInput.book_type || 'Content'
      },
      sections: compiledContent.sections.map((section, index) => ({
        id: index + 1,
        title: `Section ${index + 1}`,
        content: section.content,
        wordCount: section.content.split(' ').length
      }))
    }
  }

  generateRTFOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    let rtf = `{\\rtf1\\ansi\\deff0 {\\fonttbl {\\f0 Times New Roman;}}
{\\colortbl;\\red0\\green0\\blue0;}
\\f0\\fs24
{\\b ${title}}\\par\\par`
    
    compiledContent.sections.forEach((section, index) => {
      rtf += `{\\b Section ${index + 1}}\\par\\par`
      rtf += `${section.content.replace(/\n/g, '\\par ').replace(/\r/g, '')}\\par\\par`
    })
    
    rtf += '}'
    return rtf
  }

  generateODTOutput(compiledContent) {
    // Generate OpenDocument Text format structure
    const title = compiledContent.userInput.book_title || 'Generated Content'
    
    return {
      title: title,
      content: compiledContent.sections.map((section, index) => ({
        heading: `Section ${index + 1}`,
        text: section.content
      })),
      metadata: {
        totalWords: compiledContent.totalWords || 0,
        generatedAt: new Date().toISOString(),
        type: compiledContent.userInput.book_type || 'Content'
      }
    }
  }

  generateGenericOutput(compiledContent, format) {
    // Generic fallback for any unknown format
    const title = compiledContent.userInput.book_title || 'Generated Content'
    
    return {
      format: format,
      title: title,
      content: compiledContent.sections.map((section, index) => ({
        section: index + 1,
        content: section.content
      })),
      metadata: {
        totalWords: compiledContent.totalWords || 0,
        generatedAt: new Date().toISOString(),
        type: compiledContent.userInput.book_type || 'Content',
        note: `Generated in ${format} format (generic)`
      }
    }
  }

  
  /**
   * Convert results array to node outputs format for BookCompilationService
   * FIXED: Pass structured chapters directly instead of breaking them up
   */
  convertResultsToNodeOutputs(results) {
    // Create a single structured output with all chapters
    const nodeOutputs = {
      'multi_chapter_book': {
        type: 'multi_chapter_generation',
        content: results, // Pass the structured chapters array directly
        metadata: {
          totalChapters: results.length,
          chapters: results.map(r => ({
            number: r.chapter,
            title: this.extractChapterTitle(r.content),
            content: r.content,
            metadata: r.aiMetadata || {}
          }))
        }
      }
    }
    
    return nodeOutputs
  }

  /**
   * Extract chapter title from chapter content
   */
  extractChapterTitle(content) {
    const titleMatch = content.match(/Chapter \d+: (.+?)(?:\n|$)/)
    return titleMatch ? titleMatch[1].trim() : null
  }

  generateDeliverables(formattedOutput, nodeData) {
    const deliverables = []
    
    // Handle new format structure with multiple formats
    if (formattedOutput.allFormats) {
      // New structure with multiple formats
      Object.entries(formattedOutput.allFormats).forEach(([format, content]) => {
        // Check if content is base64 data URI (for binary formats)
        if (typeof content === 'string' && content.startsWith('data:')) {
          // SURGICAL FIX: Extract clean extension
          const cleanFormat = format.match(/\(\.(\w+)\)/) 
            ? format.match(/\(\.(\w+)\)/)[1]
            : format.replace(/[^a-z0-9]/gi, '').toLowerCase()
          
          const base64Data = content.split(',')[1]
          deliverables.push({
            format: cleanFormat,
            content: content, // Keep full data URI for download
            filename: `lekhika_generated_content.${cleanFormat}`,
            size: base64Data.length,
            isPrimary: format === formattedOutput.primaryFormat,
            mimeType: content.split(':')[1].split(';')[0],
            isBinary: true
          })
        } else {
          // CLEAN CONTENT - NO JSON GARBAGE
          let contentString = ''
          if (typeof content === 'string') {
            contentString = content
          } else if (content && content.content) {
            // Extract clean content from nested structure
            contentString = content.content
          } else if (content && typeof content === 'object') {
            // Handle object content properly - extract meaningful content
            if (content.chapters && Array.isArray(content.chapters)) {
              // Multi-chapter content - compile chapters
              contentString = content.chapters.map(chapter => 
                typeof chapter === 'string' ? chapter : 
                (chapter.content || chapter.text || JSON.stringify(chapter))
              ).join('\n\n')
            } else if (content.text || content.body) {
              contentString = content.text || content.body
            } else if (content.content) {
              // Direct content property
              contentString = content.content
            } else {
              // Last resort - try to extract any string content
              console.warn(`‚ö†Ô∏è Complex content structure for ${format}, attempting extraction`)
              contentString = JSON.stringify(content, null, 2)
            }
          } else {
            // Last resort - convert to string but this should not happen with professional formatter
            console.warn(`‚ö†Ô∏è Unexpected content structure for ${format}:`, typeof content)
            contentString = String(content)
          }
          
          // SURGICAL FIX: Extract clean extension from format (remove descriptions like "plain text (.txt)")
          const cleanFormat = format.match(/\(\.(\w+)\)/) 
            ? format.match(/\(\.(\w+)\)/)[1]  // Extract from "plain text (.txt)" -> "txt"
            : format.replace(/[^a-z0-9]/gi, '').toLowerCase() // Clean format name
          
          deliverables.push({
            format: cleanFormat,
            content: contentString,
            filename: `lekhika_generated_content.${cleanFormat}`,
            size: contentString.length,
            isPrimary: format === formattedOutput.primaryFormat,
            mimeType: this.getMimeType(cleanFormat),
            isBinary: false
          })
        }
      })
    } else {
      // Legacy structure with single format
      const exportFormats = nodeData.exportFormats || ['markdown']
      
      // CLEAN CONTENT - NO JSON GARBAGE FOR LEGACY STRUCTURE TOO
      let contentString = ''
      if (typeof formattedOutput === 'string') {
        contentString = formattedOutput
      } else if (formattedOutput && formattedOutput.content) {
        contentString = formattedOutput.content
      } else {
        console.warn(`‚ö†Ô∏è Legacy structure: Unexpected format for content:`, typeof formattedOutput)
        contentString = String(formattedOutput)
      }
      
      exportFormats.forEach(format => {
        deliverables.push({
          format,
          content: contentString,
          filename: `lekhika_generated_content.${format}`,
          size: contentString.length,
          isPrimary: true,
          mimeType: this.getMimeType(format)
        })
      })
    }

    console.log('üì¶ Generated deliverables:', deliverables.map(d => `${d.format} (${d.size} chars)`))
    return deliverables
  }

  getMimeType(format) {
    const mimeTypes = {
      'pdf': 'application/pdf',
      'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
      'html': 'text/html',
      'markdown': 'text/markdown',
      'md': 'text/markdown',
      'txt': 'text/plain',
      'text': 'text/plain',
      'json': 'application/json',
      'epub': 'application/epub+zip',
      'xml': 'application/xml'
    }
    return mimeTypes[format.toLowerCase()] || 'text/plain'
  }

  // REMOVED: generateTextFallback() - NO FALLBACK TEMPLATES


  updateExecutionState(workflowId, updates) {
    const currentState = this.executionState.get(workflowId) || {}
    this.executionState.set(workflowId, { ...currentState, ...updates })
  }

  getExecutionState(workflowId) {
    return this.executionState.get(workflowId)
  }

  clearExecutionState(workflowId) {
    this.executionState.delete(workflowId)
  }

  // Force stop workflow execution
  stopWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    if (currentState) {
      // IMMEDIATELY terminate the workflow
      this.updateExecutionState(workflowId, {
        status: 'stopped',
        currentNode: null,
        stopped: true,
        stoppedAt: new Date(),
        forceStopped: true
      })
      
      // Force clear from execution state
      this.executionState.delete(workflowId)
      
      console.log(`üõë Workflow ${workflowId} FORCE STOPPED and cleared from memory`)
      
      // Throw error to immediately terminate any running execution
      throw new Error(`Workflow ${workflowId} stopped by user`)
    }
  }

  // Check if workflow is stopped
  isWorkflowStopped(workflowId) {
    const state = this.executionState.get(workflowId)
    return state?.status === 'stopped'
  }

  // Check database for stop signal
  async checkDatabaseStopSignal(workflowId) {
    try {
      const { getSupabase } = require('./supabase')
      const supabase = getSupabase()
      
      const { data, error } = await supabase
        .from('engine_executions')
        .select('status')
        .eq('id', workflowId)
        .single()
      
      if (error) {
        console.warn(`‚ö†Ô∏è Could not check database stop signal for ${workflowId}:`, error.message)
        return false
      }
      
      const isStopped = data?.status === 'cancelled' || data?.status === 'stopped'
      if (isStopped) {
        console.log(`üõë Database stop signal detected for ${workflowId}`)
        // Also update local state
        this.updateExecutionState(workflowId, {
          status: 'stopped',
          stopped: true,
          stoppedAt: new Date()
        })
      }
      
      return isStopped
    } catch (error) {
      console.warn(`‚ö†Ô∏è Error checking database stop signal for ${workflowId}:`, error.message)
      return false
    }
  }

  // Check if workflow is paused
  isWorkflowPaused(workflowId) {
    const state = this.executionState.get(workflowId)
    return state?.status === 'paused'
  }

  // Pause workflow execution
  pauseWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    if (currentState && currentState.status === 'executing') {
      this.updateExecutionState(workflowId, {
        status: 'paused',
        pausedAt: new Date()
      })
      console.log(`‚è∏Ô∏è Workflow ${workflowId} paused by user`)
      return true
    }
    return false
  }

  // Resume workflow execution
  resumeWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    console.log(`üîÑ Attempting to resume workflow ${workflowId}:`, currentState)
    
    if (currentState && currentState.status === 'paused') {
      // Store the pauseResolver before updating state
      const pauseResolver = currentState.pauseResolver
      console.log(`üîÑ Found pauseResolver:`, !!pauseResolver)
      
      // Update state to executing
      this.updateExecutionState(workflowId, {
        ...currentState,
        status: 'executing',
        resumedAt: new Date()
      })
      
      // Resolve the pause promise to continue execution
      if (pauseResolver) {
        console.log(`‚ñ∂Ô∏è Resolving pause promise for workflow ${workflowId}`)
        pauseResolver()
        console.log(`‚ñ∂Ô∏è Pause promise resolved for workflow ${workflowId}`)
      } else {
        console.log(`‚ö†Ô∏è No pauseResolver found for workflow ${workflowId}`)
      }
      
      console.log(`‚ñ∂Ô∏è Workflow ${workflowId} resumed by user`)
      return true
    }
    
    console.log(`‚ùå Cannot resume workflow ${workflowId}: status is ${currentState?.status || 'undefined'}`)
    return false
  }

  // Resume from a specific node (milestone checkpoint)
  resumeFromNode(workflowId, nodeId) {
    const currentState = this.executionState.get(workflowId)
    console.log(`üîÑ Attempting to resume workflow ${workflowId} from node ${nodeId}:`, currentState)
    
    if (currentState) {
      // Find the checkpoint for this node
      const checkpoint = this.checkpointStates.get(`${workflowId}_${nodeId}`)
      if (checkpoint) {
        console.log(`‚úÖ Found checkpoint for node ${nodeId}, restoring state`)
        
        // Restore the execution state from checkpoint
        this.updateExecutionState(workflowId, {
          ...checkpoint.state,
          status: 'executing',
          resumedFromNode: nodeId,
          resumedAt: new Date()
        })
        
        console.log(`‚ñ∂Ô∏è Workflow ${workflowId} resumed from node ${nodeId}`)
        console.log(`‚ñ∂Ô∏è Restored state:`, this.executionState.get(workflowId))
        return true
      } else {
        console.log(`‚ùå No checkpoint found for node ${nodeId}`)
        console.log(`‚ùå Available checkpoints:`, Array.from(this.checkpointStates.keys()))
        return false
      }
    }
    
    console.log(`‚ùå Cannot resume workflow ${workflowId}: no state found`)
    return false
  }

  // Restart workflow execution from a checkpoint (for UI resume buttons)
  async restartFromCheckpoint(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser) {
    console.log(`üîÑ Restarting workflow ${workflowId} from checkpoint ${nodeId}`)
    
    try {
      // Check if we have a checkpoint for this node
      const checkpoint = this.checkpointStates.get(`${workflowId}_${nodeId}`)
      
      if (!checkpoint) {
        // If no checkpoint, try to resume from the failed node by restarting it
        console.log(`‚ö†Ô∏è No checkpoint found for node ${nodeId}, attempting to restart failed node`)
        return await this.restartFailedNode(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser)
      }
      
      // Restore the execution state from checkpoint
      this.updateExecutionState(workflowId, {
        ...checkpoint.state,
        status: 'executing',
        resumedFromNode: nodeId,
        resumedAt: new Date()
      })
      
      // Continue execution from the checkpoint
      console.log(`‚ñ∂Ô∏è Continuing workflow execution from checkpoint ${nodeId}`)
      
      // Find the node index to resume from
      const executionOrder = this.getExecutionOrder(nodes, edges)
      const nodeIndex = executionOrder.findIndex(node => node.id === nodeId)
      
      if (nodeIndex === -1) {
        throw new Error(`Node ${nodeId} not found in execution order`)
      }
      
      // Continue execution from the next node
      return await this.continueExecutionFromNode(
        workflowId, 
        nodes, 
        edges, 
        initialInput, 
        progressCallback, 
        superAdminUser,
        nodeIndex + 1, // Start from next node
        checkpoint.state.nodeOutputs || {}
      )
      
    } catch (error) {
      console.error(`‚ùå Error restarting from checkpoint:`, error)
      return { success: false, error: error.message }
    }
  }

  // Restart a failed node specifically
  async restartFailedNode(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser) {
    console.log(`üîÑ Restarting failed node ${nodeId} in workflow ${workflowId}`)
    
    try {
      // Find the node to restart
      const nodeToRestart = nodes.find(node => node.id === nodeId)
      if (!nodeToRestart) {
        throw new Error(`Node ${nodeId} not found`)
      }
      
      // Get the current execution state
      const currentState = this.executionState.get(workflowId)
      if (!currentState) {
        throw new Error(`No execution state found for workflow ${workflowId}`)
      }
      
      // Get outputs from previous nodes
      const previousOutputs = currentState.nodeOutputs || {}
      
      // Create a pipeline data object with previous outputs
      const pipelineData = {
        ...previousOutputs,
        workflowId,
        currentNodeId: nodeId,
        executionStartTime: currentState.startTime || new Date()
      }
      
      // Clear any error state for this node
      this.updateExecutionState(workflowId, {
        ...currentState,
        status: 'executing',
        currentNodeId: nodeId,
        currentNodeStatus: 'executing',
        errors: currentState.errors ? currentState.errors.filter(error => error.nodeId !== nodeId) : []
      })
      
      // Execute the specific node
      const result = await this.executeNode(nodeToRestart, pipelineData, progressCallback, workflowId, superAdminUser)
      
      if (result.success) {
        // Update the execution state with the successful result
        this.updateExecutionState(workflowId, {
          ...currentState,
          nodeOutputs: {
            ...currentState.nodeOutputs,
            [nodeId]: result.output
          },
          currentNodeStatus: 'completed'
        })
        
        console.log(`‚úÖ Successfully restarted node ${nodeId}`)
        
        // Try to continue with the next nodes
        return await this.continueWorkflowFromNode(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser)
      } else {
        throw new Error(result.error || 'Node execution failed')
      }
      
    } catch (error) {
      console.error(`‚ùå Error restarting failed node:`, error)
      return { success: false, error: error.message }
    }
  }

  // Continue workflow execution from a specific node
  async continueWorkflowFromNode(workflowId, fromNodeId, nodes, edges, initialInput, progressCallback, superAdminUser) {
    console.log(`‚ñ∂Ô∏è Continuing workflow ${workflowId} from node ${fromNodeId}`)
    
    try {
      // Get execution order
      const executionOrder = this.getExecutionOrder(nodes, edges)
      const fromNodeIndex = executionOrder.findIndex(node => node.id === fromNodeId)
      
      if (fromNodeIndex === -1) {
        throw new Error(`Node ${fromNodeId} not found in execution order`)
      }
      
      // Continue execution from the next node
      return await this.continueExecutionFromNode(
        workflowId,
        nodes,
        edges,
        initialInput,
        progressCallback,
        superAdminUser,
        fromNodeIndex + 1,
        this.executionState.get(workflowId)?.nodeOutputs || {}
      )
      
    } catch (error) {
      console.error(`‚ùå Error continuing workflow from node:`, error)
      return { success: false, error: error.message }
    }
  }

  // Continue execution from a specific node index
  async continueExecutionFromNode(workflowId, nodes, edges, initialInput, progressCallback, superAdminUser, startIndex, existingOutputs) {
    console.log(`üîÑ Continuing execution from node index ${startIndex}`)
    
    const executionOrder = this.getExecutionOrder(nodes, edges)
    
    // Initialize pipeline data with existing outputs
    const pipelineData = {
      userInput: initialInput,
      nodeOutputs: existingOutputs,
      lastNodeOutput: null,
      previousNodePassover: null
    }
    
    // Execute remaining nodes
    for (let i = startIndex; i < executionOrder.length; i++) {
      const node = executionOrder[i]
      
      // Check if workflow was paused or stopped
      if (this.isWorkflowPaused(workflowId)) {
        await this.waitForResume(workflowId)
      }
      
        if (this.isWorkflowStopped(workflowId) || await this.checkDatabaseStopSignal(workflowId)) {
        console.log(`üõë Workflow ${workflowId} stopped during continuation`)
        return { success: false, status: 'stopped' }
      }
      
      try {
        console.log(`üîç EXECUTING NODE ${i + 1}/${executionOrder.length}: ${node.id} (${node.data.label})`)
        
        // Execute individual node
        const nodeOutput = await this.executeNode(node, pipelineData, workflowId, progressCallback)
        
        // Update pipeline
        pipelineData.nodeOutputs[node.id] = nodeOutput
        pipelineData.lastNodeOutput = nodeOutput
        
        // Update execution state
        this.updateExecutionState(workflowId, {
          [`results.${node.id}`]: nodeOutput,
          nodeOutputs: pipelineData.nodeOutputs,
          currentNodeIndex: i,
          completedNodes: executionOrder.slice(0, i + 1).map(n => n.id)
        })

        // CREATE CHECKPOINT AFTER EACH NODE COMPLETION
        this.createCheckpoint(workflowId, node.id, {
          ...this.executionState.get(workflowId),
          nodeOutputs: pipelineData.nodeOutputs,
          currentNodeIndex: i,
          completedNodes: executionOrder.slice(0, i + 1).map(n => n.id)
        })
        
        // Progress callback
        const completionProgress = ((i + 1) / executionOrder.length) * 100
        if (progressCallback) {
          progressCallback({
            nodeId: node.id,
            nodeName: node.data.label,
            progress: completionProgress,
            status: 'completed',
            output: nodeOutput,
            nodeIndex: i + 1,
            totalNodes: executionOrder.length,
            isNodeComplete: true,
            checkpointCreated: true
          })
        }
        
      } catch (error) {
        console.error(`‚ùå Error executing node ${node.id}:`, error)
        throw error
      }
    }
    
    // Workflow completed
    console.log(`‚úÖ Workflow ${workflowId} completed successfully`)
    return {
      success: true,
      status: 'completed',
      results: pipelineData.nodeOutputs
    }
  }

  // Create checkpoint after node completion
  createCheckpoint(workflowId, nodeId, state, pauseResolver) {
    const checkpointKey = `${workflowId}_${nodeId}`
    this.checkpointStates.set(checkpointKey, {
      nodeId,
      state: { ...state },
      pauseResolver,
      timestamp: new Date(),
      nodeOutput: state.nodeOutputs?.[nodeId]
    })
    console.log(`üíæ Checkpoint created for node ${nodeId} in workflow ${workflowId}`)
  }

  // Wait for workflow to be resumed
  async waitForResume(workflowId) {
    console.log(`‚è≥ Creating pause promise for workflow ${workflowId}`)
    return new Promise((resolve) => {
      const currentState = this.executionState.get(workflowId)
      console.log(`‚è≥ Current state for workflow ${workflowId}:`, currentState)
      
      if (currentState) {
        // Store the resolver to be called when resumed
        currentState.pauseResolver = resolve
        this.updateExecutionState(workflowId, currentState)
        console.log(`‚è≥ Pause resolver stored for workflow ${workflowId}`)
        console.log(`‚è≥ Updated state:`, this.executionState.get(workflowId))
      } else {
        // If no state, just resolve immediately
        console.log(`‚è≥ No state found for workflow ${workflowId}, resolving immediately`)
        resolve()
      }
    })
  }

  // Stop workflow execution immediately
  stopWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    if (currentState) {
      this.updateExecutionState(workflowId, {
        status: 'stopped',
        stoppedAt: new Date(),
        forceStopped: true
      })
      
      // Resolve any pending pause to allow cleanup
      if (currentState.pauseResolver) {
        currentState.pauseResolver()
        delete currentState.pauseResolver
      }
      
      console.log(`üõë Workflow ${workflowId} force stopped by user`)
      return true
    }
    return false
  }

  // Retry a specific failed node
  retryNode(nodeId) {
    const currentState = this.executionState.get(nodeId)
    if (currentState && currentState.status === 'paused') {
      // Clear the error and mark for retry
      this.updateExecutionState(nodeId, {
        status: 'ready_for_retry',
        retryRequested: true,
        retryAt: new Date()
      })
      
      console.log(`üîÑ Node ${nodeId} marked for retry`)
      return true
    }
    return false
  }

  // Resume from a specific node
  resumeFromNode(nodeId) {
    const currentState = this.executionState.get(nodeId)
    if (currentState && currentState.status === 'paused') {
      // Resume from this specific node
      this.updateExecutionState(nodeId, {
        status: 'executing',
        resumedAt: new Date(),
        resumeFromNode: nodeId
      })
      
      // Resolve the pause promise to continue execution
      if (currentState.pauseResolver) {
        currentState.pauseResolver()
        delete currentState.pauseResolver
      }
      
      console.log(`‚ñ∂Ô∏è Workflow resumed from node ${nodeId}`)
      return true
    }
    return false
  }

  // Get current paused workflow
  getCurrentPausedWorkflow() {
    const pausedWorkflows = Array.from(this.executionState.entries())
      .filter(([id, state]) => state.status === 'paused')
    
    return pausedWorkflows.length > 0 ? pausedWorkflows[0] : null
  }

  // Check if any workflow is paused
  hasPausedWorkflow() {
    return Array.from(this.executionState.values())
      .some(state => state.status === 'paused')
  }

  // Get all execution states for debugging
  getAllExecutionStates() {
    return Array.from(this.executionState.entries())
  }

  // Get execution state for a specific workflow
  getExecutionState(workflowId) {
    return this.executionState.get(workflowId)
  }

  /**
   * Clear all executions (zombie killer)
   */
  clearAllExecutions() {
    console.log('üßπ Clearing all zombie executions...')
    this.executionState.clear()
    this.checkpointStates.clear()
    console.log('‚úÖ All executions cleared')
  }

  /**
   * Kill stuck executions older than 5 minutes
   */
  killStuckExecutions() {
    const now = Date.now()
    const stuckThreshold = 5 * 60 * 1000 // 5 minutes
    
    for (const [workflowId, state] of this.executionState) {
      if (state.startedAt && (now - state.startedAt) > stuckThreshold) {
        console.log(`üíÄ Killing stuck execution: ${workflowId}`)
        this.executionState.delete(workflowId)
      }
    }
  }
}

const workflowExecutionService = new WorkflowExecutionService()

module.exports = workflowExecutionService