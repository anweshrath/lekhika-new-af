/**
 * REAL Workflow Execution Service
 * Executes workflows with actual AI calls and data flow between nodes
 * NO MOCKUPS - REAL FUNCTIONALITY ONLY
 */

const { getSupabase } = require('./supabase')
const aiService = require('./aiService')
const { narrativeStructureService } = require('./narrativeStructureService')
const { professionalBookFormatter } = require('./professionalBookFormatter')
const exportService = require('./exportService')
const { accentInstructionService } = require('./accentInstructionService')
const { sampleAnalysisService } = require('./sampleAnalysisService')
const { typographyService } = require('./typographyService')
const sessionManager = require('./sessionManager')
const bookCompilationService = require('./BookCompilationService.js')
const { NODE_ROLE_CONFIG } = require('../data/nodePalettes')
const aiResponseValidator = require('./aiResponseValidator.js')

class WorkflowExecutionService {
  constructor() {
    this.executionState = new Map()
    this.checkpointStates = new Map() // Store checkpoint states for resume functionality
    this.currentSession = null // Current execution session
    // Use aiService for all providers since it fetches API keys from ai_providers table
    this.aiServices = {
      'openai': aiService,
      'anthropic': aiService,
      'google': aiService,
      'ai': aiService
    }
  }

  /**
   * Check for existing session and offer resume option
   * @param {string} flowId - Flow identifier
   * @param {string} userId - User identifier
   * @returns {Object} Session status and resume options
   */
  checkForExistingSession(flowId, userId) {
    return sessionManager.checkSessionStatus(flowId, userId)
  }

  /**
   * Start new execution session
   * @param {Object} params - Session parameters
   * @returns {Object} Session data
   */
  startNewSession({ flowId, userId, nodes, edges, initialInput }) {
    const sessionData = sessionManager.createSessionData({
      flowId,
      userId,
      nodes,
      edges,
      initialInput,
      currentPhase: 'initialization',
      completedNodes: [],
      currentNode: null,
      executionData: {},
      errors: [],
      warnings: []
    })

    this.currentSession = sessionData
    sessionManager.saveSession(flowId, userId, sessionData)
    
    console.log('üöÄ New execution session started:', flowId)
    return sessionData
  }

  /**
   * Resume existing session
   * @param {string} flowId - Flow identifier
   * @param {string} userId - User identifier
   * @returns {Object} Resumed session data
   */
  resumeSession(flowId, userId) {
    const session = sessionManager.loadSession(flowId, userId)
    if (!session) {
      throw new Error('No session found to resume')
    }

    this.currentSession = session
    console.log('üîÑ Resuming execution session:', flowId)
    return session
  }

  /**
   * Update session with current execution state
   * @param {Object} updates - Updates to apply
   */
  updateSession(updates) {
    if (!this.currentSession) {
      console.warn('‚ö†Ô∏è No active session to update')
      return
    }

    const updatedSession = {
      ...this.currentSession,
      ...updates,
      timestamp: Date.now()
    }

    this.currentSession = updatedSession
    sessionManager.saveSession(
      this.currentSession.flowId,
      this.currentSession.userId,
      updatedSession
    )
  }

  /**
   * Complete session and clear from storage
   */
  completeSession() {
    if (!this.currentSession) {
      console.warn('‚ö†Ô∏è No active session to complete')
      return
    }

    console.log('‚úÖ Execution session completed:', this.currentSession.flowId)
    sessionManager.clearSession(
      this.currentSession.flowId,
      this.currentSession.userId
    )
    this.currentSession = null
  }

  /**
   * Handle execution error and save session for recovery
   * @param {Error} error - Execution error
   * @param {Object} context - Error context
   */
  handleExecutionError(error, context = {}) {
    if (!this.currentSession) {
      console.error('‚ùå Execution error with no active session:', error)
      return
    }

    const errorData = {
      error: error.message,
      stack: error.stack,
      context,
      timestamp: new Date().toISOString()
    }

    this.updateSession({
      errors: [...(this.currentSession.errors || []), errorData],
      currentPhase: 'error_recovery'
    })

    console.error('‚ùå Execution error saved to session:', errorData)
  }

  /**
   * PRE-RUN TEST SYSTEM - Validate flow before execution
   * @param {Array} nodes - Workflow nodes
   * @param {Array} edges - Workflow edges
   * @param {Object} initialInput - User input
   * @param {Function} progressCallback - Progress update callback
   * @returns {Object} Test results and validation status
   */
  async preRunTest(nodes, edges, initialInput, progressCallback = null) {
    console.log('üß™ Starting Pre-Run Test for workflow validation...')
    console.log('üß™ Test Parameters:', { 
      nodesCount: nodes?.length || 0, 
      edgesCount: edges?.length || 0, 
      hasInitialInput: !!initialInput 
    })
    
    const testResults = {
      overallStatus: 'passing',
      nodeTests: {},
      connectivityTests: {},
      exportTests: {},
      warnings: [],
      errors: []
    }

    try {
      // 1. VALIDATE NODE CONFIGURATION
      console.log('üîç Testing node configurations...')
      for (const node of nodes) {
        const nodeTest = await this.testNodeConfiguration(node, initialInput)
        testResults.nodeTests[node.id] = nodeTest
        
        if (nodeTest.status === 'error') {
          testResults.overallStatus = 'failing'
          testResults.errors.push(`Node ${node.id}: ${nodeTest.error}`)
        } else if (nodeTest.status === 'warning') {
          testResults.warnings.push(`Node ${node.id}: ${nodeTest.warning}`)
        }
        
        if (progressCallback) {
          progressCallback({
            status: 'testing',
            message: `Testing node: ${node.data?.label || node.id}`,
            progress: (Object.keys(testResults.nodeTests).length / nodes.length) * 25
          })
        }
      }

      // 2. TEST AI CONNECTIVITY
      console.log('ü§ñ Testing AI connectivity...')
      const connectivityTest = await this.testAIConnectivity(nodes)
      testResults.connectivityTests = connectivityTest
      
      if (connectivityTest.status === 'error') {
        testResults.overallStatus = 'failing'
        testResults.errors.push(`AI Connectivity: ${connectivityTest.error}`)
      }

      if (progressCallback) {
        progressCallback({
          status: 'testing',
          message: 'Testing AI connectivity...',
          progress: 50
        })
      }

      // 3. TEST EXPORT SERVICES
      console.log('üìÑ Testing export services...')
      const exportTest = await this.testExportServices()
      testResults.exportTests = exportTest
      
      if (exportTest.status === 'error') {
        testResults.overallStatus = 'failing'
        testResults.errors.push(`Export Services: ${exportTest.error}`)
      }

      if (progressCallback) {
        progressCallback({
          status: 'testing',
          message: 'Testing export services...',
          progress: 75
        })
      }

      // 4. VALIDATE WORKFLOW STRUCTURE
      console.log('üîó Validating workflow structure...')
      const structureTest = this.validateWorkflowStructure(nodes, edges)
      if (!structureTest.valid) {
        testResults.overallStatus = 'failing'
        testResults.errors.push(`Workflow Structure: ${structureTest.error}`)
      }

      if (progressCallback) {
        progressCallback({
          status: testResults.overallStatus === 'passing' ? 'success' : 'error',
          message: testResults.overallStatus === 'passing' 
            ? '‚úÖ Pre-run test completed successfully!' 
            : '‚ùå Pre-run test failed - check errors',
          progress: 100,
          testResults
        })
      }

      console.log('üß™ Pre-Run Test Results:', testResults)
      return testResults

    } catch (error) {
      console.error('‚ùå Pre-Run Test Error:', error)
      console.error('‚ùå Error Stack:', error.stack)
      console.error('‚ùå Error Details:', {
        name: error.name,
        message: error.message,
        cause: error.cause,
        nodes: nodes?.length || 0,
        edges: edges?.length || 0,
        hasInput: !!initialInput
      })
      
      testResults.overallStatus = 'failing'
      testResults.errors.push(`CRITICAL SYSTEM ERROR: ${error.message}`)
      testResults.errors.push(`Error Type: ${error.name}`)
      testResults.errors.push(`Stack Trace: ${error.stack}`)
      
      // Add diagnostic information
      testResults.diagnostics = {
        nodesCount: nodes?.length || 0,
        edgesCount: edges?.length || 0,
        hasInitialInput: !!initialInput,
        inputType: typeof initialInput,
        errorTimestamp: new Date().toISOString(),
        userAgent: typeof navigator !== 'undefined' ? navigator.userAgent : 'Server-side',
        memoryUsage: typeof process !== 'undefined' ? process.memoryUsage() : 'N/A'
      }
      
      if (progressCallback) {
        progressCallback({
          status: 'error',
          message: `CRITICAL ERROR: ${error.message}`,
          progress: 100,
          testResults,
          errorDetails: {
            name: error.name,
            message: error.message,
            stack: error.stack,
            diagnostics: testResults.diagnostics
          }
        })
      }
      
      return testResults
    }
  }

  /**
   * Test individual node configuration
   */
  async testNodeConfiguration(node, initialInput) {
    try {
      // Check if node has required data
      if (!node.data) {
        return { status: 'error', error: 'Missing node data' }
      }

      // Test input nodes
      if (node.type === 'input') {
        if (!node.data.inputFields || !Array.isArray(node.data.inputFields)) {
          return { status: 'warning', warning: 'Input node missing inputFields' }
        }
        
        // Test if required fields are present
        const requiredFields = node.data.inputFields.filter(field => field.required)
        if (requiredFields.length === 0) {
          return { status: 'warning', warning: 'No required fields defined' }
        }
      }

      // Test process nodes
      if (node.type === 'process') {
        if (!node.data.role) {
          return { status: 'error', error: 'Process node missing role' }
        }

        if (!node.data.configuration) {
          return { status: 'warning', warning: 'Process node missing configuration' }
        }

        if (!node.data.configuration.systemPrompt || !node.data.configuration.userPrompt) {
          return { status: 'warning', warning: 'Process node missing prompts' }
        }
      }

      // Test output nodes
      if (node.type === 'output') {
        if (!node.data.role) {
          return { status: 'error', error: 'Output node missing role' }
        }
      }

      // Test preview nodes
      if (node.type === 'preview') {
        if (!node.data.approvalRequired) {
          return { status: 'warning', warning: 'Preview node should have approvalRequired' }
        }
      }

      return { status: 'passing', message: 'Node configuration valid' }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Test AI connectivity for all nodes
   */
  async testAIConnectivity(nodes) {
    try {
      const aiNodes = nodes.filter(node => 
        node.type === 'process' && 
        node.data?.aiEnabled && 
        node.data?.selectedModels?.length > 0
      )

      if (aiNodes.length === 0) {
        return { status: 'warning', warning: 'No AI-enabled nodes found' }
      }

      // Test each AI provider
      const providers = new Set()
      aiNodes.forEach(node => {
        node.data.selectedModels.forEach(model => {
          const provider = model.split('-')[0]?.toLowerCase()
          if (provider) providers.add(provider)
        })
      })

      const connectivityResults = {}
      for (const provider of providers) {
        try {
          // Test API key availability
          const testResult = await this.testAIProvider(provider)
          connectivityResults[provider] = testResult
        } catch (error) {
          connectivityResults[provider] = { status: 'error', error: error.message }
        }
      }

      const hasErrors = Object.values(connectivityResults).some(result => result.status === 'error')
      return {
        status: hasErrors ? 'error' : 'passing',
        providers: connectivityResults,
        error: hasErrors ? 'Some AI providers failed connectivity test' : null
      }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Test individual AI provider
   */
  async testAIProvider(provider) {
    try {
      // Test API key availability
      const { data: apiKeys, error } = await supabase
        .from('ai_providers')
        .select('*')
        .eq('provider', provider)
        .eq('is_active', true)

      if (error) throw error

      if (!apiKeys || apiKeys.length === 0) {
        return { status: 'error', error: `No active API keys found for ${provider}` }
      }

      // Test basic connectivity (optional - can be expensive)
      // For now, just check API key availability
      return { 
        status: 'passing', 
        message: `${provider} API keys available`,
        keyCount: apiKeys.length
      }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Test export services
   */
  async testExportServices() {
    try {
      const exportTests = {}

      // Test PDF export capability
      try {
        // Simple test - check if exportService has PDF method
        if (typeof exportService.generatePDF === 'function') {
          exportTests.pdf = { status: 'passing', message: 'PDF export available' }
        } else {
          exportTests.pdf = { status: 'error', error: 'PDF export method not found' }
        }
      } catch (error) {
        exportTests.pdf = { status: 'error', error: error.message }
      }

      // Test DOCX export capability
      try {
        if (typeof exportService.generateDOCX === 'function') {
          exportTests.docx = { status: 'passing', message: 'DOCX export available' }
        } else {
          exportTests.docx = { status: 'error', error: 'DOCX export method not found' }
        }
      } catch (error) {
        exportTests.docx = { status: 'error', error: error.message }
      }

      // Test EPUB export capability
      try {
        if (typeof exportService.generateEPUB === 'function') {
          exportTests.epub = { status: 'passing', message: 'EPUB export available' }
        } else {
          exportTests.epub = { status: 'error', error: 'EPUB export method not found' }
        }
      } catch (error) {
        exportTests.epub = { status: 'error', error: error.message }
      }

      const hasErrors = Object.values(exportTests).some(test => test.status === 'error')
      return {
        status: hasErrors ? 'error' : 'passing',
        services: exportTests,
        error: hasErrors ? 'Some export services failed test' : null
      }

    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }

  /**
   * Validate workflow structure
   */
  validateWorkflowStructure(nodes, edges) {
    try {
      // Check if we have nodes
      if (!nodes || nodes.length === 0) {
        return { valid: false, error: 'No nodes found' }
      }

      // Check if we have edges
      if (!edges || edges.length === 0) {
        return { valid: false, error: 'No edges found' }
      }

      // Find input nodes
      const inputNodes = nodes.filter(node => node.type === 'input')
      if (inputNodes.length === 0) {
        return { valid: false, error: 'No input nodes found' }
      }

      // Find output nodes
      const outputNodes = nodes.filter(node => node.type === 'output')
      if (outputNodes.length === 0) {
        return { valid: false, error: 'No output nodes found' }
      }

      // Check if all edges reference valid nodes
      const nodeIds = new Set(nodes.map(node => node.id))
      for (const edge of edges) {
        if (!nodeIds.has(edge.source) || !nodeIds.has(edge.target)) {
          return { valid: false, error: `Edge references non-existent node: ${edge.source} -> ${edge.target}` }
        }
      }

      return { valid: true, message: 'Workflow structure is valid' }

    } catch (error) {
      return { valid: false, error: error.message }
    }
  }

  /**
   * Execute a complete workflow with real AI calls and data flow
   * @param {Array} nodes - Workflow nodes
   * @param {Array} edges - Workflow edges  
   * @param {Object} initialInput - User input from Lekhika root app
   * @param {string} workflowId - Unique workflow execution ID
   * @param {Function} progressCallback - Progress update callback
   * @returns {Object} Final workflow output
   */
  async executeWorkflow(nodes, edges, initialInput, workflowId, progressCallback = null, superAdminUser = null) {
    const startTime = Date.now() // DEFINE START TIME FOR EXECUTION TRACKING
    
    try {
      // Validate SuperAdmin authentication
      if (!superAdminUser || !superAdminUser.id) {
        throw new Error('SuperAdmin authentication required for workflow execution')
      }

      // Initialize execution state
      this.executionState.set(workflowId, {
        status: 'running',
        currentNode: null,
        results: {},
        errors: [],
        startTime: new Date(),
        progress: 0
      })

      // Build execution order based on edges
      const executionOrder = this.buildExecutionOrder(nodes, edges)
      
      // Initialize data pipeline with user input
      let pipelineData = {
        userInput: initialInput,
        nodeOutputs: {},
        superAdminUser: superAdminUser,
        metadata: {
          workflowId,
          executionTime: new Date(),
          totalNodes: executionOrder.length
        }
      }

      console.log('üîç WORKFLOW EXECUTION DEBUG:')
      console.log('  - Initial input:', initialInput)
      console.log('  - Execution order:', executionOrder.map(n => n.id))
      console.log('  - Initial pipeline data:', pipelineData)

      // Execute nodes in sequence
      for (let i = 0; i < executionOrder.length; i++) {
        // Check if workflow was paused - wait until resumed
        if (this.isWorkflowPaused(workflowId)) {
          console.log(`‚è∏Ô∏è Workflow ${workflowId} is paused, waiting for resume...`)
          // Wait indefinitely until resumed - use Promise that resolves only on resume
          await this.waitForResume(workflowId)
        }

        // Check if workflow was resumed from a specific checkpoint
        const currentState = this.executionState.get(workflowId)
        if (currentState?.resumedFromNode) {
          console.log(`üîÑ Workflow resumed from node: ${currentState.resumedFromNode}`)
          // Skip to the node after the resumed checkpoint
          const resumedNodeIndex = executionOrder.findIndex(node => node.id === currentState.resumedFromNode)
          if (resumedNodeIndex !== -1 && resumedNodeIndex > i) {
            console.log(`‚è≠Ô∏è Skipping to node ${resumedNodeIndex + 1} (resumed from ${currentState.resumedFromNode})`)
            i = resumedNodeIndex // Skip to the resumed node
            // Clear the resumedFromNode flag
            this.updateExecutionState(workflowId, { resumedFromNode: null })
          }
        }

        // Check if workflow was stopped
        if (this.isWorkflowStopped(workflowId)) {
          console.log(`üõë Workflow ${workflowId} stopped during execution`)
          
          // IMMEDIATELY clear all processing states
          this.updateExecutionState(workflowId, {
            status: 'stopped',
            currentNode: null,
            forceStopped: true
          })
          
          // Force all nodes to stopped state - NO LINGERING "Processing"
          if (progressCallback) {
            progressCallback({
              nodeId: null,
              nodeName: 'System',
              progress: 0,
              status: 'stopped',
              message: 'Workflow killed by user - all processing stopped',
              forceStopped: true
            })
          }
          
          // COLLECT ALL PARTIAL RESULTS - DON'T LOSE GENERATED CONTENT
          const currentState = this.executionState.get(workflowId)
          const partialResults = currentState?.results || {}
          const allNodeOutputs = pipelineData.nodeOutputs || {}
          
          console.log('üì¶ Collecting partial results:', {
            partialResults,
            allNodeOutputs,
            nodeCount: Object.keys(allNodeOutputs).length
          })
          
          return {
            success: false,
            error: 'Workflow stopped by user',
            results: partialResults,
            partialOutputs: allNodeOutputs,
            pipelineData: pipelineData,
            stopped: true,
            forceStopped: true,
            message: 'Workflow killed by user. All processing stopped immediately.'
          }
        }

        const node = executionOrder[i]
        
        // DYNAMIC PROGRESS CALCULATION - NO HARDCODING
        // Base progress on node position, but allow individual nodes to override
        const progress = (i / executionOrder.length) * 100
        const baseProgress = progress
        const nodeProgress = progress
        
        this.updateExecutionState(workflowId, {
          currentNode: node.id,
          progress: nodeProgress,
          baseProgress: baseProgress,
          nodeIndex: i,
          totalNodes: executionOrder.length
        })
        
        if (progressCallback) {
          progressCallback({
            nodeId: node.id,
            nodeName: node.data.label,
            progress: nodeProgress,
            status: 'executing',
            providerName: null, // Will be updated when AI call starts
            baseProgress: baseProgress,
            nodeIndex: i,
            totalNodes: executionOrder.length
          })
        }

        try {
          console.log(`üîç EXECUTING NODE ${i + 1}/${executionOrder.length}: ${node.id} (${node.data.label})`)
          console.log('  - Pipeline data before execution:', pipelineData)
          
          // Execute individual node with pipeline data
          const nodeOutput = await this.executeNode(node, pipelineData, workflowId, progressCallback)
          
          console.log('  - Node output:', nodeOutput)
          
          // Add node output to pipeline
          pipelineData.nodeOutputs[node.id] = nodeOutput
          pipelineData.lastNodeOutput = nodeOutput
          
          console.log('  - Pipeline data after execution:', pipelineData)
          
          // Update execution state
          const updatedState = {
            [`results.${node.id}`]: nodeOutput,
            nodeOutputs: pipelineData.nodeOutputs,
            currentNodeIndex: i,
            completedNodes: executionOrder.slice(0, i + 1).map(n => n.id)
          }
          this.updateExecutionState(workflowId, updatedState)

          // CREATE CHECKPOINT AFTER NODE COMPLETION
          this.createCheckpoint(workflowId, node.id, {
            ...this.executionState.get(workflowId),
            ...updatedState
          })

          // PROPER COMPLETION PROGRESS - DYNAMIC CALCULATION
          const completionProgress = ((i + 1) / executionOrder.length) * 100
          
          if (progressCallback) {
            progressCallback({
              nodeId: node.id,
              nodeName: node.data.label,
              progress: completionProgress, // DYNAMIC: Shows actual completion
              status: 'completed',
              output: nodeOutput,
              nodeIndex: i + 1,
              totalNodes: executionOrder.length,
              isNodeComplete: true,
              checkpointCreated: true // Indicate checkpoint was created
            })
          }

        } catch (error) {
          const nodeError = {
            nodeId: node.id,
            nodeName: node.data.label,
            error: error.message,
            timestamp: new Date()
          }
          
          // PAUSE on failure instead of stopping - allow user to fix and resume
          this.updateExecutionState(workflowId, {
            status: 'paused',
            failedNodeId: node.id,
            failedNodeName: node.data.label,
            pauseReason: 'node_failure',
            [`errors`]: [...(this.executionState.get(workflowId)?.errors || []), nodeError]
          })

          if (progressCallback) {
            progressCallback({
              nodeId: node.id,
              nodeName: node.data.label,
              progress,
              status: 'paused',
              error: error.message,
              pauseReason: 'node_failure',
              message: `Node failed - workflow paused. Fix the issue and resume.`
            })
          }

          console.log(`‚è∏Ô∏è Workflow ${workflowId} paused due to node failure: ${node.data.label}`)
          
          // Wait for user to fix the issue and resume
          await this.waitForResume(workflowId)
          
          // After resume, retry the failed node or continue
          console.log(`‚ñ∂Ô∏è Workflow ${workflowId} resumed after node failure fix`)
          
          // Retry the current node after resume
          i-- // Decrement to retry the same node
          continue
        }
      }

      // Mark execution as completed
      this.updateExecutionState(workflowId, {
        status: 'completed',
        endTime: new Date(),
        progress: 100
      })

      // ENSURE FINAL 100% COMPLETION CALLBACK
      if (progressCallback) {
        progressCallback({
          status: 'completed',
          progress: 100, // GUARANTEED 100% completion
          message: 'Workflow execution completed successfully',
          nodeId: 'workflow-complete',
          nodeName: 'Workflow Complete',
          output: {
            success: true,
            results: pipelineData.nodeOutputs,
            lastNodeOutput: pipelineData.lastNodeOutput,
            nodeOutputs: pipelineData.nodeOutputs, // For deliverables access
            metadata: {
              totalNodes: executionOrder.length,
              executionTime: Date.now() - startTime,
              workflowId,
              completedNodes: executionOrder.length,
              successRate: 100
            }
          }
        })
      }

      return pipelineData

    } catch (error) {
      this.updateExecutionState(workflowId, {
        status: 'error',
        endTime: new Date(),
        finalError: error.message
      })
      throw error
    }
  }

  /**
   * Check if a node can be skipped based on content quality and workflow state
   */
  canSkipNode(node, pipelineData, workflowId = null) {
    // Skip Editor nodes if content is already high quality
    if (node.type === 'editor' && pipelineData.lastNodeOutput?.content) {
      const qualityCheck = this.assessContentQuality(pipelineData.lastNodeOutput.content)
      if (qualityCheck.isHighQuality) {
        console.log(`‚è≠Ô∏è Skipping ${node.type} node - content already high quality (${qualityCheck.score}/100)`)
        return {
          skip: true,
          reason: 'Content already meets quality standards',
          qualityScore: qualityCheck.score
        }
      }
    }

    // Skip duplicate content writer nodes if content already exists
    if (node.type === 'content_writer' && pipelineData.lastNodeOutput?.chapters?.length > 0) {
      const existingChapters = pipelineData.lastNodeOutput.chapters
      const targetChapters = pipelineData.chapter_count || pipelineData.totalChapters || 8
      
      if (existingChapters.length >= targetChapters) {
        console.log(`‚è≠Ô∏è Skipping ${node.type} node - sufficient chapters already generated (${existingChapters.length}/${targetChapters})`)
        return {
          skip: true,
          reason: 'Sufficient chapters already generated',
          existingChapters: existingChapters.length
        }
      }
    }

    // Skip research nodes if research data already exists
    if (node.type === 'researcher' && pipelineData.lastNodeOutput?.researchData) {
      console.log(`‚è≠Ô∏è Skipping ${node.type} node - research data already available`)
      return {
        skip: true,
        reason: 'Research data already available'
      }
    }

    return { skip: false }
  }

  /**
   * Execute a single node with real AI processing
   * @param {Object} node - Node to execute
   * @param {Object} pipelineData - Current pipeline data
   * @param {string} workflowId - Workflow execution ID
   * @returns {Object} Node output
   */
  async executeNode(node, pipelineData, workflowId, progressCallback = null) {
    // Check if this node can be skipped for optimization
    const skipCheck = this.canSkipNode(node, pipelineData, workflowId)
    if (skipCheck.skip) {
      console.log(`‚è≠Ô∏è Skipping node ${node.id} (${node.type}): ${skipCheck.reason}`)
      return {
        success: true,
        output: {
          content: pipelineData.lastNodeOutput?.content || '',
          skipped: true,
          skipReason: skipCheck.reason,
          ...skipCheck
        },
        metadata: {
          nodeType: node.type,
          processingTime: 0,
          skipped: true,
          skipReason: skipCheck.reason
        }
      }
    }
    const { type, data } = node
    
    switch (type) {
      case 'input':
        return await this.executeInputNode(data, pipelineData)
      
      case 'process':
        return await this.executeProcessNode(data, pipelineData, progressCallback, workflowId)
      
      case 'preview':
        return await this.executePreviewNode(data, pipelineData, progressCallback)
      
      case 'condition':
        return await this.executeConditionNode(data, pipelineData)
      
      case 'output':
        return await this.executeOutputNode(data, pipelineData)
      
      default:
        throw new Error(`Unknown node type: ${type}`)
    }
  }

  /**
   * Execute input node - validate and structure user input
   */
  async executeInputNode(nodeData, pipelineData) {
    const { userInput } = pipelineData
    const { testInputEnabled, testInputValues, processingInstructions } = nodeData

    // Use test input values if enabled, otherwise use regular userInput
    const inputToUse = testInputEnabled && testInputValues ? testInputValues : userInput
    
    console.log('üîç INPUT NODE JSON WRAPPER:')
    console.log('  - Using processingInstructions from nodePalettes.js')
    console.log('  - inputToUse:', inputToUse)

    // Create JSON wrapper as per nodePalettes.js instructions
    const jsonWrapper = {
      user_input: inputToUse,
      metadata: {
        node_id: nodeData.id || 'input-node',
        timestamp: new Date().toISOString(),
        status: 'processed',
        workflow_type: nodeData.role || 'universal'
      },
      next_node_data: {
        original_input: inputToUse,
        processing_instructions: 'All user data wrapped and ready for next node'
      }
    }

    console.log('üîç JSON WRAPPER CREATED:', jsonWrapper)

    return {
      type: 'input_json_wrapper',
      content: jsonWrapper,
      metadata: {
        nodeId: nodeData.id || 'input-node',
        timestamp: new Date(),
        wrapperCreated: true
      }
    }
  }

  /**
   * Execute process node - call AI with real API requests
   * DYNAMIC MULTI-CHAPTER SUPPORT: Reads userInput.chapterCount and follows node instructions
   */
  async executeProcessNode(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { 
      aiEnabled, 
      selectedModels, 
      systemPrompt, 
      userPrompt, 
      temperature, 
      maxTokens, 
      inputInstructions,
      processingInstructions
    } = nodeData
    
    // PERMISSION ENFORCEMENT: Get node role configuration
    const nodeRole = nodeData.role || nodeData.id
    const roleConfig = NODE_ROLE_CONFIG[nodeRole]
    
    if (roleConfig) {
      console.log(`üîê PERMISSION CHECK for node "${nodeData.label}" (${nodeRole}):`)
      console.log(`   - canWriteContent: ${roleConfig.canWriteContent}`)
      console.log(`   - canEditStructure: ${roleConfig.canEditStructure}`)
      console.log(`   - canProofRead: ${roleConfig.canProofRead}`)
      
      // Store permissions in nodeData for AI prompt enforcement
      nodeData.permissions = {
        canWriteContent: roleConfig.canWriteContent,
        canEditStructure: roleConfig.canEditStructure,
        canProofRead: roleConfig.canProofRead
      }
    } else {
      console.warn(`‚ö†Ô∏è No role configuration found for node: ${nodeRole}`)
      // Default to no permissions
      nodeData.permissions = {
        canWriteContent: false,
        canEditStructure: false,
        canProofRead: false
      }
    }
    
    if (!aiEnabled) {
      // Non-AI processing node
      return this.executeNonAIProcessing(nodeData, pipelineData)
    }

    // STEP 1: RECEIVE - Get previous node output and store in previousNodePassover
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    const { userInput } = pipelineData
    
    // STEP 2: STORE - Store complete previous node data in previousNodePassover
    const previousNodePassover = {
      previousOutput: previousOutput,
      originalUserInput: userInput,
      timestamp: new Date().toISOString(),
      nodeContext: 'stored_for_passover'
    }
    
    // Add previousNodePassover to pipelineData for template processing
    pipelineData.previousNodePassover = previousNodePassover
    // Backward-compatible alias expected by prompts and templates
    pipelineData.previous_node_output = previousNodePassover
    
    console.log('üì¶ PREVIOUS NODE PASSOVER: Stored previous data for context preservation')
    console.log('   - Previous output keys:', Object.keys(previousOutput || {}))
    console.log('   - User input keys:', Object.keys(userInput || {}))
    
    // CRITICAL: Extract user_input from JSON wrapper if available, otherwise fall back to userInput
    let structuredData = userInput
    
    if (previousOutput?.content?.user_input) {
      // Input node returned JSON wrapper - extract the user_input
      structuredData = previousOutput.content.user_input
      console.log('üîç PROCESS NODE: Using user_input from JSON wrapper:', structuredData)
    } else if (previousOutput?.structuredData) {
      // Legacy fallback for old structuredData format
      structuredData = previousOutput.structuredData
      console.log('üîç PROCESS NODE: Using legacy structuredData:', structuredData)
    } else {
      console.log('üîç PROCESS NODE: Using direct userInput:', structuredData)
    }
    

    // Determine chapter count: RESPECT USER INPUT FIRST, fallback to AI determination only if no user input
    let chapterCount = structuredData.chapterCount || structuredData.chapter_count || structuredData['Chapter Count'] || structuredData['Number of Chapters']
    
    // Parse chapter count if it's a string like "2-3" or "6-8"
    if (chapterCount && typeof chapterCount === 'string') {
      if (chapterCount.includes('-')) {
        // Take the higher number from ranges like "2-3" -> 3, "6-8" -> 8
        const parts = chapterCount.split('-')
        chapterCount = parseInt(parts[1]) || parseInt(parts[0]) || 1
      } else {
        chapterCount = parseInt(chapterCount) || 1
      }
    }
    
    if (!chapterCount || chapterCount < 1) {
      // NO hardcoded calculation - let AI decide based on story premise and word count
      const wordCount = structuredData.word_count || structuredData['Word Count'] || 2000
      const storyPremise = structuredData.story_premise || structuredData['Story Premise'] || 'A compelling story'
      
      // Pass to AI to determine optimal chapter count
      chapterCount = await this.determineOptimalChapterCount(wordCount, storyPremise, structuredData)
      
      console.log('üîç AI CHAPTER DETERMINATION:')
      console.log('  - Word count:', wordCount)
      console.log('  - Story premise:', storyPremise)
      console.log('  - AI determined chapters:', chapterCount)
    } else {
      console.log('üîç USER-SPECIFIED CHAPTERS (RESPECTED):', chapterCount)
    }
    
    console.log('üîç CHAPTER COUNT DEBUG:')
    console.log('  - structuredData:', structuredData)
    console.log('  - structuredData.chapterCount:', structuredData.chapterCount)
    console.log('  - structuredData.chapter_count:', structuredData.chapter_count)
    console.log('  - structuredData["Chapter Count"]:', structuredData['Chapter Count'])
    console.log('  - userInput.chapterCount:', userInput.chapterCount)
    console.log('  - userInput.chapter_count:', userInput.chapter_count)
    console.log('  - userInput["Chapter Count"]:', userInput['Chapter Count'])
    console.log('  - Final chapterCount:', chapterCount)
    console.log('  - Type:', typeof chapterCount)
    
    // CRITICAL: Distinguish between content generation and content refinement
    // Use nodeRole from permission check above, with label-based fallback
    const nodeLabel = (nodeData.label || '').toLowerCase()
    const isLabelBasedWriter = nodeLabel.includes('writing') || 
                               nodeLabel.includes('literary') || 
                               nodeLabel.includes('narrative') || 
                               nodeLabel.includes('content writer') ||
                               nodeLabel.includes('technical writer') ||
                               nodeLabel.includes('copywriter')
    
    const isContentWriter = nodeRole === 'content_writer' || 
                           nodeRole === 'technical_writer' || 
                           nodeRole === 'copywriter' ||
                           isLabelBasedWriter
    
    const isEditor = nodeRole === 'editor'

    console.log(`üîç NODE ROLE CHECK: ${nodeRole}, isContentWriter: ${isContentWriter}, isEditor: ${isEditor}`)
    console.log(`üîç NODE LABEL: ${nodeData.label}`)
    console.log(`üîç LABEL-BASED WRITER DETECTION: ${isLabelBasedWriter}`)

    if (parseInt(chapterCount) > 1 && isContentWriter) {
      // Multi-chapter generation: ONLY for content writing nodes
      console.log(`üîç STARTING MULTI-CHAPTER GENERATION: ${chapterCount} chapters`)
      return await this.generateMultipleChapters(nodeData, pipelineData, parseInt(chapterCount), progressCallback, workflowId)
    } else if (isEditor) {
      // Content refinement: Editor processes existing content with checklist
      console.log(`üîç STARTING CONTENT REFINEMENT (Editor node)`)
      return await this.executeContentRefinement(nodeData, pipelineData, progressCallback, workflowId)
    } else {
      // Single generation: For research, analysis, and other non-writing nodes
      console.log(`üîç STARTING SINGLE GENERATION (${isContentWriter ? 'content writer' : 'research/analysis node'})`)
      return await this.executeSingleAIGeneration(nodeData, pipelineData, progressCallback, workflowId)
    }
  }

  /**
   * Generate multiple chapters based on user input and node instructions
   * This follows the node's own instructions dynamically
   */
  async generateMultipleChapters(nodeData, pipelineData, chapterCount, progressCallback = null, workflowId = null) {
    const results = []
    
    console.log(`üîç GENERATING ${chapterCount} CHAPTERS`)
    
    for (let i = 1; i <= chapterCount; i++) {
      // CRITICAL: Check if workflow was stopped before each chapter
      if (workflowId && this.isWorkflowStopped(workflowId)) {
        console.log(`üõë WORKFLOW STOPPED DURING CHAPTER GENERATION - PRESERVING ${results.length} COMPLETED CHAPTERS`)
        
        // Compile partial results into a book
        if (results.length > 0) {
          const nodeOutputs = this.convertResultsToNodeOutputs(results)
          const partialBookResult = await bookCompilationService.compileBook(nodeOutputs, pipelineData.userInput)
          const partialBook = partialBookResult.content
          return {
            type: 'partial_book',
            content: partialBook,
            metadata: {
              nodeId: nodeData.id,
              timestamp: new Date(),
              chaptersCompleted: results.length,
              totalChapters: chapterCount,
              stopped: true,
              partialGeneration: true
            },
            partialResults: results,
            stopped: true,
            message: `Workflow stopped. ${results.length} of ${chapterCount} chapters completed.`
          }
        } else {
          return {
            type: 'stopped_no_content',
            content: 'Workflow stopped before any chapters were generated.',
            metadata: {
              nodeId: nodeData.id,
              timestamp: new Date(),
              chaptersCompleted: 0,
              totalChapters: chapterCount,
              stopped: true
            },
            stopped: true,
            message: 'Workflow stopped before any content was generated.'
          }
        }
      }
      
      console.log(`üîç GENERATING CHAPTER ${i} OF ${chapterCount}`)
      
      // Update pipeline data with current chapter context
      const chapterPipelineData = {
        ...pipelineData,
        currentChapter: i,
        totalChapters: chapterCount,
        previousChapters: results
      }
      
      // Update progress callback to show chapter progress
      if (progressCallback) {
        // More granular progress: Previous chapters complete + current chapter starting
        const baseProgress = ((i - 1) / chapterCount) * 100
        const chapterStartProgress = baseProgress + (5 / chapterCount) // 5% for starting chapter
        progressCallback({
          nodeId: nodeData.id,
          nodeName: `${nodeData.label} (Generating Chapter ${i}/${chapterCount})`,
          status: 'executing',
          progress: Math.round(chapterStartProgress),
          providerName: null,
          timestamp: new Date().toLocaleTimeString(),
          cost: 0,
          tokens: 0,
          words: 0,
          chapterInfo: {
            currentChapter: i,
            totalChapters: chapterCount,
            chapterStatus: 'starting'
          }
        })
      }
      
      // RETRY LOGIC: Attempt chapter generation with retries
      const MAX_RETRIES = 3
      let chapterResult = null
      let lastChapterError = null
      
      for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
        try {
          console.log(`üîÑ Chapter ${i} Generation Attempt ${attempt}/${MAX_RETRIES}`)
          
          // Generate this chapter using the node's instructions
          chapterResult = await this.executeSingleAIGeneration(nodeData, chapterPipelineData, progressCallback)
          
          // VALIDATE CHAPTER CONTENT BEFORE ACCEPTING
          const chapterValidation = aiResponseValidator.validateResponse(
            { content: chapterResult.content },
            'chapter'
          )
          
          if (!chapterValidation.isValid) {
            const validationErrors = chapterValidation.errors
              .map(e => `${e.code}: ${e.message}`)
              .join('; ')
            
            throw new Error(`Chapter validation failed: ${validationErrors}`)
          }
          
          // Chapter is valid - break retry loop
          console.log(`‚úÖ Chapter ${i} validated successfully`)
          break
          
        } catch (error) {
          lastChapterError = error
          console.error(`‚ùå Chapter ${i} Attempt ${attempt} failed:`, error.message)
          
          if (attempt < MAX_RETRIES) {
            // Exponential backoff: 2s, 4s, 8s
            const backoffDelay = Math.pow(2, attempt) * 1000
            console.log(`‚è≥ Waiting ${backoffDelay}ms before retry...`)
            await new Promise(resolve => setTimeout(resolve, backoffDelay))
          }
        }
      }
      
      // CRITICAL: If all retries failed, stop the workflow
      if (!chapterResult) {
        const errorMessage = `Chapter ${i} generation failed after ${MAX_RETRIES} attempts. Last error: ${lastChapterError?.message}`
        console.error(`üö® ${errorMessage}`)
        
        if (progressCallback) {
          progressCallback({
            nodeId: nodeData.id,
            nodeName: `${nodeData.label} (Failed)`,
            status: 'failed',
            error: errorMessage,
            chapterInfo: {
              currentChapter: i,
              totalChapters: chapterCount,
              chapterStatus: 'failed',
              attemptsBeforeFailing: MAX_RETRIES
            }
          })
        }
        
        throw new Error(errorMessage)
      }
      
      // Push validated chapter to results
      results.push({
        chapter: i,
        content: chapterResult.content,
        metadata: chapterResult.metadata,
        aiMetadata: chapterResult.aiMetadata
      })
      
      console.log(`‚úÖ CHAPTER ${i} COMPLETED: ${chapterResult.content.length} characters`)
      
      // Update progress callback with chapter completion
      if (progressCallback) {
        const chapterCompleteProgress = (i / chapterCount) * 100
        progressCallback({
          nodeId: nodeData.id,
          nodeName: `${nodeData.label} (Chapter ${i}/${chapterCount} Complete)`,
          status: i === chapterCount ? 'completed' : 'executing',
          progress: Math.round(chapterCompleteProgress),
          providerName: chapterResult.aiMetadata?.provider || null,
          timestamp: new Date().toLocaleTimeString(),
          cost: chapterResult.aiMetadata?.cost || 0,
          tokens: chapterResult.aiMetadata?.tokens || 0,
          words: chapterResult.aiMetadata?.words || 0,
          chapterInfo: {
            currentChapter: i,
            totalChapters: chapterCount,
            chapterStatus: 'completed',
            chapterWordCount: chapterResult.aiMetadata?.words || 0
          }
        })
      }
      
      // CRITICAL: Check if workflow was stopped after each chapter
      if (workflowId && this.isWorkflowStopped(workflowId)) {
        console.log(`üõë WORKFLOW STOPPED AFTER CHAPTER ${i} - PRESERVING ${results.length} COMPLETED CHAPTERS`)
        
        // Compile partial results into a book
        const nodeOutputs = this.convertResultsToNodeOutputs(results)
      const partialBookResult = await bookCompilationService.compileBook(nodeOutputs, pipelineData.userInput)
      const partialBook = partialBookResult.content
        return {
          type: 'partial_book',
          content: partialBook,
          metadata: {
            nodeId: nodeData.id,
            timestamp: new Date(),
            chaptersCompleted: results.length,
            totalChapters: chapterCount,
            stopped: true,
            partialGeneration: true
          },
          partialResults: results,
          stopped: true,
          message: `Workflow stopped. ${results.length} of ${chapterCount} chapters completed.`
        }
      }
      
      // Add small delay between chapters to prevent rate limiting
      if (i < chapterCount) {
        await new Promise(resolve => setTimeout(resolve, 1000))
      }
    }
    
    // Compile all chapters into complete book
    const nodeOutputs = this.convertResultsToNodeOutputs(results)
    const completeBookResult = await bookCompilationService.compileBook(nodeOutputs, pipelineData.userInput)
    const completeBook = completeBookResult.content
    
    // Calculate total metrics
    const totalWords = results.reduce((sum, result) => sum + (result.aiMetadata?.words || 0), 0)
    const totalTokens = results.reduce((sum, result) => sum + (result.aiMetadata?.tokens || 0), 0)
    const totalCost = results.reduce((sum, result) => sum + (result.aiMetadata?.cost || 0), 0)
    
    return {
      type: 'multi_chapter_generation',
      content: completeBook,
      chapters: results,
      aiMetadata: {
        model: nodeData.selectedModels[0],
        totalChapters: chapterCount,
        totalWords: totalWords,
        totalTokens: totalTokens,
        totalCost: totalCost
      },
      metadata: {
        nodeId: nodeData.id || 'process-node',
        timestamp: new Date(),
        chapterCount: chapterCount,
        totalCharacters: completeBook.length
      },
      structuredData: {
        ...pipelineData.userInput,
        // Don't pass completeBook to subsequent nodes - only pass current chapter data
        currentChapter: chapterCount, // Mark this as the final chapter
        totalChapters: chapterCount,
        totalWords: totalWords,
        // Store chapters separately for final compilation only
        _chapters: results
      }
    }
  }

  /**
   * Execute single AI generation - used for both single chapters and individual chapters in multi-chapter mode
   */
  async executeSingleAIGeneration(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { selectedModels, systemPrompt, userPrompt, temperature, maxTokens, inputInstructions } = nodeData
    
    // Get previous node output
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    // Ensure previousNodePassover is available for template processing
    if (!pipelineData.previousNodePassover) {
      pipelineData.previousNodePassover = previousOutput || pipelineData.userInput
    }
    
    // Build dynamic prompt with real data substitution
    const processedPrompts = this.processPromptVariables({
      systemPrompt,
      userPrompt
    }, pipelineData, nodeData.permissions)

    // Get allData for word count enforcement
    const { userInput, nodeOutputs, lastNodeOutput } = pipelineData
    
    // Extract user_input from JSON wrapper if available
    let structuredData = userInput
    if (lastNodeOutput?.content?.user_input) {
      structuredData = lastNodeOutput.content.user_input
    } else if (lastNodeOutput?.structuredData) {
      structuredData = lastNodeOutput.structuredData
    }
    
    // Filter out completeBook and other compilation data to prevent passing to subsequent nodes
    const filteredStructuredData = { ...structuredData }
    delete filteredStructuredData.completeBook
    delete filteredStructuredData._chapters
    
    const allData = { 
      ...userInput, 
      ...filteredStructuredData,
      currentChapter: pipelineData.currentChapter,
      totalChapters: pipelineData.totalChapters,
      previousChapters: pipelineData.previousChapters
    }

    // Debug selectedModels
    console.log('üîç Selected models for AI generation:', selectedModels)
    console.log('üîç First selected model:', selectedModels[0])
    
    if (!selectedModels || selectedModels.length === 0) {
      throw new Error('No AI models selected for this node. Please configure AI Integration in the node modal.')
    }
    
    // CRITICAL: Check if workflow was stopped before AI call
    if (workflowId && this.isWorkflowStopped(workflowId)) {
      console.log(`üõë WORKFLOW STOPPED BEFORE AI CALL - RETURNING STOPPED RESULT`)
      return {
        type: 'stopped_before_ai',
        content: 'Workflow stopped before AI generation could complete.',
        metadata: {
          nodeId: nodeData.id,
          timestamp: new Date(),
          stopped: true
        },
        stopped: true,
        message: 'Workflow stopped before AI generation.'
      }
    }

    // Get AI service for the selected model
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    console.log('üîç Parsed model config:', modelConfig)
    const aiServiceInstance = this.getAIService(modelConfig.provider)

    // Add timeout to prevent infinite loops - increased for book generation
    // Use dynamic timeout based on content type
    const isBookGeneration = nodeData.label?.toLowerCase().includes('writing') || 
                           nodeData.label?.toLowerCase().includes('narrative') ||
                           nodeData.label?.toLowerCase().includes('literary')
    
    const timeoutDuration = isBookGeneration ? 600000 : 300000 // 10 minutes for book generation, 5 minutes for others
    const timeoutPromise = new Promise((_, reject) => {
      setTimeout(() => reject(new Error(`AI generation timeout after ${timeoutDuration / 60000} minutes`)), timeoutDuration)
    })

    // Update progress with provider info
    if (progressCallback) {
      progressCallback({
        nodeId: nodeData.id,
        nodeName: nodeData.label || 'AI Generation',
        status: 'executing',
        progress: 50, // Mid-point when AI call starts
        providerName: modelConfig.providerName,
        timestamp: new Date().toLocaleTimeString(),
        cost: 0,
        tokens: 0,
        words: 0,
        aiResponse: null,
        processedContent: null,
        rawData: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          providerName: modelConfig.providerName,
          temperature: temperature || 0.7,
          maxTokens: maxTokens || 8000,
          systemPrompt: processedPrompts.systemPrompt,
          userPrompt: processedPrompts.userPrompt,
          inputData: allData,
          dynamicInputs: Object.keys(allData),
          modelCostPer1k: modelConfig.costPer1k
        }
      })
    }

    if (!aiServiceInstance) {
      throw new Error(`AI service not available for provider: ${modelConfig.provider}`)
    }

    try {
      // Build final prompt combining system and user prompts
      let finalPrompt = processedPrompts.systemPrompt 
        ? `${processedPrompts.systemPrompt}\n\n${processedPrompts.userPrompt}`
        : processedPrompts.userPrompt

      // CRITICAL: Add explicit word count enforcement
      const wordCount = allData.word_count || allData['Word Count'] || '2000'
      const chapterCount = allData.chapter_count || allData['Chapter Count'] || '1'
      const wordsPerChapter = Math.floor(parseInt(wordCount) / parseInt(chapterCount))
      
      console.log('üîç WORD COUNT ENFORCEMENT DEBUG:')
      console.log('  - wordCount:', wordCount)
      console.log('  - chapterCount:', chapterCount)
      console.log('  - wordsPerChapter:', wordsPerChapter)
      
      // DYNAMIC INPUT PROCESSING - Include ALL variables from ANY flow
      const dynamicInputs = []
      const excludedKeys = ['currentChapter', 'totalChapters', 'previousChapters', 'word_count', 'chapter_count', 'Word Count', 'Chapter Count']
      
      Object.entries(allData).forEach(([key, value]) => {
        if (value && !excludedKeys.includes(key) && typeof value === 'string' && value.trim() !== '') {
          const formattedKey = key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase())
          dynamicInputs.push(`- ${formattedKey}: ${value}`)
        }
      })

      const wordCountEnforcement = `

CRITICAL BOOK STRUCTURE REQUIREMENTS:
- TOTAL WORD COUNT: ${wordCount} words (NON-NEGOTIABLE)
- CHAPTER COUNT: ${chapterCount} chapters
- WORDS PER CHAPTER: ${wordsPerChapter} words (¬±10%)
- CURRENT CHAPTER: ${allData.currentChapter || 1} of ${chapterCount}

DYNAMIC USER INPUTS (ALL PROVIDED INFORMATION):
${dynamicInputs.length > 0 ? dynamicInputs.join('\n') : '- No specific inputs provided'}

CRITICAL ORCHESTRATION REQUIREMENTS:
- YOU MUST GENERATE A COMPLETE, PROFESSIONAL BOOK CHAPTER
- INCLUDE PROPER CHAPTER TITLE AND STRUCTURE
- USE ALL PROVIDED INPUTS ABOVE TO CREATE RELEVANT CONTENT
- INCORPORATE ALL USER-SPECIFIED DETAILS INTO THE NARRATIVE

CHAPTER STRUCTURE REQUIREMENTS:
- START WITH CHAPTER TITLE: "Chapter ${allData.currentChapter || 1}: [Descriptive Title]"
- INCLUDE PROPER INTRODUCTION TO THE CHAPTER
- DEVELOP MAIN CONTENT WITH CLEAR SECTIONS
- INCLUDE PRACTICAL EXAMPLES AND APPLICATIONS
- END WITH CHAPTER SUMMARY AND TRANSITION TO NEXT CHAPTER

WORD COUNT ENFORCEMENT:
- YOU MUST GENERATE EXACTLY ${wordsPerChapter} WORDS FOR THIS CHAPTER
- COUNT YOUR WORDS AND ENSURE YOU MEET THE REQUIREMENT
- NO LESS, NO MORE - EXACTLY ${wordsPerChapter} WORDS

CHAPTER CONTEXT:
- YOU ARE WRITING CHAPTER ${allData.currentChapter || 1} OF ${chapterCount} TOTAL CHAPTERS
- ${allData.currentChapter > 1 ? 'BUILD UPON PREVIOUS CHAPTERS WITHOUT REPEATING CONTENT' : 'ESTABLISH THE FOUNDATION FOR THE ENTIRE BOOK'}

${narrativeStructureService.buildChapterContext(
  allData.currentChapter || 1,
  allData.totalChapters || chapterCount,
  allData.previousChapters || [],
  allData
)}

CUSTOM INSTRUCTIONS: ${allData.custom_instructions || 'Generate comprehensive, professional content that provides real value to the target audience.'}`

      finalPrompt = finalPrompt + wordCountEnforcement
      
      // Debug final prompt
      console.log('üìù Final Prompt Debug:')
      console.log('  - System prompt:', processedPrompts.systemPrompt)
      console.log('  - User prompt:', processedPrompts.userPrompt)
      console.log('  - Dynamic inputs included:', dynamicInputs.length)
      console.log('  - Dynamic inputs:', dynamicInputs)
      console.log('  - Final prompt length:', finalPrompt.length)
      console.log('  - Final prompt preview:', finalPrompt.substring(0, 500))
      
      // Set SuperAdmin user in aiService to load API keys from database FIRST
      // The SuperAdmin user should be passed from the calling component
      if (pipelineData.superAdminUser) {
        await aiServiceInstance.setUser(pipelineData.superAdminUser)
      } else {
        throw new Error('SuperAdmin user not provided for AI service')
      }
      
      // Check if provider is available and has API key AFTER loading providers
      if (!aiServiceInstance.providers[modelConfig.providerName]) {
        throw new Error(`Provider ${modelConfig.providerName} not available. Please check API key configuration.`)
      }
      
      const providerConfig = aiServiceInstance.providers[modelConfig.providerName]
      if (!providerConfig.apiKey) {
        throw new Error(`No API key configured for ${modelConfig.providerName}. Please add API key in SuperAdmin settings.`)
      }
      
      // Make REAL AI API call using aiService with specific provider and maxTokens
      // Wrap with timeout to prevent infinite loops
      let aiResponse
      let lastError
      
      try {
        aiResponse = await Promise.race([
          aiServiceInstance.generateContent(finalPrompt, modelConfig.providerName, maxTokens || 8000),
          timeoutPromise
        ])
        console.log('‚úÖ Primary provider succeeded:', modelConfig.providerName)
      } catch (error) {
        console.error(`‚ùå Primary provider ${modelConfig.providerName} failed:`, error.message)
        lastError = error
        
        // If it's a timeout, try to get partial response if available
        if (error.message.includes('timeout')) {
          console.log('‚è∞ Timeout occurred, checking for partial response...')
          // Could potentially implement partial response recovery here
        }
        
        // Try fallback providers if primary fails
        const fallbackProviders = ['OPENA-01-first', 'CLAUD-01-lekhika', 'MISTR-01-1']
        for (const fallbackProvider of fallbackProviders) {
          if (fallbackProvider === modelConfig.providerName) continue
          
          try {
            console.log(`üîÑ Trying fallback provider: ${fallbackProvider}`)
            aiResponse = await Promise.race([
              aiServiceInstance.generateContent(finalPrompt, fallbackProvider, maxTokens || 8000),
              timeoutPromise
            ])
            console.log(`‚úÖ Fallback provider ${fallbackProvider} succeeded`)
            break
          } catch (fallbackError) {
            console.error(`‚ùå Fallback provider ${fallbackProvider} failed:`, fallbackError.message)
            lastError = fallbackError
          }
        }
        
        if (!aiResponse) {
          // Enhanced error reporting with specific provider failures
          const errorDetails = {
            primaryProvider: modelConfig.providerName,
            fallbackProviders: fallbackProviders,
            lastError: lastError?.message || 'Unknown error',
            errorType: lastError?.name || 'ProviderError',
            timestamp: new Date().toISOString()
          }
          
          console.error('‚ùå All AI providers failed:', errorDetails)
          
          // Provide actionable error message
          let errorMessage = `Content generation failed: All AI providers failed. `
          if (lastError?.message?.includes('rate limit')) {
            errorMessage += `Rate limit exceeded. Please try again later or check your API key limits.`
          } else if (lastError?.message?.includes('timeout')) {
            errorMessage += `Request timeout. The content may be too complex or the API is slow.`
          } else if (lastError?.message?.includes('invalid')) {
            errorMessage += `Invalid request. Please check your API configuration and content format.`
          } else if (lastError?.message?.includes('authentication')) {
            errorMessage += `Authentication failed. Please check your API keys.`
          } else {
            errorMessage += `Last error: ${lastError?.message || 'Unknown error'}`
          }
          
          throw new Error(errorMessage)
        }
      }

      // VALIDATE AI RESPONSE - NO GARBAGE ALLOWED
      console.log('üîç Validating AI response quality...')
      const validation = aiResponseValidator.validateResponse(aiResponse, 'chapter')
      
      // Debug AI response
      console.log('ü§ñ AI Response Debug:')
      console.log('  - Raw response type:', typeof aiResponse)
      console.log('  - Validation result:', validation.isValid ? '‚úÖ VALID' : '‚ùå INVALID')
      console.log('  - Errors:', validation.errors)
      console.log('  - Warnings:', validation.warnings)
      console.log('  - Extracted content length:', validation.content?.length || 0)
      console.log('  - Content preview:', validation.content?.substring(0, 200))
      
      // CRITICAL: Reject invalid responses - NO BAND-AIDS
      if (!validation.isValid) {
        const errorReport = aiResponseValidator.createErrorReport(validation)
        console.error('‚ùå AI Response Validation Failed:', errorReport)
        
        const errorMessage = validation.errors
          .map(e => `${e.code}: ${e.message}`)
          .join('; ')
        
        throw new Error(`AI response validation failed: ${errorMessage}. Recommendation: ${errorReport.recommendation}`)
      }
      
      // Use validated content
      const content = validation.content
      
      // Calculate real metrics with actual AI response data
      const actualWordCount = typeof content === 'string' ? content.split(' ').length : 0
      
      // Get actual token count from AI response usage data
      let actualTokens = 0
      let actualCost = 0
      
      if (aiResponse && aiResponse.usage) {
        // Get actual token count from AI provider response
        actualTokens = aiResponse.usage.total_tokens || 
                      aiResponse.usage.completion_tokens || 
                      aiResponse.token_count ||
                      Math.ceil(actualWordCount * 1.3) // Fallback estimation
        
        // Calculate cost using actual model pricing
        const modelCostPer1k = modelConfig.costPer1k || 0.00003
        actualCost = (actualTokens / 1000) * modelCostPer1k
      } else {
        // Fallback to estimation if no usage data
        actualTokens = Math.ceil(actualWordCount * 1.3)
        actualCost = (actualTokens / 1000) * (modelConfig.costPer1k || 0.00003)
      }
      
      // Send progress update with AI response data
      if (progressCallback) {
        progressCallback({
          nodeId: nodeData.id,
          nodeName: nodeData.label || 'AI Generation',
          status: 'processing',
          progress: 75, // Processing AI response
          providerName: modelConfig.providerName,
          timestamp: new Date().toLocaleTimeString(),
          cost: actualCost,
          tokens: actualTokens,
          words: actualWordCount,
          aiResponse: aiResponse,
          processedContent: content,
          rawData: {
            model: selectedModels[0],
            provider: modelConfig.provider,
            providerName: modelConfig.providerName,
            temperature: temperature || 0.7,
            maxTokens: maxTokens || 8000,
            systemPrompt: processedPrompts.systemPrompt,
            userPrompt: processedPrompts.userPrompt,
            inputData: allData,
            dynamicInputs: Object.keys(allData),
            modelCostPer1k: modelConfig.costPer1k,
            actualTokens: actualTokens,
            actualCost: actualCost
          }
        })
      }
      
      
      // Update progress with completion and metrics
      if (progressCallback) {
        progressCallback({
          nodeId: nodeData.id,
          nodeName: nodeData.label || 'AI Generation',
          status: 'completed',
          progress: 100,
          providerName: modelConfig.providerName,
          timestamp: new Date().toLocaleTimeString(),
          cost: actualCost,
          tokens: actualTokens,
          words: actualWordCount,
          output: content.substring(0, 200) + (content.length > 200 ? '...' : ''),
          aiResponse: aiResponse,
          processedContent: content,
          rawData: {
            model: selectedModels[0],
            provider: modelConfig.provider,
            temperature,
            maxTokens,
            systemPrompt: processedPrompts.systemPrompt,
            userPrompt: processedPrompts.userPrompt,
            inputData: previousOutput,
            dynamicInputs: dynamicInputs,
            modelCostPer1k: modelConfig.costPer1k
          }
        })
      }

      // STEP 4: COMBINE - AI output + previousNodePassover data
      const combinedDataPackage = {
        type: 'ai_generation',
        content: content,
        previousNodePassover: pipelineData.previousNodePassover, // CRITICAL: Include previous node data
        aiMetadata: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          tokens: actualTokens,
          cost: actualCost,
          words: actualWordCount,
          modelCostPer1k: modelConfig.costPer1k
        },
        inputData: previousOutput,
        instructions: inputInstructions,
        metadata: {
          nodeId: nodeData.id || 'process-node',
          timestamp: new Date(),
          processingTime: aiResponse.processingTime || 0,
          dataPreservation: 'complete_context_maintained'
        }
      }
      
      console.log('üîó COMBINED DATA PACKAGE: AI output + previous node data ready for next node')
      console.log('   - AI content length:', content?.length || 0)
      console.log('   - Previous data preserved:', !!pipelineData.previousNodePassover)
      
      // STEP 5: PASSOVER - Return combined package for next workflow node
      return combinedDataPackage

    } catch (error) {
      throw new Error(`AI generation failed: ${error.message}`)
    }
  }

  /**
   * Assess content quality to determine if refinement is needed
   */
  assessContentQuality(content) {
    if (!content || typeof content !== 'string') {
      return {
        isHighQuality: false,
        score: 0,
        wordCount: 0,
        assessment: 'Invalid or empty content'
      }
    }

    const wordCount = content.trim().split(/\s+/).filter(word => word.length > 0).length
    const sentences = content.split(/[.!?]+/).filter(s => s.trim().length > 0).length
    const paragraphs = content.split(/\n\s*\n/).filter(p => p.trim().length > 0).length

    let score = 0
    let assessment = []

    // Word count assessment (0-30 points)
    if (wordCount >= 1000) {
      score += 30
      assessment.push('Excellent word count')
    } else if (wordCount >= 500) {
      score += 20
      assessment.push('Good word count')
    } else if (wordCount >= 300) {
      score += 10
      assessment.push('Adequate word count')
    } else {
      assessment.push('Low word count')
    }

    // Structure assessment (0-25 points)
    if (sentences >= 20) {
      score += 25
      assessment.push('Well-structured with good sentence variety')
    } else if (sentences >= 10) {
      score += 15
      assessment.push('Decent sentence structure')
    } else if (sentences >= 5) {
      score += 10
      assessment.push('Basic sentence structure')
    } else {
      assessment.push('Poor sentence structure')
    }

    // Paragraph structure (0-20 points)
    if (paragraphs >= 5) {
      score += 20
      assessment.push('Good paragraph structure')
    } else if (paragraphs >= 3) {
      score += 15
      assessment.push('Adequate paragraph structure')
    } else if (paragraphs >= 1) {
      score += 10
      assessment.push('Basic paragraph structure')
    } else {
      assessment.push('Poor paragraph structure')
    }

    // Content richness (0-15 points)
    const hasDialogue = /"[^"]*"/.test(content) || /'[^']*'/.test(content)
    const hasDescriptions = content.includes('was') || content.includes('were') || content.includes('had')
    const hasAction = /(walked|ran|moved|went|came|looked|saw|heard|felt)/.test(content)

    if (hasDialogue && hasDescriptions && hasAction) {
      score += 15
      assessment.push('Rich content with dialogue, descriptions, and action')
    } else if (hasDescriptions && hasAction) {
      score += 10
      assessment.push('Good content with descriptions and action')
    } else if (hasDescriptions || hasAction) {
      score += 5
      assessment.push('Basic content elements present')
    } else {
      assessment.push('Limited content richness')
    }

    // Grammar and readability (0-10 points)
    const hasProperCapitalization = /[A-Z]/.test(content)
    const hasProperPunctuation = /[.!?]/.test(content)
    const averageWordLength = content.split(/\s+/).reduce((sum, word) => sum + word.length, 0) / wordCount

    if (hasProperCapitalization && hasProperPunctuation && averageWordLength > 4) {
      score += 10
      assessment.push('Good grammar and readability')
    } else if (hasProperCapitalization && hasProperPunctuation) {
      score += 7
      assessment.push('Decent grammar')
    } else if (hasProperCapitalization || hasProperPunctuation) {
      score += 4
      assessment.push('Basic grammar')
    } else {
      assessment.push('Poor grammar')
    }

    const isHighQuality = score >= 75 // Threshold for high quality content

    return {
      isHighQuality,
      score,
      wordCount,
      sentences,
      paragraphs,
      assessment: assessment.join(', ')
    }
  }

  /**
   * Execute content refinement - Editor processes existing content with checklist
   */
  async executeContentRefinement(nodeData, pipelineData, progressCallback = null, workflowId = null) {
    const { selectedModels, systemPrompt, userPrompt, temperature, maxTokens, inputInstructions } = nodeData
    
    // Get previous node output (the content to refine)
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    if (!previousOutput || !previousOutput.content) {
      throw new Error('No content available for refinement. Editor node must come after a content generation node.')
    }

    // SMART EDITOR LOGIC: Check if content is already high quality
    const contentQualityCheck = this.assessContentQuality(previousOutput.content)
    
    if (contentQualityCheck.isHighQuality) {
      console.log('‚úÖ Content is already high quality, skipping refinement')
      console.log(`üìä Quality metrics: ${contentQualityCheck.score}/100, Word count: ${contentQualityCheck.wordCount}`)
      
      // Return the content as-is with quality assessment
      return {
        success: true,
        output: {
          content: previousOutput.content,
          refinement: {
            skipped: true,
            reason: 'Content already meets quality standards',
            qualityScore: contentQualityCheck.score,
            wordCount: contentQualityCheck.wordCount,
            assessment: contentQualityCheck.assessment
          },
          metadata: {
            nodeType: 'editor',
            processingTime: 0,
            refinementSkipped: true,
            qualityScore: contentQualityCheck.score
          }
        }
      }
    }
    
    // Build dynamic prompt with real data substitution
    const processedPrompts = this.processPromptVariables({
      systemPrompt,
      userPrompt
    }, pipelineData, nodeData.permissions)

    // Get allData for refinement context
    const { userInput, nodeOutputs, lastNodeOutput } = pipelineData
    
    // Extract user_input from JSON wrapper if available
    let structuredData = userInput
    if (lastNodeOutput?.content?.user_input) {
      structuredData = lastNodeOutput.content.user_input
    } else if (lastNodeOutput?.structuredData) {
      structuredData = lastNodeOutput.structuredData
    }
    
    const allData = { 
      ...userInput, 
      ...structuredData,
      existingContent: previousOutput.content,
      previousNodeOutput: previousOutput
    }

    console.log('üîç Starting content refinement for Editor node')
    console.log('üîç Previous content length:', previousOutput.content?.length || 0)
    
    if (!selectedModels || selectedModels.length === 0) {
      throw new Error('No AI models selected for Editor node. Please configure AI Integration in the node modal.')
    }
    
    // CRITICAL: Check if workflow was stopped before AI call
    if (workflowId && this.isWorkflowStopped(workflowId)) {
      console.log(`üõë WORKFLOW STOPPED BEFORE EDITOR AI CALL`)
      return {
        type: 'stopped_before_ai',
        content: 'Workflow stopped before content refinement could complete.',
        metadata: {
          nodeId: nodeData.id,
          timestamp: new Date(),
          stopped: true
        },
        stopped: true,
        message: 'Workflow stopped before content refinement.'
      }
    }

    // Get AI service for the selected model
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    console.log('üîç Editor parsed model config:', modelConfig)
    const aiServiceInstance = this.getAIService(modelConfig.provider)

    // Update progress with provider info
    if (progressCallback) {
      progressCallback({
        nodeId: nodeData.id,
        nodeName: nodeData.label || 'Content Refinement',
        status: 'executing',
        progress: 50,
        providerName: modelConfig.providerName,
        timestamp: new Date().toLocaleTimeString(),
        cost: 0,
        tokens: 0,
        words: 0,
        aiResponse: null,
        processedContent: null,
        rawData: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          providerName: modelConfig.providerName,
          temperature: temperature || 0.3, // Lower temperature for refinement
          maxTokens: maxTokens || 8000,
          systemPrompt: processedPrompts.systemPrompt,
          userPrompt: processedPrompts.userPrompt,
          inputData: allData,
          refinementMode: true
        }
      })
    }

    if (!aiServiceInstance) {
      throw new Error(`AI service not available for provider: ${modelConfig.provider}`)
    }

    try {
      // Build refinement-specific prompt
      const refinementPrompt = `
${processedPrompts.systemPrompt || 'You are an expert content editor and proofreader. Your role is to refine and improve existing content while maintaining its original intent and structure.'}

CONTENT TO REFINE:
${previousOutput.content}

REFINEMENT CHECKLIST:
- Fix any typos, grammatical errors, or spelling mistakes
- Ensure consistent tone and voice throughout
- Improve clarity and readability where needed
- Check for factual accuracy and logical flow
- Remove any hallucinations or made-up information
- Ensure proper formatting and structure
- Maintain the original content's intent and message
- Apply user-specific requirements (tone, accent, style preferences)

USER REQUIREMENTS:
${Object.entries(allData).filter(([key, value]) => 
  ['tone', 'accent', 'style', 'custom_instructions', 'branding_style'].includes(key) && value
).map(([key, value]) => `- ${key.replace(/_/g, ' ').toUpperCase()}: ${value}`).join('\n')}

${processedPrompts.userPrompt}

INSTRUCTIONS:
1. Read the existing content carefully
2. Identify areas that need improvement based on the checklist
3. Make only necessary corrections and improvements
4. Preserve the original structure and intent
5. Output the refined content with your changes clearly integrated

REFINED CONTENT:`

      // Set SuperAdmin user in aiService
      if (pipelineData.superAdminUser) {
        await aiServiceInstance.setUser(pipelineData.superAdminUser)
      } else {
        throw new Error('SuperAdmin user not provided for AI service')
      }
      
      // Check if provider is available and has API key
      if (!aiServiceInstance.providers[modelConfig.providerName]) {
        throw new Error(`Provider ${modelConfig.providerName} not available. Please check API key configuration.`)
      }

      // Execute AI refinement with timeout
      const timeoutDuration = 300000 // 5 minutes for refinement
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error(`Content refinement timeout after ${timeoutDuration / 60000} minutes`)), timeoutDuration)
      })

      const aiPromise = aiServiceInstance.generateContent({
        prompt: refinementPrompt,
        providerKey: modelConfig.providerName,
        modelId: modelConfig.modelId,
        temperature: temperature || 0.3, // Lower temperature for refinement
        maxTokens: maxTokens || 8000
      })

      const aiResponse = await Promise.race([aiPromise, timeoutPromise])
      const refinedContent = aiResponse.content || aiResponse

      // Calculate metrics
      const actualTokens = aiResponse.tokens || 0
      const actualCost = aiResponse.cost || 0
      const actualWordCount = refinedContent.split(/\s+/).length

      console.log('‚úÖ Content refinement completed')
      console.log(`   - Refined content length: ${refinedContent.length} characters`)
      console.log(`   - Word count: ${actualWordCount} words`)
      console.log(`   - Tokens used: ${actualTokens}`)
      console.log(`   - Cost: $${actualCost}`)

      // Update progress with completion
      if (progressCallback) {
        progressCallback({
          nodeId: nodeData.id,
          nodeName: nodeData.label || 'Content Refinement',
          status: 'completed',
          progress: 100,
          providerName: modelConfig.providerName,
          timestamp: new Date().toLocaleTimeString(),
          cost: actualCost,
          tokens: actualTokens,
          words: actualWordCount,
          output: refinedContent.substring(0, 200) + (refinedContent.length > 200 ? '...' : ''),
          aiResponse: aiResponse,
          processedContent: refinedContent,
          rawData: {
            model: selectedModels[0],
            provider: modelConfig.provider,
            temperature: temperature || 0.3,
            maxTokens: maxTokens || 8000,
            systemPrompt: processedPrompts.systemPrompt,
            userPrompt: processedPrompts.userPrompt,
            inputData: allData,
            originalContentLength: previousOutput.content?.length || 0,
            refinementMode: true
          }
        })
      }

      // Return refined content package
      const refinedDataPackage = {
        type: 'content_refinement',
        content: refinedContent,
        previousNodePassover: pipelineData.previousNodePassover,
        originalContent: previousOutput.content,
        aiMetadata: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          tokens: actualTokens,
          cost: actualCost,
          words: actualWordCount,
          modelCostPer1k: modelConfig.costPer1k,
          refinementMode: true
        },
        inputData: previousOutput,
        instructions: inputInstructions,
        metadata: {
          nodeId: nodeData.id || 'editor-node',
          timestamp: new Date(),
          processingTime: aiResponse.processingTime || 0,
          refinementApplied: true,
          originalLength: previousOutput.content?.length || 0,
          refinedLength: refinedContent.length
        }
      }
      
      console.log('üîó REFINED DATA PACKAGE: Editor output ready for next node')
      console.log('   - Refined content length:', refinedContent?.length || 0)
      console.log('   - Original content preserved:', !!previousOutput.content)
      
      return refinedDataPackage

    } catch (error) {
      throw new Error(`Content refinement failed: ${error.message}`)
    }
  }

  /**
   * Execute preview node - generate preview content for customer approval
   */
  async executePreviewNode(nodeData, pipelineData, progressCallback = null) {
    const { 
      aiEnabled, 
      selectedModels, 
      systemPrompt, 
      userPrompt, 
      maxAttempts = 3,
      previewLength = '1 chapter',
      currentAttempt = 0,
      customerFeedback = '',
      isApproved = false
    } = nodeData

    if (!aiEnabled || !selectedModels || selectedModels.length === 0) {
      throw new Error('Preview node requires AI configuration')
    }

    // Get previous output or user input
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    const { userInput, nodeOutputs } = pipelineData

    // Build dynamic prompts with customer feedback if available
    const processedPrompts = this.processPromptVariables(
      systemPrompt || 'Generate a preview of the content based on the user requirements.',
      userPrompt || 'Create a preview that showcases the writing style and content quality.',
      userInput,
      nodeOutputs,
      previousOutput,
      customerFeedback // Include customer feedback in prompt processing
    )

    // Generate preview content
    const modelConfig = await this.parseModelConfig(selectedModels[0])
    const aiServiceInstance = this.getAIService(modelConfig.provider)
    
    // Set SuperAdmin user in aiService to load API keys from database
    if (pipelineData.superAdminUser) {
      await aiServiceInstance.setUser(pipelineData.superAdminUser)
    } else {
      throw new Error('SuperAdmin user not provided for AI service')
    }
    
    // Check if provider is available and has API key
    if (!aiServiceInstance.providers[modelConfig.providerName]) {
      throw new Error(`Provider ${modelConfig.providerName} not available. Please check API key configuration.`)
    }
    
    const providerConfig = aiServiceInstance.providers[modelConfig.providerName]
    if (!providerConfig.apiKey) {
      throw new Error(`No API key configured for ${modelConfig.providerName}. Please add API key in SuperAdmin settings.`)
    }
    
    // Build final prompt combining system and user prompts
    let finalPrompt = processedPrompts.systemPrompt 
      ? `${processedPrompts.systemPrompt}\n\n${processedPrompts.userPrompt}`
      : processedPrompts.userPrompt
    
    // Make REAL AI API call
    const aiResponse = await aiServiceInstance.generateContent(finalPrompt, modelConfig.providerName, maxTokens || 8000)
    
    const content = aiResponse.content || aiResponse.text || JSON.stringify(aiResponse)
    
    // Calculate metrics
    const actualWordCount = typeof content === 'string' ? content.split(' ').length : 0
    let actualTokens = 0
    let actualCost = 0
    
    if (aiResponse && aiResponse.usage) {
      actualTokens = aiResponse.usage.total_tokens || 
                    aiResponse.usage.completion_tokens || 
                    aiResponse.token_count ||
                    Math.ceil(actualWordCount * 1.3)
      
      const modelCostPer1k = modelConfig.costPer1k || 0.00003
      actualCost = (actualTokens / 1000) * modelCostPer1k
    } else {
      actualTokens = Math.ceil(actualWordCount * 1.3)
      actualCost = (actualTokens / 1000) * (modelConfig.costPer1k || 0.00003)
    }

    // Send progress update
    if (progressCallback) {
      progressCallback({
        nodeId: nodeData.id,
        nodeName: nodeData.label || 'Preview Generation',
        status: 'completed',
        progress: 100,
        providerName: modelConfig.providerName,
        timestamp: new Date().toLocaleTimeString(),
        cost: actualCost,
        tokens: actualTokens,
        words: actualWordCount,
        output: content.substring(0, 200) + (content.length > 200 ? '...' : ''),
        aiResponse: aiResponse,
        processedContent: content,
        rawData: {
          model: selectedModels[0],
          provider: modelConfig.provider,
          systemPrompt: processedPrompts.systemPrompt,
          userPrompt: processedPrompts.userPrompt,
          inputData: previousOutput,
          customerFeedback: customerFeedback,
          currentAttempt: currentAttempt + 1,
          maxAttempts: maxAttempts,
          previewLength: previewLength
        }
      })
    }

    return {
      type: 'preview_generation',
      content: content,
      aiMetadata: {
        model: selectedModels[0],
        provider: modelConfig.provider,
        tokens: actualTokens,
        cost: actualCost,
        words: actualWordCount,
        modelCostPer1k: modelConfig.costPer1k
      },
      inputData: previousOutput,
      instructions: processedPrompts.userPrompt,
      metadata: {
        nodeId: nodeData.id || 'preview-node',
        timestamp: new Date(),
        previewLength: previewLength,
        currentAttempt: currentAttempt + 1,
        maxAttempts: maxAttempts,
        customerFeedback: customerFeedback,
        isApproved: false, // Will be set to true when approved
        totalCharacters: content.length
      },
      structuredData: {
        ...previousOutput?.structuredData,
        previewContent: content,
        previewApproved: false,
        previewAttempt: currentAttempt + 1
      }
    }
  }

  /**
   * Execute condition node - evaluate conditions and route data
   */
  async executeConditionNode(nodeData, pipelineData) {
    const { conditions, aiEnabled } = nodeData
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput

    const evaluationResults = []

    for (const condition of conditions || []) {
      const result = this.evaluateCondition(condition, previousOutput, pipelineData)
      evaluationResults.push(result)

      // Execute action based on condition result
      if (result.passed && condition.trueAction) {
        await this.executeConditionAction(condition.trueAction, pipelineData)
      } else if (!result.passed && condition.falseAction) {
        await this.executeConditionAction(condition.falseAction, pipelineData)
      }
    }

    return {
      type: 'condition_evaluation',
      evaluations: evaluationResults,
      inputData: previousOutput,
      metadata: {
        nodeId: nodeData.id || 'condition-node',
        timestamp: new Date(),
        conditionCount: conditions?.length || 0
      }
    }
  }

  /**
   * Execute output node - format and deliver final results
   */
  async executeOutputNode(nodeData, pipelineData) {
    const { outputFormat, exportFormats, generateCover, includeImages, includeTOC } = nodeData
    const allNodeOutputs = pipelineData.nodeOutputs

    // Compile all content from previous nodes
    const compiledContent = this.compileWorkflowContent(allNodeOutputs, pipelineData.userInput)

    // Get format options from user input (from input node) - FIXED to handle string/array properly
    let userInputFormats = pipelineData.userInput.output_formats || pipelineData.userInput.outputFormats || pipelineData.userInput.exportFormats || []
    const nodeFormats = exportFormats || []
    
    // CRITICAL FIX: Handle case where output_formats is a string instead of array
    if (typeof userInputFormats === 'string') {
      // If it's a comma-separated string like "pdf,docx,epub", split it
      userInputFormats = userInputFormats.split(',').map(f => f.trim())
    } else if (!Array.isArray(userInputFormats)) {
      // If it's not an array, make it one
      userInputFormats = []
    }
    
    // Priority: user input formats > node formats > default format
    const finalExportFormats = userInputFormats.length > 0 ? userInputFormats : 
                                nodeFormats.length > 0 ? nodeFormats : 
                                [outputFormat || 'markdown']
    
    console.log('üéØ User input formats:', userInputFormats)
    console.log('üéØ Final export formats:', finalExportFormats)

    console.log('üéØ Output node format resolution:')
    console.log('  - User input formats:', userInputFormats)
    console.log('  - Node formats:', nodeFormats)
    console.log('  - Final formats:', finalExportFormats)

    // Format according to specified output format
    const formattedOutput = await this.formatFinalOutput(compiledContent, {
      outputFormat,
      exportFormats: finalExportFormats,
      generateCover,
      includeImages,
      includeTOC
    })

    return {
      type: 'final_output',
      content: formattedOutput.content || formattedOutput,
      compiledData: compiledContent,
      deliverables: this.generateDeliverables(formattedOutput, nodeData),
      metadata: {
        nodeId: nodeData.id || 'output-node',
        timestamp: new Date(),
        totalWords: compiledContent.totalWords || 0,
        totalCharacters: compiledContent.totalCharacters || 0,
        nodeCount: compiledContent.metadata?.nodeCount || 0,
        wordCountByNode: compiledContent.metadata?.wordCountByNode || {},
        characterCountByNode: compiledContent.metadata?.characterCountByNode || {},
        formats: finalExportFormats,
        allFormats: formattedOutput.allFormats || {},
        primaryFormat: formattedOutput.primaryFormat || finalExportFormats[0],
        requestedFormats: formattedOutput.requestedFormats || finalExportFormats,
        generationStats: {
          totalSections: compiledContent.sections.length,
          averageWordsPerSection: compiledContent.sections.length > 0 ? Math.round(compiledContent.totalWords / compiledContent.sections.length) : 0,
          averageCharactersPerSection: compiledContent.sections.length > 0 ? Math.round(compiledContent.totalCharacters / compiledContent.sections.length) : 0
        }
      }
    }
  }

  /**
   * Helper methods for execution
   */
  
  buildExecutionOrder(nodes, edges) {
    // Build dependency graph and return topologically sorted execution order
    const nodeMap = new Map(nodes.map(node => [node.id, node]))
    const incomingEdges = new Map()
    const outgoingEdges = new Map()

    // Initialize maps
    nodes.forEach(node => {
      incomingEdges.set(node.id, [])
      outgoingEdges.set(node.id, [])
    })

    // Build edge maps
    edges.forEach(edge => {
      outgoingEdges.get(edge.source).push(edge.target)
      incomingEdges.get(edge.target).push(edge.source)
    })

    // Topological sort
    const visited = new Set()
    const result = []

    const visit = (nodeId) => {
      if (visited.has(nodeId)) return
      visited.add(nodeId)
      
      // Visit dependencies first
      incomingEdges.get(nodeId).forEach(depId => visit(depId))
      
      result.push(nodeMap.get(nodeId))
    }

    // Start with nodes that have no incoming edges (input nodes)
    nodes.forEach(node => {
      if (incomingEdges.get(node.id).length === 0) {
        visit(node.id)
      }
    })

    // Visit remaining nodes
    nodes.forEach(node => visit(node.id))

    return result
  }

  validateInputFields(userInput, inputFields) {
    const errors = []
    
    console.log('üîç Input validation debug:')
    console.log('  - User input keys:', Object.keys(userInput))
    console.log('  - User input values:', userInput)
    console.log('  - Input fields:', inputFields?.map(f => ({ name: f.name, variable: f.variable, required: f.required })))
    
    inputFields?.forEach(field => {
      // Check both field.name and field.variable for compatibility with different flow types
      const fieldValue = userInput[field.name] || userInput[field.variable]
      console.log(`  - Checking field "${field.name}" (variable: "${field.variable}"):`, fieldValue, '(required:', field.required, ')')
      
      if (field.required && (!fieldValue || fieldValue === '')) {
        errors.push(`${field.name} is required`)
      }
    })

    console.log('  - Validation errors:', errors)
    return {
      isValid: errors.length === 0,
      errors
    }
  }

  async structureInputData(userInput, inputFields) {
    const structured = {}
    
    console.log('üîç STRUCTURE INPUT DATA DEBUG:')
    console.log('  - userInput:', userInput)
    console.log('  - inputFields:', inputFields)
    
    for (const field of inputFields || []) {
      console.log(`  - Processing field: ${field.name} (variable: ${field.variable || field.name})`)
      console.log(`    - userInput[${field.name}]:`, userInput[field.name])
      
      if (userInput[field.name] !== undefined) {
        // Handle file uploads specially
        if (field.type === 'file' && userInput[field.name]) {
          try {
            const uploadedUrl = await this.uploadFileToSupabase(userInput[field.name], field.name)
            structured[field.variable || field.name] = uploadedUrl
            console.log(`    - File uploaded: ${uploadedUrl}`)
          } catch (error) {
            console.error(`Failed to upload file for ${field.name}:`, error)
            structured[field.variable || field.name] = null
          }
        } else {
          structured[field.variable || field.name] = userInput[field.name]
          console.log(`    - Structured value: ${structured[field.variable || field.name]}`)
        }
      } else {
        console.log(`    - Field ${field.name} not found in userInput`)
      }
    }

    console.log('  - Final structured data:', structured)
    return structured
  }

  async uploadFileToSupabase(file, fieldName) {
    try {
      // Generate unique filename
      const timestamp = Date.now()
      const sanitizedName = fieldName.replace(/[^a-zA-Z0-9]/g, '_')
      const fileExtension = file.name.split('.').pop()
      const fileName = `${sanitizedName}_${timestamp}.${fileExtension}`
      
      // Upload to Supabase storage
      const { data, error } = await getSupabase().storage
        .from('workflow-assets')
        .upload(`covers/${fileName}`, file, {
          cacheControl: '3600',
          upsert: false
        })

      if (error) throw error

      // Get public URL
      const { data: urlData } = getSupabase().storage
        .from('workflow-assets')
        .getPublicUrl(`covers/${fileName}`)

      return urlData.publicUrl
    } catch (error) {
      console.error('File upload error:', error)
      throw new Error(`Failed to upload file: ${error.message}`)
    }
  }

  identifyMissingOptionals(userInput, inputFields) {
    const missing = []
    
    inputFields?.forEach(field => {
      if (!field.required && (!userInput[field.name] || userInput[field.name] === '')) {
        missing.push({
          field: field.name,
          variable: field.variable,
          type: field.type,
          placeholder: field.placeholder
        })
      }
    })

    return missing
  }

  createNextNodeInstructions(missingOptionals, baseInstructions) {
    if (missingOptionals.length === 0) {
      return baseInstructions
    }

    const missingFieldGuidance = missingOptionals.map(field => 
      `- ${field.field}: Not provided, use intelligent defaults or skip if not essential`
    ).join('\n')

    return `${baseInstructions}\n\nMISSING OPTIONAL FIELDS GUIDANCE:\n${missingFieldGuidance}\n\nAdjust processing accordingly to handle missing information gracefully.`
  }

  processPromptVariables(prompts, pipelineData, nodePermissions = null) {
    const { userInput, nodeOutputs, lastNodeOutput } = pipelineData
    
    // CRITICAL: Extract user_input from JSON wrapper if available, otherwise fall back to userInput
    let structuredData = userInput
    if (lastNodeOutput?.content?.user_input) {
      structuredData = lastNodeOutput.content.user_input
    } else if (lastNodeOutput?.structuredData) {
      structuredData = lastNodeOutput.structuredData
    }
    
    // FIXED: NO DUPLICATE VARIABLES - Clean data structure
    const allData = { 
      // Primary data source (no duplicates)
      ...structuredData,
      // Chapter context (if available)
      ...(pipelineData.currentChapter && { currentChapter: pipelineData.currentChapter }),
      ...(pipelineData.totalChapters && { totalChapters: pipelineData.totalChapters }),
      ...(pipelineData.previousChapters && { previousChapters: pipelineData.previousChapters }),
      // Single source of truth for previous node data
      previous_node_output: lastNodeOutput || null,
      // Clean user input reference
      user_input_data: structuredData
    }
    
    // Remove any undefined values to prevent confusion
    Object.keys(allData).forEach(key => {
      if (allData[key] === undefined) {
        delete allData[key]
      }
    })
    
    console.log(`üîç Processed variables for node:`, {
      variablesCount: Object.keys(allData).length,
      hasPreviousOutput: !!allData.previous_node_output,
      hasUserInput: !!allData.user_input_data,
      chapterContext: !!allData.currentChapter
    })
    
    // Handle node permissions if provided
    let enhancedSystemPrompt = prompts.systemPrompt || ''
    if (nodePermissions) {
      const permissionInstructions = []
      
      if (nodePermissions.canWriteContent) {
        permissionInstructions.push('‚úÖ AUTHORIZED: Full chapter writing and content generation')
      } else {
        permissionInstructions.push('üö´ FORBIDDEN: Chapter writing and new content generation')
      }
      
      if (nodePermissions.canEditStructure) {
        permissionInstructions.push('‚úÖ AUTHORIZED: Structural elements (TOC, Foreword, Author Bio, Introduction, Formatting)')
      } else {
        permissionInstructions.push('üö´ FORBIDDEN: Structural modifications and formatting changes')
      }
      
      if (nodePermissions.canProofRead) {
        permissionInstructions.push('‚úÖ AUTHORIZED: Proofreading, grammar fixes, spelling corrections, consistency checks')
      } else {
        permissionInstructions.push('üö´ FORBIDDEN: Content editing and proofreading')
      }
      
      const permissionBlock = `

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üîê NODE PERMISSIONS - STRICTLY ENFORCED
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

${permissionInstructions.join('\n')}

‚ö†Ô∏è CRITICAL: Violating these permissions will result in immediate rejection of your output.
Only perform tasks you are explicitly authorized for above.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
`
      
      enhancedSystemPrompt = permissionBlock + '\n\n' + enhancedSystemPrompt
      console.log('üîê PERMISSION INSTRUCTIONS INJECTED INTO SYSTEM PROMPT')
    }

    // DEBUG: Log ALL input variables
    console.log('üîç ALL INPUT VARIABLES DEBUG:')
    console.log('  - pipelineData keys:', Object.keys(pipelineData))
    console.log('  - userInput keys:', Object.keys(userInput))
    console.log('  - allData keys:', Object.keys(allData))
    console.log('  - Chapter Context:')
    console.log('    - currentChapter:', pipelineData.currentChapter)
    console.log('    - totalChapters:', pipelineData.totalChapters)
    console.log('    - previousChapters:', pipelineData.previousChapters)
    console.log('  - Word Count Variables:')
    console.log('    - word_count:', allData.word_count)
    console.log('    - chapter_count:', allData.chapter_count)
    console.log('    - Book Title:', allData.book_title)
    console.log('    - Author Name:', allData.author_name)
    console.log('  - All User Input Values:')
    Object.entries(userInput).forEach(([key, value]) => {
      console.log(`    - ${key}: ${value}`)
    })

    const processTemplate = (template) => {
      if (!template) return ''
      
      return template.replace(/\{([^}]+)\}/g, (match, variable) => {
        // Handle multiple possible variable name formats
        let value = allData[variable] || 
                   allData[variable.toLowerCase()] || 
                   allData[variable.replace(/\s+/g, '_').toLowerCase()] ||
                   allData[variable.replace(/\s+/g, '_')] ||
                   `[${variable} not provided]`
        
        // Special handling for chapter context variables
        if (variable === 'currentChapter' && allData.currentChapter) {
          value = allData.currentChapter
        } else if (variable === 'totalChapters' && allData.totalChapters) {
          value = allData.totalChapters
        } else if (variable === 'previousChapters' && allData.previousChapters) {
          // Format previous chapters context for AI
          const previousChapters = allData.previousChapters
          if (Array.isArray(previousChapters) && previousChapters.length > 0) {
            value = previousChapters.map((chapter, index) => {
              const chapterNum = chapter.chapter || (index + 1)
              const content = chapter.content || ''
              const summary = content.length > 200 ? content.substring(0, 200) + '...' : content
              return `Chapter ${chapterNum}: ${summary}`
            }).join('\n\n')
          } else {
            value = 'No previous chapters available'
          }
        } else if (variable === 'previousChapterSummary' && allData.previousChapters) {
          // Create a concise summary of previous chapters
          const previousChapters = allData.previousChapters
          if (Array.isArray(previousChapters) && previousChapters.length > 0) {
            const lastChapter = previousChapters[previousChapters.length - 1]
            const chapterNum = lastChapter.chapter || previousChapters.length
            const content = lastChapter.content || ''
            const summary = content.length > 150 ? content.substring(0, 150) + '...' : content
            value = `In Chapter ${chapterNum}, ${summary}`
          } else {
            value = 'This is the first chapter'
          }
        }
        
        // Special handling for common variable mappings
        if (value === `[${variable} not provided]`) {
          const mappings = {
            'Book Title': 'book_title',
            'Author Name': 'author_name', 
            'Author Bio': 'author_bio',
            'Word Count': 'word_count',
            'Chapter Count': 'chapter_count',
            'Tone': 'tone',
            'Accent': 'accent',
            'Target Audience': 'target_audience',
            'Industry Focus': 'industry_focus',
            'Custom Instructions': 'custom_instructions',
            'Include Case Studies': 'include_case_studies',
            'Include Templates': 'include_templates',
            'Include Worksheets': 'include_worksheets',
            'Research Level': 'research_level',
            'Practical Applications': 'practical_applications',
            'Content Depth': 'content_depth',
            'Publishing Format': 'publishing_format',
            'Business Model': 'business_model',
            'Typography Style': 'typography_style'
          }
          
          const mappedKey = mappings[variable]
          if (mappedKey && allData[mappedKey]) {
            value = allData[mappedKey]
          }
        }
        
        console.log(`üîç Variable ${variable}: ${value}`)
        return value
      })
    }

    // APPLY ACCENT-SPECIFIC INSTRUCTIONS TO ALL PROMPTS
    const accent = allData.accent || allData['Language Accent'] || 'neutral'
    const tone = allData.tone || allData.Tone || 'professional'
    
    let baseUserPrompt = processTemplate(prompts.userPrompt)
    
    // ENHANCE PROMPT WITH CHAPTER CONTEXT FOR MULTI-CHAPTER GENERATION
    if (allData.currentChapter && allData.totalChapters && allData.totalChapters > 1) {
      const chapterContext = this.generateChapterContext(allData)
      if (chapterContext) {
        baseUserPrompt = chapterContext + '\n\n' + baseUserPrompt
        console.log(`üìö CHAPTER CONTEXT ENHANCED PROMPT:`, chapterContext.substring(0, 200) + '...')
      }
    }
    
    // ENHANCE PROMPT WITH ACCENT INSTRUCTIONS
    if (accent && accent !== 'neutral') {
      baseUserPrompt = accentInstructionService.buildAccentSpecificPrompt(baseUserPrompt, accent, tone)
      console.log(`üéØ ACCENT ENHANCED PROMPT for ${accent}:`, baseUserPrompt.substring(0, 200) + '...')
    }

    // ENHANCE PROMPT WITH TYPOGRAPHY INSTRUCTIONS
    if (allData.typography_combo) {
      const typographyInstructions = typographyService.generateTypographyInstructions(allData.typography_combo)
      baseUserPrompt = baseUserPrompt + '\n\n' + typographyInstructions
      console.log(`üé® TYPOGRAPHY ENHANCED PROMPT for ${allData.typography_combo}:`, typographyInstructions.substring(0, 200) + '...')
    }

    const processedPrompts = {
      systemPrompt: processTemplate(enhancedSystemPrompt),
      userPrompt: baseUserPrompt
    }

    console.log('üîç PROCESSED SYSTEM PROMPT:', processedPrompts.systemPrompt.substring(0, 500) + '...')
    console.log('üîç PROCESSED USER PROMPT:', processedPrompts.userPrompt.substring(0, 500) + '...')

    return processedPrompts
  }

  /**
   * Generate chapter-specific context for multi-chapter generation
   */
  generateChapterContext(allData) {
    const currentChapter = allData.currentChapter
    const totalChapters = allData.totalChapters
    const previousChapters = allData.previousChapters
    
    if (!currentChapter || !totalChapters) return null
    
    let context = `üìñ CHAPTER ${currentChapter} OF ${totalChapters}\n\n`
    
    if (currentChapter === 1) {
      context += `This is the opening chapter. Establish the story world, introduce main characters, and set the initial situation. Create a compelling beginning that hooks the reader.\n\n`
    } else if (currentChapter === totalChapters) {
      context += `This is the final chapter. Provide a satisfying conclusion that resolves the main plot threads and character arcs. Ensure all loose ends are tied up.\n\n`
    } else {
      context += `This is a middle chapter. Advance the plot from where the previous chapter ended, develop character relationships, and build toward the climax.\n\n`
    }
    
    // Add previous chapter context if available
    if (previousChapters && Array.isArray(previousChapters) && previousChapters.length > 0) {
      context += `BUILD UPON PREVIOUS CHAPTERS:\n`
      previousChapters.forEach((chapter, index) => {
        const chapterNum = chapter.chapter || (index + 1)
        const content = chapter.content || ''
        const summary = content.length > 100 ? content.substring(0, 100) + '...' : content
        context += `- Chapter ${chapterNum}: ${summary}\n`
      })
      context += `\nIMPORTANT: Do not repeat the same scenes, descriptions, or dialogue from previous chapters. Each chapter should advance the story forward with new developments, character growth, and plot progression.\n\n`
    }
    
    context += `WRITING GUIDELINES:\n`
    context += `- Maintain consistency with the established world and characters\n`
    context += `- Each chapter should have its own narrative arc while contributing to the overall story\n`
    context += `- Avoid repetitive descriptions or similar scene setups\n`
    context += `- Build tension and character development progressively\n\n`
    
    return context
  }

  async parseModelConfig(modelString) {
    if (!modelString) {
      throw new Error('No model selected for AI generation')
    }
    
    // Handle both old format (OPENA-01-first) and new format (providerName:modelId)
    let providerName, modelId
    
    if (modelString.includes(':')) {
      // New format: providerName:modelId
      [providerName, modelId] = modelString.split(':')
    } else {
      // Old format: OPENA-01-first - extract provider name from the string
      providerName = modelString
      modelId = 'default' // Use default model for old format
      
      console.log('üîç Using legacy model format:', modelString, '-> provider:', providerName, 'model:', modelId)
    }
    
    if (!providerName) {
      throw new Error(`Invalid model format: ${modelString}. Expected format: providerName:modelId or legacy format`)
    }
    
    // Query database to get actual provider type - NO HARDCODED MAPPING
    const { data: providerData, error } = await supabase
      .from('ai_providers')
      .select('provider')
      .eq('name', providerName)
      .single()
    
    if (error || !providerData) {
      console.warn(`Provider ${providerName} not found in database, trying legacy mapping...`)
      
      // Legacy fallback - map old format to provider type
      let legacyProviderType = 'openai' // default
      if (providerName.startsWith('OPENA')) legacyProviderType = 'openai'
      else if (providerName.startsWith('MISTR')) legacyProviderType = 'mistral'
      else if (providerName.startsWith('GEMIN')) legacyProviderType = 'gemini'
      else if (providerName.startsWith('CLAUD')) legacyProviderType = 'claude'
      else if (providerName.startsWith('PERPL')) legacyProviderType = 'perplexity'
      else if (providerName.startsWith('GROK')) legacyProviderType = 'grok'
      else if (providerName.startsWith('COHER')) legacyProviderType = 'cohere'
      
      console.log(`üîç Using legacy provider mapping: ${providerName} -> ${legacyProviderType}`)
      
      // Create a mock provider data object for legacy format
      const mockProviderData = { provider: legacyProviderType }
      return { 
        provider: legacyProviderType, 
        model: modelId, 
        providerName,
        costPer1k: 0.00003 // Default cost
      }
    }
    
    // Query database to get model cost data
    const { data: modelData, error: modelError } = await supabase
      .from('ai_model_metadata')
      .select('input_cost_per_million')
      .eq('key_name', providerName)
      .single()
    
    const costPer1k = modelData?.input_cost_per_million ? (modelData.input_cost_per_million / 1000) : 0.00003 // Convert from per million to per 1k tokens
    
    return { 
      provider: providerData.provider, 
      model: modelId, 
      providerName,
      costPer1k: costPer1k
    }
  }

  getAIService(provider) {
    return this.aiServices[provider] || this.aiServices['ai']
  }

  evaluateCondition(condition, data, pipelineData) {
    const { field, operator, value } = condition
    // Extract field value from JSON wrapper or structuredData
    let fieldValue = pipelineData.userInput[field]
    if (data?.content?.user_input?.[field]) {
      fieldValue = data.content.user_input[field]
    } else if (data?.structuredData?.[field]) {
      fieldValue = data.structuredData[field]
    }

    let passed = false

    switch (operator) {
      case 'equals':
        passed = fieldValue === value || fieldValue === (value === 'true')
        break
      case 'not_equals':
        passed = fieldValue !== value
        break
      case 'contains':
        passed = String(fieldValue).toLowerCase().includes(String(value).toLowerCase())
        break
      case 'greater_than':
        passed = Number(fieldValue) > Number(value)
        break
      case 'less_than':
        passed = Number(fieldValue) < Number(value)
        break
      default:
        passed = false
    }

    return {
      condition,
      fieldValue,
      passed,
      timestamp: new Date()
    }
  }

  async executeConditionAction(action, pipelineData) {
    switch (action.type) {
      case 'generate_content':
        // Execute AI generation for condition-based content
        if (action.prompt) {
          const processedPrompt = this.processPromptVariables({ userPrompt: action.prompt }, pipelineData)
          // Add to pipeline for next node
          pipelineData.conditionGeneration = processedPrompt.userPrompt
        }
        break
      case 'skip_to':
        // Mark node to skip to
        pipelineData.skipToNode = action.target
        break
      case 'continue':
        // Continue normal flow
        break
    }
  }

  executeNonAIProcessing(nodeData, pipelineData) {
    // Handle non-AI process nodes (data transformation, validation, etc.)
    const previousOutput = pipelineData.lastNodeOutput || pipelineData.userInput
    
    return {
      type: 'data_processing',
      processedData: previousOutput,
      instructions: nodeData.inputInstructions,
      metadata: {
        nodeId: nodeData.id || 'process-node',
        timestamp: new Date(),
        aiEnabled: false
      }
    }
  }

  compileWorkflowContent(nodeOutputs, userInput) {
    const content = {
      userInput,
      generatedContent: {},
      totalWords: 0,
      totalCharacters: 0,
      sections: [],
      metadata: {
        nodeCount: 0,
        wordCountByNode: {},
        characterCountByNode: {}
      }
    }

    Object.entries(nodeOutputs).forEach(([nodeId, output]) => {
      // FIXED: Include all content generation types, not just 'ai_generation'
      if ((output.type === 'ai_generation' || output.type === 'multi_chapter_generation' || output.type === 'process') && output.content) {
        
        // Handle multi-chapter generation (array content)
        if (output.type === 'multi_chapter_generation' && Array.isArray(output.content)) {
          console.log(`üìö Processing multi-chapter content from ${nodeId}: ${output.content.length} chapters`)
          
          // Process each chapter in the array
          output.content.forEach((chapter, index) => {
            if (chapter.content && typeof chapter.content === 'string') {
              const wordCount = chapter.content.split(/\s+/).filter(word => word.length > 0).length
              const charCount = chapter.content.length
              
              content.totalWords += wordCount
              content.totalCharacters += charCount
              content.metadata.wordCountByNode[`${nodeId}_chapter_${chapter.chapter || index + 1}`] = wordCount
              content.metadata.characterCountByNode[`${nodeId}_chapter_${chapter.chapter || index + 1}`] = charCount
              content.metadata.nodeCount++
              
              console.log(`üìä Node ${nodeId} Chapter ${chapter.chapter || index + 1} word count: ${wordCount} words, ${charCount} characters`)
            }
          })
          
          // Store the complete multi-chapter content
          content.generatedContent[nodeId] = output.content
          content.sections.push({
            nodeId,
            content: output.content,
            metadata: output.metadata,
            contentType: 'multi_chapter'
          })
        }
        // Handle single content generation (string content)
        else if (typeof output.content === 'string') {
          const wordCount = output.content.split(/\s+/).filter(word => word.length > 0).length
          const charCount = output.content.length
          
          content.totalWords += wordCount
          content.totalCharacters += charCount
          content.metadata.wordCountByNode[nodeId] = wordCount
          content.metadata.characterCountByNode[nodeId] = charCount
          content.metadata.nodeCount++
          
          console.log(`üìä Node ${nodeId} word count: ${wordCount} words, ${charCount} characters`)
          
          // Store single content
          content.generatedContent[nodeId] = output.content
          content.sections.push({
            nodeId,
            content: output.content,
            metadata: output.metadata,
            contentType: 'single_content'
          })
        }
        // Handle other content types (objects, etc.)
        else {
          console.log(`üìä Node ${nodeId} content type: ${typeof output.content}, processing as-is`)
          
          content.generatedContent[nodeId] = output.content
          content.sections.push({
            nodeId,
            content: output.content,
            metadata: output.metadata,
            contentType: typeof output.content
          })
        }
      }
    })

    console.log(`üìä Total compiled content: ${content.totalWords} words, ${content.totalCharacters} characters`)
    return content
  }

  async determineOptimalChapterCount(wordCount, storyPremise, structuredData, pipelineData = null) {
    try {
      // Let AI determine optimal chapter count based on story premise and word count
      const prompt = `You are a professional book editor. Determine the optimal number of chapters for this content:

Word Count: ${wordCount}
Story Premise: ${storyPremise}
Content Type: ${structuredData.reference_type || 'general content'}
Subject Area: ${structuredData.subject_area || 'general'}

Chapter Distribution Guidelines:
- For ${wordCount} words, aim for ${Math.ceil(wordCount / 1000)}-${Math.ceil(wordCount / 800)} chapters
- Each chapter should be 800-1200 words for optimal reading experience
- Consider natural story arcs, plot points, and character development
- Ensure each chapter has a clear beginning, middle, and end
- Balance pacing between action and character development

Examples:
- 5000 words = 5-6 chapters (800-1000 words each)
- 3000 words = 3-4 chapters (750-1000 words each)
- 2000 words = 2-3 chapters (650-1000 words each)

Respond with ONLY a single number (e.g., "5" or "6"). No explanation needed.`

      // Use the existing AI service to determine chapter count
      const modelConfig = await this.parseModelConfig('OPENA-01-first') // Use default model
      const aiServiceInstance = this.getAIService(modelConfig.provider)
      
      // Set SuperAdmin user if available
      if (pipelineData?.superAdminUser) {
        await aiServiceInstance.setUser(pipelineData.superAdminUser)
      }
      
      const response = await aiServiceInstance.generateContent({
        prompt: prompt,
        providerKey: modelConfig.providerName,
        modelId: modelConfig.modelId,
        temperature: 0.3,
        maxTokens: 10
      })

      const content = response.content || response.text || JSON.stringify(response)
      const aiChapterCount = parseInt(content.trim())
      
      // Validate AI response
      if (aiChapterCount && aiChapterCount >= 1 && aiChapterCount <= 20) {
        console.log(`‚úÖ AI determined optimal chapter count: ${aiChapterCount}`)
        return aiChapterCount
      } else {
        // Enhanced fallback calculation for better chapter distribution
        let fallbackCount
        if (wordCount >= 5000) {
          // For longer content, aim for 800-1200 words per chapter
          fallbackCount = Math.max(4, Math.min(8, Math.ceil(wordCount / 1000)))
        } else if (wordCount >= 2000) {
          // For medium content, aim for 500-800 words per chapter
          fallbackCount = Math.max(3, Math.min(6, Math.ceil(wordCount / 600)))
        } else {
          // For shorter content, aim for 400-600 words per chapter
          fallbackCount = Math.max(2, Math.min(4, Math.ceil(wordCount / 500)))
        }
        console.log(`‚ö†Ô∏è AI gave invalid response, using enhanced fallback: ${fallbackCount} (${wordCount} words)`)
        return fallbackCount
      }
    } catch (error) {
      console.error('‚ùå Error determining chapter count:', error)
      // Fallback to reasonable default
      const fallbackCount = Math.max(3, Math.min(10, Math.ceil(wordCount / 2000)))
      console.log(`‚ö†Ô∏è Using fallback chapter count: ${fallbackCount}`)
      return fallbackCount
    }
  }

  async formatFinalOutput(compiledContent, formatOptions) {
    const { outputFormat = 'markdown', exportFormats = [] } = formatOptions
    
    // If exportFormats is provided (from input node), use those instead of single outputFormat
    let formatsToGenerate = exportFormats.length > 0 ? exportFormats : [outputFormat]
    
    // ALWAYS generate HTML for preview - add it if not already present
    if (!formatsToGenerate.includes('html') && !formatsToGenerate.includes('HTML')) {
      formatsToGenerate = ['html', ...formatsToGenerate]
      console.log('üìÑ Added HTML format for preview')
    }
    
    console.log('üéØ Formatting output for formats:', formatsToGenerate)
    console.log('üéØ Format types:', formatsToGenerate.map(f => typeof f))
    console.log('üìä Compiled content:', compiledContent)
    
    const formattedOutputs = {}
    
    // Generate content for each requested format using PROFESSIONAL FORMATTER
    for (const format of formatsToGenerate) {
      // CRITICAL FIX: Ensure format is a string, not a character
      const formatStr = typeof format === 'string' ? format : String(format)
      console.log(`üéØ Processing format: "${formatStr}" (type: ${typeof format})`)
      
      // Skip if format is empty or just a single character (likely from string splitting bug)
      if (!formatStr || formatStr.length < 2) {
        console.log(`‚ö†Ô∏è Skipping invalid format: "${formatStr}"`)
        continue
      }
      try {
        // Use PROFESSIONAL FORMATTER for all formats
        const userInput = compiledContent.userInput || {}
        
        // Extract the actual book content from the compiled workflow data
        let bookContent = ''
        if (compiledContent.generatedContent) {
          // Get the main content from the last process node that generated book content
          const contentNodes = Object.values(compiledContent.generatedContent)
          bookContent = contentNodes[contentNodes.length - 1] || ''
          
          // CRITICAL FIX: Clean multi-chapter content before formatting
          if (typeof bookContent === 'string' && bookContent.includes('Chapter')) {
            // This is multi-chapter content - clean it properly
            bookContent = bookContent
              .replace(/<[^>]*>/g, '') // Remove HTML tags
              .replace(/font-family[^;]*;?/g, '') // Remove font-family CSS
              .replace(/font-weight[^;]*;?/g, '') // Remove font-weight CSS
              .replace(/font-size[^;]*;?/g, '') // Remove font-size CSS
              .replace(/margin[^;]*;?/g, '') // Remove margin CSS
              .replace(/line-height[^;]*;?/g, '') // Remove line-height CSS
              .replace(/"[^"]*"/g, '') // Remove quoted CSS values
              .replace(/\n{3,}/g, '\n\n') // Clean excessive newlines
              .trim()
          }
        } else {
          bookContent = compiledContent.content || compiledContent
        }
        
        console.log(`üìö Generating ${formatStr} format`)
        console.log(`üìö Book content length:`, typeof bookContent === 'string' ? bookContent.length : 'Not a string')
        console.log(`üìö User input:`, Object.keys(userInput))
        
        // Use appropriate service based on format type
        if (['pdf', 'docx', 'epub'].includes(formatStr.toLowerCase())) {
          // Use exportService for binary formats
          console.log(`üìö Using exportService for binary format: ${formatStr}`)
          
          // Pass compiled sections directly - NO TRANSFORMATION
          const compiledData = {
            userInput: userInput,
            sections: compiledContent.sections || [],
            totalWords: compiledContent.totalWords || 0,
            metadata: compiledContent.metadata || {},
            content: bookContent  // Include raw content as fallback
          }
          
          console.log(`üìö Passing to ${formatStr.toUpperCase()}:`, {
            sections: compiledData.sections.length,
            hasUserInput: !!compiledData.userInput,
            totalWords: compiledData.totalWords,
            typography: {
              font_family: userInput.font_family,
              typography_combo: userInput.typography_combo
            }
          })
          
          switch (formatStr.toLowerCase()) {
            case 'pdf':
              formattedOutputs[formatStr] = await exportService.generatePDF(compiledData)
              break
            case 'docx':
              console.log('üîß Calling exportService.generateDOCX with:', compiledData)
              formattedOutputs[formatStr] = await exportService.generateDOCX(compiledData)
              console.log('üîß DOCX generation completed successfully')
              break
            case 'epub':
              formattedOutputs[formatStr] = await exportService.generateEPUB(compiledData)
              break
          }
        } else {
          // Use professionalBookFormatter for text formats with user preferences
          console.log(`üìö Using professionalBookFormatter for text format: ${formatStr}`)
          
          // Extract user typography preferences
          const typographyPrefs = {
            fontFamily: userInput.typography_combo || userInput.font_family || 'Georgia, serif',
            writingStyle: userInput.writing_style || 'descriptive',
            tone: userInput.tone || 'professional',
            format: formatStr
          }
          
          formattedOutputs[formatStr] = await professionalBookFormatter.formatCompleteBook(
            bookContent, 
            userInput, 
            formatStr,
            typographyPrefs
          )
        }
        
        const contentLength = typeof formattedOutputs[formatStr] === 'string' ? 
          formattedOutputs[formatStr].length : 
          JSON.stringify(formattedOutputs[formatStr]).length
        
        console.log(`‚úÖ Generated PROFESSIONAL ${formatStr} format (${contentLength} chars)`)
      } catch (error) {
        console.error(`‚ùå Error generating ${formatStr} format:`, error)
        // NO FALLBACK - FAIL FAST AND REPORT ERROR
        throw new Error(`Professional formatter failed for ${formatStr}: ${error.message}`)
      }
    }
    
    // ALWAYS use text/markdown for preview display, NEVER blob URLs
    const previewFormat = formattedOutputs['text'] || formattedOutputs['markdown'] || formattedOutputs['html'] || formattedOutputs['HTML']
    const primaryFormat = 'text'  // Always text for display
    
    console.log('üìÑ Preview format content type:', typeof previewFormat)
    console.log('üìÑ All formats generated:', Object.keys(formattedOutputs))
    
    return {
      content: previewFormat,  // Text/Markdown for display
      allFormats: formattedOutputs,
      requestedFormats: formatsToGenerate,
      primaryFormat: primaryFormat
    }
  }

  generateMarkdownOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const totalWords = compiledContent.totalWords || 0
    
    let markdown = `# ${title}\n\n`
    markdown += `**Author:** ${author}\n\n`
    markdown += `**Generated:** ${new Date().toLocaleDateString()}\n\n`
    markdown += `**Total Words:** ${totalWords.toLocaleString()}\n\n`
    markdown += `---\n\n`
    
    // Add table of contents if multiple sections
    if (compiledContent.sections.length > 1) {
      markdown += `## Table of Contents\n\n`
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        markdown += `${index + 1}. [${sectionTitle}](#${sectionTitle.toLowerCase().replace(/\s+/g, '-')})\n`
      })
      markdown += `\n---\n\n`
    }
    
    // Add content sections with clean formatting
    compiledContent.sections.forEach((section, index) => {
      const sectionTitle = section.title || `Chapter ${index + 1}`
      markdown += `## ${sectionTitle}\n\n`
      
      // Clean up the content - preserve paragraph structure and format properly
      let cleanContent = section.content
        .replace(/\n{4,}/g, '\n\n\n') // Replace excessive newlines with triple newlines (preserve paragraph breaks)
        .replace(/^\s+|\s+$/gm, '') // Trim whitespace from each line
        .replace(/[ \t]{2,}/g, ' ') // Replace multiple spaces/tabs with single space (preserve newlines)
        .replace(/([.!?])\s*([A-Z])/g, '$1\n\n$2') // Add paragraph breaks after sentences
        .replace(/([.!?])\s*\n\s*([A-Z])/g, '$1\n\n$2') // Ensure proper paragraph spacing
      
      markdown += `${cleanContent}\n\n`
      
      // Add page break between chapters (except for the last one)
      if (index < compiledContent.sections.length - 1) {
        markdown += `---\n\n`
      }
    })
    
    return markdown
  }

  generateHTMLOutput(compiledContent) {
    // Generate clean, professional HTML format
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const totalWords = compiledContent.totalWords || 0
    
    let html = `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>${title}</title>
    <style>
        body { 
            font-family: 'Georgia', 'Times New Roman', serif; 
            line-height: 1.6; 
            margin: 0; 
            padding: 40px; 
            max-width: 800px; 
            margin: 0 auto; 
            background: #fff;
            color: #333;
        }
        h1 { 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db; 
            padding-bottom: 15px; 
            text-align: center;
            margin-bottom: 30px;
        }
        h2 { 
            color: #34495e; 
            margin-top: 40px; 
            margin-bottom: 20px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        .metadata { 
            background: #f8f9fa; 
            padding: 20px; 
            border-radius: 8px; 
            margin-bottom: 30px; 
            border-left: 4px solid #3498db;
        }
        .section { 
            margin-bottom: 40px; 
            padding: 20px;
            background: #fafafa;
            border-radius: 5px;
        }
        .toc {
            background: #e8f4f8;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        .toc h2 {
            margin-top: 0;
            color: #2c3e50;
        }
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        .toc li {
            padding: 5px 0;
            border-bottom: 1px solid #ddd;
        }
        .toc li:last-child {
            border-bottom: none;
        }
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        hr {
            border: none;
            border-top: 2px solid #ecf0f1;
            margin: 30px 0;
        }
    </style>
</head>
<body>
    <h1>${title}</h1>
    <div class="metadata">
        <p><strong>Author:</strong> ${author}</p>
        <p><strong>Generated:</strong> ${new Date().toLocaleDateString()}</p>
        <p><strong>Total Words:</strong> ${totalWords.toLocaleString()}</p>
    </div>`
    
    // Add table of contents if multiple sections
    if (compiledContent.sections.length > 1) {
      html += `
    <div class="toc">
        <h2>Table of Contents</h2>
        <ul>`
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        html += `<li>${index + 1}. ${sectionTitle}</li>`
      })
      html += `
        </ul>
    </div>`
    }
    
    // Add content sections with clean formatting
    compiledContent.sections.forEach((section, index) => {
      const sectionTitle = section.title || `Chapter ${index + 1}`
      
      // Clean up the content
      let cleanContent = section.content
        .replace(/\n{3,}/g, '\n\n') // Replace multiple newlines with double newlines
        .replace(/^\s+|\s+$/gm, '') // Trim whitespace from each line
        .replace(/\s{2,}/g, ' ') // Replace multiple spaces with single space
        .replace(/\n/g, '</p><p>') // Convert newlines to paragraph breaks
      
      html += `
    <div class="section">
        <h2>${sectionTitle}</h2>
        <p>${cleanContent}</p>
    </div>`
    })
    
    html += `
</body>
</html>`
    
    return html
  }

  generatePlainTextOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const totalWords = compiledContent.totalWords || 0
    
    let textContent = `${title}\n\n`
    textContent += `Author: ${author}\n\n`
    textContent += `Generated: ${new Date().toLocaleDateString()}\n\n`
    textContent += `Total Words: ${totalWords.toLocaleString()}\n\n`
    textContent += `${'='.repeat(50)}\n\n`
    
    // Add table of contents if multiple sections
    if (compiledContent.sections.length > 1) {
      textContent += `Table of Contents\n\n`
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        textContent += `${index + 1}. ${sectionTitle}\n`
      })
      textContent += `\n${'='.repeat(50)}\n\n`
    }
    
    // Add content sections with clean formatting
    compiledContent.sections.forEach((section, index) => {
      const sectionTitle = section.title || `Chapter ${index + 1}`
      textContent += `${sectionTitle}\n\n`
      
      // Clean up the content - remove excessive whitespace and format properly
      let cleanContent = section.content
        .replace(/\n{3,}/g, '\n\n') // Replace multiple newlines with double newlines
        .replace(/^\s+|\s+$/gm, '') // Trim whitespace from each line
        .replace(/\s{2,}/g, ' ') // Replace multiple spaces with single space
      
      textContent += `${cleanContent}\n\n`
      
      // Add page break between chapters (except for the last one)
      if (index < compiledContent.sections.length - 1) {
        textContent += `${'='.repeat(50)}\n\n`
      }
    })
    
    return textContent
  }

  async generateFormatOutput(compiledContent, format) {
    // Simple, dynamic format generation - whatever the user selected
    const title = compiledContent.userInput.book_title || 'Generated Content'
    const author = compiledContent.userInput.author_name || 'AI Generated'
    const content = compiledContent.sections.map(section => section.content).join('\n\n')
    
    // Generate based on format - simple and dynamic
    switch (format.toLowerCase()) {
      case 'json':
        return JSON.stringify(compiledContent, null, 2)
      
      case 'markdown':
      case 'md':
        return this.generateMarkdownOutput(compiledContent)
      
      case 'html':
        return this.generateHTMLOutput(compiledContent)
      
      case 'pdf':
        return this.generatePDFOutput(compiledContent)
      
      case 'epub':
        return await this.generateEPUBOutput(compiledContent)
      
      case 'docx':
        return await this.generateDOCXOutput(compiledContent)
      
      case 'text':
      case 'txt':
      default:
        return content
    }
  }

  generatePDFOutput(compiledContent) {
    try {
      // Import jsPDF dynamically to avoid SSR issues
      const { jsPDF } = require('jspdf')
      
      const title = compiledContent.userInput.book_title || 'Generated Content'
      const author = compiledContent.userInput.author_name || 'AI Generated'
      const totalWords = compiledContent.totalWords || 0
      
      // Create new PDF document
      const doc = new jsPDF()
      
      // Set font and add title
      doc.setFontSize(20)
      doc.setFont(undefined, 'bold')
      doc.text(title, 20, 30)
      
      // Add author and metadata
      doc.setFontSize(12)
      doc.setFont(undefined, 'normal')
      doc.text(`Author: ${author}`, 20, 45)
      doc.text(`Generated: ${new Date().toLocaleDateString()}`, 20, 55)
      doc.text(`Total Words: ${totalWords.toLocaleString()}`, 20, 65)
      
      let yPosition = 85
      const pageHeight = doc.internal.pageSize.height
      const margin = 20
      const lineHeight = 7
      
      // Add table of contents if multiple sections
      if (compiledContent.sections.length > 1) {
        doc.setFontSize(16)
        doc.setFont(undefined, 'bold')
        doc.text('Table of Contents', 20, yPosition)
        yPosition += 15
        
        doc.setFontSize(12)
        doc.setFont(undefined, 'normal')
        compiledContent.sections.forEach((section, index) => {
          const sectionTitle = section.title || `Chapter ${index + 1}`
          doc.text(`${index + 1}. ${sectionTitle}`, 25, yPosition)
          yPosition += lineHeight
          
          // Check if we need a new page
          if (yPosition > pageHeight - 30) {
            doc.addPage()
            yPosition = 30
          }
        })
        yPosition += 10
      }
      
      // Add content sections
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        
        // Add chapter title
        doc.setFontSize(16)
        doc.setFont(undefined, 'bold')
        doc.text(sectionTitle, 20, yPosition)
        yPosition += 15
        
        // Add chapter content
        doc.setFontSize(11)
        doc.setFont(undefined, 'normal')
        
        // Split content into lines that fit the page width
        const content = section.content.replace(/\n{3,}/g, '\n\n').replace(/^\s+|\s+$/gm, '').replace(/\s{2,}/g, ' ')
        const lines = doc.splitTextToSize(content, 170) // 170mm width
        
        lines.forEach(line => {
          // Check if we need a new page
          if (yPosition > pageHeight - 30) {
            doc.addPage()
            yPosition = 30
          }
          
          doc.text(line, 20, yPosition)
          yPosition += lineHeight
        })
        
        yPosition += 10
      })
      
      // Return PDF as base64 string for download
      return doc.output('datauristring')
      
    } catch (error) {
      console.error('Error generating PDF:', error)
      // NO FALLBACK - THROW ERROR INSTEAD
      throw new Error(`PDF generation failed: ${error.message}`)
    }
  }

  async generateEPUBOutput(compiledContent) {
    // TEMPORARILY DISABLED: epub-gen is a Node.js library and cannot run in browser
    throw new Error('EPUB generation is temporarily unavailable. The epub-gen library requires Node.js and cannot run in the browser. Please use PDF, DOCX, or Markdown formats instead.')
    
    /* ORIGINAL CODE COMMENTED OUT FOR FUTURE SERVER-SIDE IMPLEMENTATION
    try {
      // Import epub-gen dynamically to avoid SSR issues
      const epub = require('epub-gen')
      
      const title = compiledContent.userInput.book_title || 'Generated Content'
      const author = compiledContent.userInput.author_name || 'AI Generated'
      
      // Prepare EPUB content structure
      const epubContent = {
        title: title,
        author: author,
        language: 'en',
        content: compiledContent.sections.map((section, index) => ({
          title: section.title || `Chapter ${index + 1}`,
          data: `<h1>${section.title || `Chapter ${index + 1}`}</h1>\n\n${section.content.replace(/\n/g, '<br>')}`
        })),
        output: false, // Don't write to file, return buffer
        verbose: false
      }
      
      // Generate EPUB buffer
      const buffer = await epub(epubContent)
      // Return as base64 string for download
      return `data:application/epub+zip;base64,${buffer.toString('base64')}`
      
    } catch (error) {
      console.error('Error generating EPUB:', error)
      // NO FALLBACK - THROW ERROR INSTEAD
      throw new Error(`EPUB generation failed: ${error.message}`)
    }
    */
  }

  async generateDOCXOutput(compiledContent) {
    try {
      // Import docx dynamically to avoid SSR issues
      const { Document, Packer, Paragraph, TextRun, HeadingLevel } = require('docx')
      
      const title = compiledContent.userInput.book_title || 'Generated Content'
      const author = compiledContent.userInput.author_name || 'AI Generated'
      const totalWords = compiledContent.totalWords || 0
      
      // Create document sections
      const sections = []
      
      // Add title
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: title,
              bold: true,
              size: 32
            })
          ],
          heading: HeadingLevel.TITLE,
          spacing: { after: 400 }
        })
      )
      
      // Add metadata
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: `Author: ${author}`,
              size: 24
            })
          ],
          spacing: { after: 200 }
        })
      )
      
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: `Generated: ${new Date().toLocaleDateString()}`,
              size: 24
            })
          ],
          spacing: { after: 200 }
        })
      )
      
      sections.push(
        new Paragraph({
          children: [
            new TextRun({
              text: `Total Words: ${totalWords.toLocaleString()}`,
              size: 24
            })
          ],
          spacing: { after: 400 }
        })
      )
      
      // Add table of contents if multiple sections
      if (compiledContent.sections.length > 1) {
        sections.push(
          new Paragraph({
            children: [
              new TextRun({
                text: 'Table of Contents',
                bold: true,
                size: 28
              })
            ],
            heading: HeadingLevel.HEADING_1,
            spacing: { after: 300 }
          })
        )
        
        compiledContent.sections.forEach((section, index) => {
          const sectionTitle = section.title || `Chapter ${index + 1}`
          sections.push(
            new Paragraph({
              children: [
                new TextRun({
                  text: `${index + 1}. ${sectionTitle}`,
                  size: 24
                })
              ],
              spacing: { after: 150 }
            })
          )
        })
      }
      
      // Add content sections
      compiledContent.sections.forEach((section, index) => {
        const sectionTitle = section.title || `Chapter ${index + 1}`
        
        // Add chapter title
        sections.push(
          new Paragraph({
            children: [
              new TextRun({
                text: sectionTitle,
                bold: true,
                size: 28
              })
            ],
            heading: HeadingLevel.HEADING_1,
            spacing: { before: 400, after: 300 }
          })
        )
        
        // Add chapter content
        const cleanContent = section.content
          .replace(/\n{3,}/g, '\n\n')
          .replace(/^\s+|\s+$/gm, '')
          .replace(/\s{2,}/g, ' ')
        
        // Split content into paragraphs
        const paragraphs = cleanContent.split('\n\n').filter(p => p.trim())
        paragraphs.forEach(paragraph => {
          sections.push(
            new Paragraph({
              children: [
                new TextRun({
                  text: paragraph.trim(),
                  size: 24
                })
              ],
              spacing: { after: 200 }
            })
          )
        })
      })
      
      // Create document
      const doc = new Document({
        sections: [{
          properties: {},
          children: sections
        }]
      })
      
      // Generate DOCX blob (browser-compatible)
      const blob = await Packer.toBlob(doc)
      // Convert blob to base64 for download
      const arrayBuffer = await blob.arrayBuffer()
      const base64 = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)))
      return `data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,${base64}`
      
    } catch (error) {
      console.error('Error generating DOCX:', error)
      // NO FALLBACK - THROW ERROR INSTEAD
      throw new Error(`DOCX generation failed: ${error.message}`)
    }
  }

  generateXMLOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    let xml = `<?xml version="1.0" encoding="UTF-8"?>
<document>
    <title>${title}</title>
    <metadata>
        <generated>${new Date().toISOString()}</generated>
        <totalWords>${compiledContent.totalWords || 0}</totalWords>
        <type>${compiledContent.userInput.book_type || 'Content'}</type>
    </metadata>
    <content>`
    
    compiledContent.sections.forEach((section, index) => {
      xml += `
        <section id="${index + 1}">
            <heading>Section ${index + 1}</heading>
            <text><![CDATA[${section.content}]]></text>
        </section>`
    })
    
    xml += `
    </content>
</document>`
    
    return xml
  }

  generateCSVOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    let csv = 'Section,Content,WordCount\n'
    
    compiledContent.sections.forEach((section, index) => {
      const content = section.content.replace(/"/g, '""').replace(/\n/g, ' ')
      const wordCount = section.content.split(' ').length
      csv += `"Section ${index + 1}","${content}",${wordCount}\n`
    })
    
    return csv
  }

  generateYAMLOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    
    return {
      title: title,
      metadata: {
        generated: new Date().toISOString(),
        totalWords: compiledContent.totalWords || 0,
        type: compiledContent.userInput.book_type || 'Content'
      },
      sections: compiledContent.sections.map((section, index) => ({
        id: index + 1,
        title: `Section ${index + 1}`,
        content: section.content,
        wordCount: section.content.split(' ').length
      }))
    }
  }

  generateRTFOutput(compiledContent) {
    const title = compiledContent.userInput.book_title || 'Generated Content'
    let rtf = `{\\rtf1\\ansi\\deff0 {\\fonttbl {\\f0 Times New Roman;}}
{\\colortbl;\\red0\\green0\\blue0;}
\\f0\\fs24
{\\b ${title}}\\par\\par`
    
    compiledContent.sections.forEach((section, index) => {
      rtf += `{\\b Section ${index + 1}}\\par\\par`
      rtf += `${section.content.replace(/\n/g, '\\par ').replace(/\r/g, '')}\\par\\par`
    })
    
    rtf += '}'
    return rtf
  }

  generateODTOutput(compiledContent) {
    // Generate OpenDocument Text format structure
    const title = compiledContent.userInput.book_title || 'Generated Content'
    
    return {
      title: title,
      content: compiledContent.sections.map((section, index) => ({
        heading: `Section ${index + 1}`,
        text: section.content
      })),
      metadata: {
        totalWords: compiledContent.totalWords || 0,
        generatedAt: new Date().toISOString(),
        type: compiledContent.userInput.book_type || 'Content'
      }
    }
  }

  generateGenericOutput(compiledContent, format) {
    // Generic fallback for any unknown format
    const title = compiledContent.userInput.book_title || 'Generated Content'
    
    return {
      format: format,
      title: title,
      content: compiledContent.sections.map((section, index) => ({
        section: index + 1,
        content: section.content
      })),
      metadata: {
        totalWords: compiledContent.totalWords || 0,
        generatedAt: new Date().toISOString(),
        type: compiledContent.userInput.book_type || 'Content',
        note: `Generated in ${format} format (generic)`
      }
    }
  }

  
  /**
   * Convert results array to node outputs format for BookCompilationService
   * FIXED: Pass structured chapters directly instead of breaking them up
   */
  convertResultsToNodeOutputs(results) {
    // Create a single structured output with all chapters
    const nodeOutputs = {
      'multi_chapter_book': {
        type: 'multi_chapter_generation',
        content: results, // Pass the structured chapters array directly
        metadata: {
          totalChapters: results.length,
          chapters: results.map(r => ({
            number: r.chapter,
            title: this.extractChapterTitle(r.content),
            content: r.content,
            metadata: r.aiMetadata || {}
          }))
        }
      }
    }
    
    return nodeOutputs
  }

  /**
   * Extract chapter title from chapter content
   */
  extractChapterTitle(content) {
    const titleMatch = content.match(/Chapter \d+: (.+?)(?:\n|$)/)
    return titleMatch ? titleMatch[1].trim() : null
  }

  generateDeliverables(formattedOutput, nodeData) {
    const deliverables = []
    
    // Handle new format structure with multiple formats
    if (formattedOutput.allFormats) {
      // New structure with multiple formats
      Object.entries(formattedOutput.allFormats).forEach(([format, content]) => {
        // Check if content is base64 data URI (for binary formats)
        if (typeof content === 'string' && content.startsWith('data:')) {
          const base64Data = content.split(',')[1]
          deliverables.push({
            format,
            content: content, // Keep full data URI for download
            filename: `lekhika_generated_content.${format}`,
            size: base64Data.length,
            isPrimary: format === formattedOutput.primaryFormat,
            mimeType: content.split(':')[1].split(';')[0],
            isBinary: true
          })
        } else {
          // CLEAN CONTENT - NO JSON GARBAGE
          let contentString = ''
          if (typeof content === 'string') {
            contentString = content
          } else if (content && content.content) {
            // Extract clean content from nested structure
            contentString = content.content
          } else if (content && typeof content === 'object') {
            // Handle object content properly - extract meaningful content
            if (content.chapters && Array.isArray(content.chapters)) {
              // Multi-chapter content - compile chapters
              contentString = content.chapters.map(chapter => 
                typeof chapter === 'string' ? chapter : 
                (chapter.content || chapter.text || JSON.stringify(chapter))
              ).join('\n\n')
            } else if (content.text || content.body) {
              contentString = content.text || content.body
            } else if (content.content) {
              // Direct content property
              contentString = content.content
            } else {
              // Last resort - try to extract any string content
              console.warn(`‚ö†Ô∏è Complex content structure for ${format}, attempting extraction`)
              contentString = JSON.stringify(content, null, 2)
            }
          } else {
            // Last resort - convert to string but this should not happen with professional formatter
            console.warn(`‚ö†Ô∏è Unexpected content structure for ${format}:`, typeof content)
            contentString = String(content)
          }
          
          deliverables.push({
            format,
            content: contentString,
            filename: `lekhika_generated_content.${format}`,
            size: contentString.length,
            isPrimary: format === formattedOutput.primaryFormat,
            mimeType: this.getMimeType(format),
            isBinary: false
          })
        }
      })
    } else {
      // Legacy structure with single format
      const exportFormats = nodeData.exportFormats || ['markdown']
      
      // CLEAN CONTENT - NO JSON GARBAGE FOR LEGACY STRUCTURE TOO
      let contentString = ''
      if (typeof formattedOutput === 'string') {
        contentString = formattedOutput
      } else if (formattedOutput && formattedOutput.content) {
        contentString = formattedOutput.content
      } else {
        console.warn(`‚ö†Ô∏è Legacy structure: Unexpected format for content:`, typeof formattedOutput)
        contentString = String(formattedOutput)
      }
      
      exportFormats.forEach(format => {
        deliverables.push({
          format,
          content: contentString,
          filename: `lekhika_generated_content.${format}`,
          size: contentString.length,
          isPrimary: true,
          mimeType: this.getMimeType(format)
        })
      })
    }

    console.log('üì¶ Generated deliverables:', deliverables.map(d => `${d.format} (${d.size} chars)`))
    return deliverables
  }

  getMimeType(format) {
    const mimeTypes = {
      'pdf': 'application/pdf',
      'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
      'html': 'text/html',
      'markdown': 'text/markdown',
      'md': 'text/markdown',
      'txt': 'text/plain',
      'text': 'text/plain',
      'json': 'application/json',
      'epub': 'application/epub+zip',
      'xml': 'application/xml'
    }
    return mimeTypes[format.toLowerCase()] || 'text/plain'
  }

  // REMOVED: generateTextFallback() - NO FALLBACK TEMPLATES


  updateExecutionState(workflowId, updates) {
    const currentState = this.executionState.get(workflowId) || {}
    this.executionState.set(workflowId, { ...currentState, ...updates })
  }

  getExecutionState(workflowId) {
    return this.executionState.get(workflowId)
  }

  clearExecutionState(workflowId) {
    this.executionState.delete(workflowId)
  }

  // Force stop workflow execution
  stopWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    if (currentState) {
      this.updateExecutionState(workflowId, {
        status: 'stopped',
        currentNode: null,
        stopped: true,
        stoppedAt: new Date()
      })
      console.log(`üõë Workflow ${workflowId} stopped by user`)
    }
  }

  // Check if workflow is stopped
  isWorkflowStopped(workflowId) {
    const state = this.executionState.get(workflowId)
    return state?.status === 'stopped'
  }

  // Check if workflow is paused
  isWorkflowPaused(workflowId) {
    const state = this.executionState.get(workflowId)
    return state?.status === 'paused'
  }

  // Pause workflow execution
  pauseWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    if (currentState && currentState.status === 'executing') {
      this.updateExecutionState(workflowId, {
        status: 'paused',
        pausedAt: new Date()
      })
      console.log(`‚è∏Ô∏è Workflow ${workflowId} paused by user`)
      return true
    }
    return false
  }

  // Resume workflow execution
  resumeWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    console.log(`üîÑ Attempting to resume workflow ${workflowId}:`, currentState)
    
    if (currentState && currentState.status === 'paused') {
      // Store the pauseResolver before updating state
      const pauseResolver = currentState.pauseResolver
      console.log(`üîÑ Found pauseResolver:`, !!pauseResolver)
      
      // Update state to executing
      this.updateExecutionState(workflowId, {
        ...currentState,
        status: 'executing',
        resumedAt: new Date()
      })
      
      // Resolve the pause promise to continue execution
      if (pauseResolver) {
        console.log(`‚ñ∂Ô∏è Resolving pause promise for workflow ${workflowId}`)
        pauseResolver()
        console.log(`‚ñ∂Ô∏è Pause promise resolved for workflow ${workflowId}`)
      } else {
        console.log(`‚ö†Ô∏è No pauseResolver found for workflow ${workflowId}`)
      }
      
      console.log(`‚ñ∂Ô∏è Workflow ${workflowId} resumed by user`)
      return true
    }
    
    console.log(`‚ùå Cannot resume workflow ${workflowId}: status is ${currentState?.status || 'undefined'}`)
    return false
  }

  // Resume from a specific node (milestone checkpoint)
  resumeFromNode(workflowId, nodeId) {
    const currentState = this.executionState.get(workflowId)
    console.log(`üîÑ Attempting to resume workflow ${workflowId} from node ${nodeId}:`, currentState)
    
    if (currentState) {
      // Find the checkpoint for this node
      const checkpoint = this.checkpointStates.get(`${workflowId}_${nodeId}`)
      if (checkpoint) {
        console.log(`‚úÖ Found checkpoint for node ${nodeId}, restoring state`)
        
        // Restore the execution state from checkpoint
        this.updateExecutionState(workflowId, {
          ...checkpoint.state,
          status: 'executing',
          resumedFromNode: nodeId,
          resumedAt: new Date()
        })
        
        console.log(`‚ñ∂Ô∏è Workflow ${workflowId} resumed from node ${nodeId}`)
        console.log(`‚ñ∂Ô∏è Restored state:`, this.executionState.get(workflowId))
        return true
      } else {
        console.log(`‚ùå No checkpoint found for node ${nodeId}`)
        console.log(`‚ùå Available checkpoints:`, Array.from(this.checkpointStates.keys()))
        return false
      }
    }
    
    console.log(`‚ùå Cannot resume workflow ${workflowId}: no state found`)
    return false
  }

  // Restart workflow execution from a checkpoint (for UI resume buttons)
  async restartFromCheckpoint(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser) {
    console.log(`üîÑ Restarting workflow ${workflowId} from checkpoint ${nodeId}`)
    
    try {
      // Check if we have a checkpoint for this node
      const checkpoint = this.checkpointStates.get(`${workflowId}_${nodeId}`)
      
      if (!checkpoint) {
        // If no checkpoint, try to resume from the failed node by restarting it
        console.log(`‚ö†Ô∏è No checkpoint found for node ${nodeId}, attempting to restart failed node`)
        return await this.restartFailedNode(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser)
      }
      
      // Restore the execution state from checkpoint
      this.updateExecutionState(workflowId, {
        ...checkpoint.state,
        status: 'executing',
        resumedFromNode: nodeId,
        resumedAt: new Date()
      })
      
      // Continue execution from the checkpoint
      console.log(`‚ñ∂Ô∏è Continuing workflow execution from checkpoint ${nodeId}`)
      
      // Find the node index to resume from
      const executionOrder = this.getExecutionOrder(nodes, edges)
      const nodeIndex = executionOrder.findIndex(node => node.id === nodeId)
      
      if (nodeIndex === -1) {
        throw new Error(`Node ${nodeId} not found in execution order`)
      }
      
      // Continue execution from the next node
      return await this.continueExecutionFromNode(
        workflowId, 
        nodes, 
        edges, 
        initialInput, 
        progressCallback, 
        superAdminUser,
        nodeIndex + 1, // Start from next node
        checkpoint.state.nodeOutputs || {}
      )
      
    } catch (error) {
      console.error(`‚ùå Error restarting from checkpoint:`, error)
      return { success: false, error: error.message }
    }
  }

  // Restart a failed node specifically
  async restartFailedNode(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser) {
    console.log(`üîÑ Restarting failed node ${nodeId} in workflow ${workflowId}`)
    
    try {
      // Find the node to restart
      const nodeToRestart = nodes.find(node => node.id === nodeId)
      if (!nodeToRestart) {
        throw new Error(`Node ${nodeId} not found`)
      }
      
      // Get the current execution state
      const currentState = this.executionState.get(workflowId)
      if (!currentState) {
        throw new Error(`No execution state found for workflow ${workflowId}`)
      }
      
      // Get outputs from previous nodes
      const previousOutputs = currentState.nodeOutputs || {}
      
      // Create a pipeline data object with previous outputs
      const pipelineData = {
        ...previousOutputs,
        workflowId,
        currentNodeId: nodeId,
        executionStartTime: currentState.startTime || new Date()
      }
      
      // Clear any error state for this node
      this.updateExecutionState(workflowId, {
        ...currentState,
        status: 'executing',
        currentNodeId: nodeId,
        currentNodeStatus: 'executing',
        errors: currentState.errors ? currentState.errors.filter(error => error.nodeId !== nodeId) : []
      })
      
      // Execute the specific node
      const result = await this.executeNode(nodeToRestart, pipelineData, progressCallback, workflowId, superAdminUser)
      
      if (result.success) {
        // Update the execution state with the successful result
        this.updateExecutionState(workflowId, {
          ...currentState,
          nodeOutputs: {
            ...currentState.nodeOutputs,
            [nodeId]: result.output
          },
          currentNodeStatus: 'completed'
        })
        
        console.log(`‚úÖ Successfully restarted node ${nodeId}`)
        
        // Try to continue with the next nodes
        return await this.continueWorkflowFromNode(workflowId, nodeId, nodes, edges, initialInput, progressCallback, superAdminUser)
      } else {
        throw new Error(result.error || 'Node execution failed')
      }
      
    } catch (error) {
      console.error(`‚ùå Error restarting failed node:`, error)
      return { success: false, error: error.message }
    }
  }

  // Continue workflow execution from a specific node
  async continueWorkflowFromNode(workflowId, fromNodeId, nodes, edges, initialInput, progressCallback, superAdminUser) {
    console.log(`‚ñ∂Ô∏è Continuing workflow ${workflowId} from node ${fromNodeId}`)
    
    try {
      // Get execution order
      const executionOrder = this.getExecutionOrder(nodes, edges)
      const fromNodeIndex = executionOrder.findIndex(node => node.id === fromNodeId)
      
      if (fromNodeIndex === -1) {
        throw new Error(`Node ${fromNodeId} not found in execution order`)
      }
      
      // Continue execution from the next node
      return await this.continueExecutionFromNode(
        workflowId,
        nodes,
        edges,
        initialInput,
        progressCallback,
        superAdminUser,
        fromNodeIndex + 1,
        this.executionState.get(workflowId)?.nodeOutputs || {}
      )
      
    } catch (error) {
      console.error(`‚ùå Error continuing workflow from node:`, error)
      return { success: false, error: error.message }
    }
  }

  // Continue execution from a specific node index
  async continueExecutionFromNode(workflowId, nodes, edges, initialInput, progressCallback, superAdminUser, startIndex, existingOutputs) {
    console.log(`üîÑ Continuing execution from node index ${startIndex}`)
    
    const executionOrder = this.getExecutionOrder(nodes, edges)
    
    // Initialize pipeline data with existing outputs
    const pipelineData = {
      userInput: initialInput,
      nodeOutputs: existingOutputs,
      lastNodeOutput: null,
      previousNodePassover: null
    }
    
    // Execute remaining nodes
    for (let i = startIndex; i < executionOrder.length; i++) {
      const node = executionOrder[i]
      
      // Check if workflow was paused or stopped
      if (this.isWorkflowPaused(workflowId)) {
        await this.waitForResume(workflowId)
      }
      
      if (this.isWorkflowStopped(workflowId)) {
        console.log(`üõë Workflow ${workflowId} stopped during continuation`)
        return { success: false, status: 'stopped' }
      }
      
      try {
        console.log(`üîç EXECUTING NODE ${i + 1}/${executionOrder.length}: ${node.id} (${node.data.label})`)
        
        // Execute individual node
        const nodeOutput = await this.executeNode(node, pipelineData, workflowId, progressCallback)
        
        // Update pipeline
        pipelineData.nodeOutputs[node.id] = nodeOutput
        pipelineData.lastNodeOutput = nodeOutput
        
        // Update execution state
        this.updateExecutionState(workflowId, {
          [`results.${node.id}`]: nodeOutput,
          nodeOutputs: pipelineData.nodeOutputs,
          currentNodeIndex: i,
          completedNodes: executionOrder.slice(0, i + 1).map(n => n.id)
        })

        // CREATE CHECKPOINT AFTER EACH NODE COMPLETION
        this.createCheckpoint(workflowId, node.id, {
          ...this.executionState.get(workflowId),
          nodeOutputs: pipelineData.nodeOutputs,
          currentNodeIndex: i,
          completedNodes: executionOrder.slice(0, i + 1).map(n => n.id)
        })
        
        // Progress callback
        const completionProgress = ((i + 1) / executionOrder.length) * 100
        if (progressCallback) {
          progressCallback({
            nodeId: node.id,
            nodeName: node.data.label,
            progress: completionProgress,
            status: 'completed',
            output: nodeOutput,
            nodeIndex: i + 1,
            totalNodes: executionOrder.length,
            isNodeComplete: true,
            checkpointCreated: true
          })
        }
        
      } catch (error) {
        console.error(`‚ùå Error executing node ${node.id}:`, error)
        throw error
      }
    }
    
    // Workflow completed
    console.log(`‚úÖ Workflow ${workflowId} completed successfully`)
    return {
      success: true,
      status: 'completed',
      results: pipelineData.nodeOutputs
    }
  }

  // Create checkpoint after node completion
  createCheckpoint(workflowId, nodeId, state, pauseResolver) {
    const checkpointKey = `${workflowId}_${nodeId}`
    this.checkpointStates.set(checkpointKey, {
      nodeId,
      state: { ...state },
      pauseResolver,
      timestamp: new Date(),
      nodeOutput: state.nodeOutputs?.[nodeId]
    })
    console.log(`üíæ Checkpoint created for node ${nodeId} in workflow ${workflowId}`)
  }

  // Wait for workflow to be resumed
  async waitForResume(workflowId) {
    console.log(`‚è≥ Creating pause promise for workflow ${workflowId}`)
    return new Promise((resolve) => {
      const currentState = this.executionState.get(workflowId)
      console.log(`‚è≥ Current state for workflow ${workflowId}:`, currentState)
      
      if (currentState) {
        // Store the resolver to be called when resumed
        currentState.pauseResolver = resolve
        this.updateExecutionState(workflowId, currentState)
        console.log(`‚è≥ Pause resolver stored for workflow ${workflowId}`)
        console.log(`‚è≥ Updated state:`, this.executionState.get(workflowId))
      } else {
        // If no state, just resolve immediately
        console.log(`‚è≥ No state found for workflow ${workflowId}, resolving immediately`)
        resolve()
      }
    })
  }

  // Stop workflow execution immediately
  stopWorkflow(workflowId) {
    const currentState = this.executionState.get(workflowId)
    if (currentState) {
      this.updateExecutionState(workflowId, {
        status: 'stopped',
        stoppedAt: new Date(),
        forceStopped: true
      })
      
      // Resolve any pending pause to allow cleanup
      if (currentState.pauseResolver) {
        currentState.pauseResolver()
        delete currentState.pauseResolver
      }
      
      console.log(`üõë Workflow ${workflowId} force stopped by user`)
      return true
    }
    return false
  }

  // Retry a specific failed node
  retryNode(nodeId) {
    const currentState = this.executionState.get(nodeId)
    if (currentState && currentState.status === 'paused') {
      // Clear the error and mark for retry
      this.updateExecutionState(nodeId, {
        status: 'ready_for_retry',
        retryRequested: true,
        retryAt: new Date()
      })
      
      console.log(`üîÑ Node ${nodeId} marked for retry`)
      return true
    }
    return false
  }

  // Resume from a specific node
  resumeFromNode(nodeId) {
    const currentState = this.executionState.get(nodeId)
    if (currentState && currentState.status === 'paused') {
      // Resume from this specific node
      this.updateExecutionState(nodeId, {
        status: 'executing',
        resumedAt: new Date(),
        resumeFromNode: nodeId
      })
      
      // Resolve the pause promise to continue execution
      if (currentState.pauseResolver) {
        currentState.pauseResolver()
        delete currentState.pauseResolver
      }
      
      console.log(`‚ñ∂Ô∏è Workflow resumed from node ${nodeId}`)
      return true
    }
    return false
  }

  // Get current paused workflow
  getCurrentPausedWorkflow() {
    const pausedWorkflows = Array.from(this.executionState.entries())
      .filter(([id, state]) => state.status === 'paused')
    
    return pausedWorkflows.length > 0 ? pausedWorkflows[0] : null
  }

  // Check if any workflow is paused
  hasPausedWorkflow() {
    return Array.from(this.executionState.values())
      .some(state => state.status === 'paused')
  }

  // Get all execution states for debugging
  getAllExecutionStates() {
    return Array.from(this.executionState.entries())
  }

  // Get execution state for a specific workflow
  getExecutionState(workflowId) {
    return this.executionState.get(workflowId)
  }

  /**
   * Clear all executions (zombie killer)
   */
  clearAllExecutions() {
    console.log('üßπ Clearing all zombie executions...')
    this.executionState.clear()
    this.checkpointStates.clear()
    console.log('‚úÖ All executions cleared')
  }

  /**
   * Kill stuck executions older than 5 minutes
   */
  killStuckExecutions() {
    const now = Date.now()
    const stuckThreshold = 5 * 60 * 1000 // 5 minutes
    
    for (const [workflowId, state] of this.executionState) {
      if (state.startedAt && (now - state.startedAt) > stuckThreshold) {
        console.log(`üíÄ Killing stuck execution: ${workflowId}`)
        this.executionState.delete(workflowId)
      }
    }
  }
}

const workflowExecutionService = new WorkflowExecutionService()

module.exports = workflowExecutionService